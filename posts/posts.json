[
  {
    "path": "posts/2023-01-12-bayes-factor/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część III: Czynnik Bayesowski",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2023-01-13",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWnioskowanie Statystyczne\r\nPorównywanie modeli\r\nCzynnik Bayesowski\r\nCzynnik Bayesowski a złożoność modelu\r\nBayesian Point Null Hypothesis Testing\r\nSavage-Dickey density ratio\r\nLikelihood sampling\r\n\r\nPorównywanie modeli?\r\nCzynnik Bayesa nie mierzy czy model jest prawdziwy\r\nCzynnik Bayesowski jest wrażliwy na rozkłady a priori parametrów modelu\r\nRozkłady posteriori parametrów nie muszą się zgadzać z czynnikiem Bayesa\r\nCzynnik Bayesa a prawdopodobieństwo braku efektu\r\nI co z tym wszystkim zrobić?\r\n\r\n\r\nZakończenie\r\n\r\n\r\n\r\n.MathJax_Display, .MJXc-display, .MathJax_SVG_Display {\r\n    overflow-x: auto;\r\n    overflow-y: hidden;\r\n}\r\n\r\nimg[src*='#center'] { \r\n    display: block;\r\n    margin: auto;\r\n}\r\n\r\n\r\n\r\nMathJax.Hub.Config({\r\n  TeX: { equationNumbers: { autoNumber: \"AMS\" } }\r\n});\r\n\r\nOsobiście nienawidzę czynników Bayesa…\r\n\r\n\r\n— Andrew Gelman, 2017\r\n\r\nWnioskowanie Statystyczne\r\nJak pamiętamy, wnioskowanie w statystyce częstościowej opiera się na testowaniu hipotezy zerowej. Hipotezą zerową zwykle jest model, który świadczy o braku efektu, na przykład, że różnica średnich wynosi lub współczynnik regresji wynosi 0. Im mniejsza p-value, tym bardziej otrzymane dane nie pasują to tego modelu. Jeśli jednak p-value jest większa niż 0.05 nie oznacza to, że model zerowy jest bardziej prawdopodobny niż inne. Innymi słowy, ten rodzaj testowania może nam jedynie powiedzieć o występowaniu różnicy średnich czy występowaniu korelacji, ale nie o jej braku.\r\nPrzypomnijmy sobie, że wielkość przedziału ufności dla testu zależy od wariancji i wielkości próby. Gdyby NHST pozwalało nam orzekać o równości średnich, udowodnienie takiej hipotezy byłoby niezwykle łatwe.\r\nNa przykład załóżmy, że chcemy sprawdzić czy nowy lek nie różni się skutecznością od leku będącego już na rynku. Wystarczyłoby manipulować wielkością próby, aby otrzymać wniosek o braku istotnej różnicy pomiędzy wpływem tych dwóch leków!\r\nDlatego chcielibyśmy móc orzec, który z modeli (hipotez) jest bardziej prawdopodobny. Podejście bayesowskie daje nam taką możliwość (choć, nie do końca).\r\nPorównywanie modeli\r\nCzym jest model? Najprościej mówiąc model jest zbiorem ograniczeń, który nakładamy na proces generujący dane. Na przykład może interesować nas porównanie dwóch różnych modeli regresji z innymi zbiorami predyktorów. Przykładowy model \\(M_i\\) posiada parametry \\(\\theta\\) i funkcję wiarygodności \\(P(D \\mid \\theta, M)\\). Załóżmy, że chcielibyśmy obliczyć prawdopodobieństwo a posteriori modelu \\(P(M \\mid D)\\).\r\n\\[P(M_i \\mid D) = \\frac{ P(D \\mid M_i) \\ P(M_i) }{P(D) = \\int P(D \\mid M) \\ P(M) \\  \\text{d} M}\\,\\]\r\nWidzimy, że analogicznie jak w poprzednich częściach tutorialu za pomocą twierdzenia Bayesa, interesuje nas obliczenie prawdopodobieństwa modelu \\(M_i\\) pod warunkiem otrzymanych przez nas danych.\r\nPrawdopodobieństwo \\(P(M_1)\\) jest oczywiście prawdopodobieństwem a priori naszego modelu. Sami je ustalamy, więc nie ma z nim problemu.\r\nNasuwa się jednak pytanie jak obliczyć funkcję wiarygodności? \\(P(D|M_1)\\) oznacza marginalne prawdopodobieństwo (marginal likelihood) otrzymania danych pod warunkiem modelu dla wszystkich możliwych wartości parametrów:\r\n\\[P(D \\mid M_i) = \\int P(D \\mid \\theta, M_i) \\ P(\\theta \\mid M_i) \\ \\text{d}\\theta\\]\r\nCo to znaczy dla wszystkich możliwych parametrów? Załóżmy, że mamy dwa modele regresji liniowej, różniące się tym, że jeden ma dodatkowy predyktor. Tak jak pisałem wcześniej w statystyce bayesowskiej nie otrzymamy punktowych estymat parametrów \\(\\theta\\), ale ich rozkłady \\(P(\\theta|y)\\). Ponieważ jednak chcemy otrzymać jedną wartość - prawdopodobieństwo otrzymania danych wygenerowanych przez model \\(M_1\\), a nie przez jakieś konkretne wartości parametrów tego modelu, musimy całkować po parametrach, by pozbyć się \\(\\theta\\) z równania.\r\nTeraz zerknijmy na mianownik naszego równania zawierający \\(P(D)\\). Tutaj sprawa się komplikuje, ponieważ by obliczyć \\(P(D)\\) musielibyśmy całkować po wszystkich (nieskończenie wielu) możliwych modelach. Jego obliczenie jest w zasadzie niemożliwe zarówno analitycznie (jak wyznaczyć rozkład \\(P(M)\\), dla wszystkich możliwych modeli), jak i numerycznie (ponieważ musielibyśmy wymyślić i policzyć te wszystkie modele).\r\nA co jeśli porównamy dwa modele ze sobą?\r\n\\[ \\underbrace{{\\frac{P(M_1|D)}{P(M_2|D)}}}_{\\text{stosunek a posteriori}} = \\frac{\\frac{P(D|M_1)P(M_1)}{P(D)}}{\\frac{P(D|M_2)P(M_2)}{P(D)}} = \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{{BF_{12}}} * \\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{stosunek a prior}}\\]\r\nRobiąc tak, pozbywamy się konieczności obliczania \\(P(D)\\)!\r\nCzynnik Bayesowski\r\nJednak oceniając modele nie używamy stosunku prawdopodobieństw posteriori, lecz stosunku marginalnych wiarygodności nazywanym Czynnikiem Bayesowskim:\r\n\\[BF_{12} = \\frac{P(D|M_1)}{P(D|M_2)}\\]\r\nJest tak dlatego, że stosunek prawdopodobieństw a posteriori zależy od danych, ale także od prawdopodobieństw a priori modeli. Manipulując nimi, moglibyśmy zawsze uzyskać miarę faworyzującą nasz model. Użycie stosunku funkcji wiarygodności, mówi nam o ile bardziej/mniej prawdopodobne jest, że \\(M_1\\) wyprodukował obserwowane dane od \\(M_2\\). Ponadto jeśli przepiszemy wzór na stosunek a posteriori, uzyskamy\r\n\\[ \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{BF_{12}} = \\underbrace{\\frac{P(M_1|D)}{P(M_2|D)}}_{\\text{stosunek posteriori }}:\\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{stosunek a priori}}\\]\r\nCo daje nam dodatkową interpretację czynnika Bayesowskiego. Jest on stosunkiem prawdopodobieństw posteriori podzielony przez stosunek prawdopodobieństw a priori naszych dwóch modeli. Mówi nam o ile zmieniły się nasze przekonania a priori po zobaczeniu danych. Innymi słowy, Czynnik Bayesowski mówi nam jak zmieniły się pod wpływem danych nasze początkowe przekonania co do tego, który model jest lepszy.\r\nZałóżmy, że przeprowadziliśmy analizę dwóch modeli i otrzymaliśmy \\(stosunek \\ posteriori = 4\\), faworyzujący pierwszy model. Jednak \\(BF_{12} = 0.5\\), mówiąc nam, że drugi model wyprodukował obserwowane dane z dwa razy większym prawdopodobieństwem. Skąd taka rozbieżność? Wynika ze tego, że \\(stosunek \\ a \\ priori = 8\\), od początku faworyzował pierwszy model.\r\nZwróćmy też uwagę, że jeśli przypiszemy modelom takie same prawdopodobieństwa a priori, stosunek a posteriori będzie się równał czynnikowi Bayesa.\r\nCzynnik Bayesowski a złożoność modelu\r\nCzynnik Bayesowski w naturalny sposób karze za złożoność modelu, choć ta właściwość nie jest widoczna na pierwszy rzut oka. Żeby ją zwizualizować, wyobraźmy sobie, że \\(P(D|M)\\) nie jest pojedyńczą wartością prawdopodobieństwa otrzymania zaobserwowanych danych, lecz rozkładem prawdopodobieństwa wygenerowania wszystkich możliwych zbiorów danych, przez model. Weźmy dwa takie rozkłady dla modeli: prostszego modelu - \\(M_1\\) i bardziej złożonego \\(M_2\\). Bardziej złożony model jest w stanie wygenerować więcej możliwych zbiorów danych niż prostszy, ponieważ jest bardziej elastyczny.\r\n\r\nCałka prawdopodobieństw (pole pod wykresem) każdego z rozkładów musi wynosić 1 (z definicji rozkładów prawdopodobieństwa). Oznacza to, że dla przedziału \\(C_1\\) model \\(M_1\\) będzie miał zawsze większe prawdopodobieństwo wygenerowania danych niż \\(M_2\\), w konsekwencji czynnik bayesa będzie faworyzował \\(M_1\\). Logika stojąca za taka własnością \\(BF\\) jest iście Occamowska. Zadajmy sobie pytanie, który z modeli ma większe prawdopodobieństwo wygenerowania danych w przedziale \\(C_1\\) jeśli otrzymane przez nas dane mogą być wyjaśnione przez oba modele. Ten, który częściej będzie generował zbiory danych zawierające się w przedziale \\(C_1\\), czyli \\(M_1\\).\r\nBayesian Point Null Hypothesis Testing\r\nPrzypomnijmy sobie NHST (Null Hypothesis Significance Testing), które testuje hipotezy zerową i alternatywną. Hipoteza zerowa zwykle oznacza, że jakiś parametr \\(\\delta = 0\\), a hipoteza alternatywna, że \\(\\delta \\neq 0\\). Czynnik Bayesa w naukach społecznych jest często wykorzystywany podobnym celu. Nazywa się to Bayesian Point Null Hypothesis Testing (BPNHT). Testujemy model, który zakłada, że \\(\\delta\\) może przyjąć dowolną wartość, względem takiego, w którym jego wartość jest zerowa. Takie podobieństwo pomiędzy tymi metodami nie jest przypadkowe i dzieli problemy z wersją częstościową, o czym wkrótce.\r\nJako przykład posłużymy się czynnikiem Bayesa do przeprowadzenia bayesowskiej wersji jednoczynnikowej analizy wariancji. Wygenerujmy sobie dane:\r\n\r\n\r\nset.seed(123)\r\nfactor_a = sample(c(rep(0,50),rep(1,50)))\r\ny = rnorm(100, 3 + 0.80*factor_a, 1)\r\ndata = data.frame(id = 1:50, y = y, factor_a = factor_a)\r\n\r\n\r\nCi z Was, którzy są zaznajomieni z klasyczną wersją analizy wariancji wiedzą, że Anova to specjalny przypadek regresji liniowej. Jest też idealnym przykładem do użycia czynnika bayesowskiego w praktyce, ponieważ polega na porównaniu modeli. Najprostszym modelem, naszym modelem zerowym, będzie model, który zawiera tylko stałą. Modelem bardziej skomplikowanym, który chcemy przetestować jest model zawierający dodatkowo współczynnik regresji dla czynnika a:\r\n\\[\\begin{eqnarray*}\r\n&  &\r\nM_0: \\ \\ y = \\alpha\\\\\r\n&  &\r\nM_1: \\ \\ y = \\alpha + \\beta*X_a \\\\\r\n&  &\r\n\\end{eqnarray*}\\]\r\nJeśli \\(y\\) nie jest powiązany z czynnikiem \\(X_a\\), czynnik Bayesa powinien faworyzować model zerowy.\r\nSavage-Dickey density ratio\r\nDla pewnych modeli, jeśli wybierzemy odpowiednie rozkłady a priori, możemy obliczyć \\(BF\\) analitycznie. W przypadku bardziej skomplikowanych zdani jesteśmy na metody numeryczne. Niestety, dla wielu modeli estymacja \\(BF\\) jest trudna i niełatwa do zastosowania w praktyce.\r\nIstnieje jednak prosty sposób na obliczenie czynnika bayesowskiego dla modeli zagnieżdżonych (nested models), czyli takich, w których jeden model możemy traktować jako rozszerzenie drugiego. Popatrzmy na \\(M_0\\) i \\(M_1\\). Możemy przeformułować \\(M_0\\) w następujący sposób:\r\n\\[M_0: \\ \\ y = \\alpha + \\beta*X_a, \\  \\ \\beta = 0\\] Teraz widzimy, że \\(M_0\\) jest zagnieżdżony w \\(M_1\\), to znaczy jest modelem \\(M_1\\), którego wartość parametru \\(\\beta_1\\) została ustawiona jako stała.\r\nGeneralnie, mając dwa modele z parametrami \\(\\varphi\\) i \\(\\delta\\), takie, że \\(M_0:\\delta=\\delta_0,\\varphi\\) i \\(M_1:\\delta,\\varphi\\), które spełniają warunek \\(p(\\varphi\\mid M_0) = p(\\varphi\\mid \\delta=\\delta_0,M_1)\\), możemy obliczyć \\(BF_{01}\\) w następujący sposób:\r\n\\[\\text{BF}_{01} = \\frac{p(\\delta=\\delta_0\\mid y,M_1)}{p(\\delta=\\delta_0\\mid M_1)}\\]\r\nDowód\r\nZ definicji:\r\n\\[\\begin{equation}\\label{eq:bf}\\text{BF}_{01}=\\frac{p(y \\mid M_0)}{p(y \\mid M_1)}.\\end{equation}\\]\r\nMożemy przekształcić \\(p(y \\mid M_0)\\) w: \\[\\begin{equation}\r\n\\begin{split}\r\n p(y \\mid M_0) &= \\int p(y \\mid \\varphi,M_0) \\, p(\\varphi\\mid M_0) \\, \\mathrm{d} \\varphi \\\\\r\n  &= \\int p(y \\mid \\varphi,\\delta=\\delta_0,M_1) \\, p(\\varphi\\mid \\delta=\\delta_0,M_1) \\, \\mathrm{d} \\varphi \\\\\r\n  &= p(y \\mid \\delta=\\delta_0,M_1).\\\\\r\n\\end{split}\r\n\\label{eq:ml-m0}\r\n\\end{equation}\\]\r\nStosując twierdzenie Bayesa do ostatniego wyniku:\r\n\\[\\begin{equation}\\label{eq:ml-bt}\r\np(y \\mid \\delta=\\delta_0,M_1) = \\frac{p(\\delta=\\delta_0\\mid y,M_1) \\, p(y \\mid M_1)}{p(\\delta=\\delta_0\\mid M_1)}\\end{equation}\\]\r\nWięc:\r\n\\[\\begin{equation}\r\n\\begin{split}\r\n  \\text{BF}_{01} &\\overset{\\eqref{eq:bf}}{=} \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}\\\\\r\n  &= p(y \\mid M_0) \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &\\overset{\\eqref{eq:ml-m0}}{=} p(y \\mid \\delta=\\delta_0,M_1) \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &\\overset{\\eqref{eq:ml-bt}}{=} \\frac{p(\\delta=\\delta_0\\mid y,M_1) \\, p(y \\mid M_1)}{p(\\delta=\\delta_0\\mid M_1)} \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &= \\frac{p(\\delta=\\delta_0 \\mid y,M_1)}{p(\\delta=\\delta_0\\mid M_1)},\r\n\\end{split}\r\n\\end{equation}\\]\r\nObliczmy sobie \\(BF_{10}\\) posługując się numerycznym przybliżeniem stosunku Savage-Dickey. By to zrobić musimy najpierw zdefiniować nasz \\(M_1\\). Przyjmijmy, że rozkłady a priori zarówno \\(\\alpha\\) jak i \\(\\beta\\) to:\r\n\\[ \\alpha, \\beta \\sim N(0,10)\\] Wobec tego mianownik stosunku Savage-Dickey, będzie gęstością prawdopodobieństwa wylosowania 0 z powyższego rozkładu.\r\n\r\n\r\nprior_beta_0 = dnorm(0,0,10)\r\n\r\n\r\nNasz model w JAGS będzie wyglądał tak:\r\n\r\n\r\nm1 = '\r\nmodel{\r\nfor(i in 1:length(y)){\r\ny[i] ~ dnorm(mu[i], precision)\r\nmu[i] = alpha + beta*factor_a[i]\r\n}\r\nprecision ~ dunif(0.0000001,100)\r\nalpha ~ dnorm(0,10^-2)\r\nbeta ~ dnorm(0,10^-2)\r\n}'\r\n\r\n\r\nWyprodukujmy rozkłady posteriori parametru \\(\\beta\\).\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rjags)\r\nlibrary(ggmcmc)\r\nlibrary(polspline)\r\n\r\n# Parametry, które chcemy śledzić. \r\nparams = c(\"beta\")\r\n# Inicjacja modelu\r\njmod1 = jags.model(file = textConnection(m1), data = data, n.chains = 4, inits = NULL, n.adapt = 100)\r\n# Wypalanie\r\nupdate(jmod1, n.iter=100^2, by=1)\r\n# Losowanie próbek z rozkładu posteriori\r\npost = coda.samples(jmod1, params, n.iter = 10*100^2, thin = 1)\r\n# tidy format\r\nmodel1 = ggs(post)\r\n\r\n\r\nTeraz wyestymujemy prawdopodobieństwo wylosowania \\(\\beta = 0\\) z rozkładu posteriori.\r\n\r\n\r\npost_samples = filter(model1, Parameter == \"beta\")$value\r\nfit.posterior <- logspline(post_samples)\r\nposterior_beta_0 <- dlogspline(0, fit.posterior)\r\n\r\n\r\nMamy już wszystko by policzyć \\(BF_{01}\\). Jednak ponieważ \\(M_0\\) jest naszym modelem zerowym, popatrzmy na \\(BF_{10}\\) czyli \\(\\frac{1}{BF_{01}}\\).\r\nNasz \\(BF_{10}\\) wynosi\r\n\r\n\r\n1/(posterior_beta_0/prior_beta_0)\r\n\r\n[1] 3.813111\r\n\r\nOtrzymaliśmy wartość \\(BF_{10}\\) wskazującą to, że porównując te dwa modele, jest ca. 4 razy bardziej prawdopodobne, że \\(M_1\\) wyprodukował obserwowane dane niż \\(M_0\\). Dużo? Mało? Jedna z propozycji interpretacji \\(BF\\) jest następująca (Kass & Raftery, 1995):\r\n\r\nBF\r\nDowody\r\n< 1\r\nWspierające model zerowy\r\n1-3\r\nAnegdotyczne\r\n3-10\r\nZnaczne\r\n10-100\r\nSilne\r\n> 100\r\nDecyzyjne\r\n\r\nLikelihood sampling\r\nW przypadku gdy nasze modele nie są zagnieżdżone, musimy szukać innych metod. Jedną z nich jest naiwne symulowanie metodą Monte Carlo. Opiera się na pomyśle, że \\(P(Y|M_i)\\) możemy przybliżyć w następujący sposób:\r\n\\[P(D \\mid M_i) = \\int P(D \\mid \\theta, M_i) \\ P(\\theta, M_i) \\ \\text{d}\\theta \\approx \\frac{1}{n} \\sum^{n}_{\\theta_i \\sim P(\\theta \\mid M_i)} P(D \\mid \\theta, M_i)\\] Popatrzmy na naszą całkę. Jeśli potraktujemy \\(P(D \\mid \\theta, M_i)\\) jako zmienną \\(x\\), a \\(P(\\theta, M_i)\\) jako związaną z \\(x\\) funkcję prawdopodobieństwa \\(P(X)\\) (którą w istocie jest), otrzymamy:\r\n\\[P(D \\mid M_i) = \\int x \\ P(x) \\ \\text{dx}\\]\r\nCzyli wzór na średnią. Możemy więc przybliżyć wartość \\(P(D|M_i)\\) losując najpierw \\(\\theta\\) z rozkładu a priori, następnie obliczając \\(P(D \\mid \\theta, M_i)\\) poprzez wstawienie uzyskanej \\(\\theta\\). Jeśli powtórzymy to wielokrotnie, a otrzymane prawdopodobieństwa uśredniamy, uzyskamy estymatę \\(P(D \\mid M_i)\\). Takie losowanie również możemy wykonać przy pomocy JAGS, jednak uzyskany \\(BF\\) nie jest zbyt stabilny. Dlatego stosuje się udoskonalone metody losowania takie jak np. Bridge Sampling. Nie będziemy jednak ich teraz dokładnie omawiać.\r\nPorównywanie modeli?\r\nCzynnik Bayesa wydaje się być niezłą metodą, którą można wykorzystać do wnioskowania statystycznego. Być może nawet pomoże nam on rozwiązać kryzys replikacyjny. Tak też pomyślało wielu, co skutkowało tym, że programy statystyczne takie jak np. JASP zwracają nam podsumowania bayesowskich analiz defaultowo w postaci Czynnika Bayesa. Również czasopisma naukowe często oczekują od autorów, jeśli Ci posługują się statystyką bayesowską, wyników w postaci czynników Bayesa.\r\nNiektóre czasopisma wręcz wymagają czynników Bayesa jako metody przeciwdziałającej kryzysowi replikacyjnemu (pisałem o kryzysie replikacyjnym trochę tutaj). Skoro określono algorytm weryfikacji hipotez statystycznych w statystyce bayesowskiej (rzekomo skuteczniejszy od statystyki częstosciowej), możemy wreszcie wrócić do pracy, nie martwiąc się o problem replikacji.\r\nNiestety, jak to zwykle okazuje, rzeczywistość nie jest taka różowa. Czynnik Bayesa jest kontrowersyjny wśród statystyków. Różnice w podejściach możemy zauważyć na przykład w zaciętych dysputach Davida Mackaya z Adrew Gelmanem. Mackay napisał doktorat wykazujący właściwość czynnika bayesa do karania skomplikowanych modeli, którą omówiliśmy wcześniej. Andrew Gelman i inni wykazali wiele niedociągnięć i problemów związanych z czynnikiem Bayesa zwłaszcza w kontekście Bayesian Point Null Hypothesis Testing.\r\nGrafika skradziona ze slajdów Richard E. TurneraRzućmy okiem na kilka najważniejszych zastrzeżeń. To, co ważne, to fakt nie wszystkie z nich są na pierwszy rzut oka oczywiste.\r\nCzynnik Bayesa nie mierzy czy model jest prawdziwy\r\nCzynnik bayesowski jest stosunkiem marginalnej wiarygodności dwóch modeli, czyli o ile bardziej jest prawdopodobne, że jeden z modeli wyprodukował obserwowane dane niż drugi. Nie mówi nam on w żaden sposób o tym, jak dobre te modele w absolutnym sensie (Gelman & Rubin, 1995). Możemy być w sytuacji, w której czynnik Bayesa bardzo faworyzuje jeden z modeli, ale oba modele są fatalnie dopasowane do danych.\r\nW konsekwencji jeśli stosujemy sposób wnioskowania oparty na testowaniu hipotezy zerowej i alternatywnej (tak jak zrobiliśmy wyżej w przypadku Anovy), zasadniczo czynnik Bayesa nie mówi nam więcej niż p-value bez spojrzenia na wielkość efektu. By zbadać dopasowanie modelu potrzebujemy więc przeanalizować rozkłady posteriori parametrów.\r\nCzynnik Bayesowski jest wrażliwy na rozkłady a priori parametrów modelu\r\nRozkład posteriori parametrów \\(\\theta\\) jest niezależny od rozkładów a prior tychże, gdy liczba obserwacji dąży do nieskończoności.\r\n\\[\\lim_{n\\to\\infty}P(\\theta|D,M) \\propto P(D|\\theta, M)\\]\r\nTak jak pokazaliśmy w pierwszej części tutorialu, gdy liczba obserwacji wzrasta, wpływ rozkładów a priori maleje. Jak jest jednak w przypadku czynnika Bayesa? Przyjmując pewne upraszczające założenia (MacKay, 2003), możemy powiedzieć, że marginalna wiarygodność to:\r\n\\[P(D|M) \\approx P(D|\\theta_{map}, M)P(\\theta_{map}|M)\\sigma_{\\theta|D}\\] Gdzie \\(\\theta_{map} = \\arg \\max_{\\theta} P(\\theta|D,M)\\) to maximum posteriori approximation, czyli takie \\(\\theta\\), dla którego prawdopodobieństwo a posteriori jest największe. Z kolei \\(\\sigma_{\\theta|D}\\) to odchylenie standardowe \\(P(\\theta|D,M)\\).\r\nW konsekwencji czynnik bayesowski zależy od a priori rozkładów parametru modelu \\(P(\\theta_{map}|M)\\). W przypadku gdy użyjemy nieinformatywnego rozkładu a priori dla \\(\\theta\\), rozkładu jednostajnego o szerokości \\(\\sigma_{\\theta}\\), wtedy wzór upraszcza się do\r\n\\[P(D|M) \\approx P(D|\\theta_{map}, M)\\frac{\\sigma_{\\theta|D}}{\\sigma_{\\theta}}\\]\r\nIm bardziej chcemy by dane przemówiły za siebie (im bardziej nieinformatywny rozkład a priori zastosujemy), tym bardziej faworyzowany będzie model zerowy w przypadku Bayesian Point Null Hypothesis Testing. Wrażliwość na rozkłady a priori nie jest tak istotnym problemem, gdy porównujemy ze sobą niezagnieżdżone modele to znaczy w przypadku, gdy testujemy dwa alternatywne wyjaśnienia zjawiska.\r\nRozkłady posteriori parametrów nie muszą się zgadzać z czynnikiem Bayesa\r\nW statystyce częstościowej p-value < 0.05 dla parametru oznacza odrzucenie hipotezy zerowej. Jednak w przypadku analizy bayesowskiej możemy spotkać się z sytuacjami, w których czynnik Bayesa sugeruje przyjęcie modelu zerowego, jednak interesujący nas parametr w modelu alternatywnym nie zawiera w 95% przedziale wiarygodności, i vice versa (Kruschke & Liddell, 2018). Ta własność czynnika Bayesa jest często nieintuicyjna, ponieważ przenosimy nasze oczekiwania ze statystyki częstościowej.\r\nCzynnik Bayesa a prawdopodobieństwo braku efektu\r\nCzęsto się mówi, że by wnioskować o braku efektu musimy użyć statystyki Bayesowskiej, ponieważ w statystyce klasycznej możemy mówić co najwyżej o braku dowodów na istnienie efektu. Niestety, przypadku BPNHT, pojawia się pewien problem, ponieważ czynnik Bayesa jest wrażliwy na wielkość próby (Morey & Rouder, 2011).\r\nBy zobaczyć to na własne oczy posłużymy się przykładem. Załóżmy, że zebraliśmy dane, które mają rozkład normalny \\(y \\sim N(\\mu,\\sigma)\\), ze znanym odchyleniem standardowym \\(\\sigma = 5\\). Chcemy przetestować dwa modele w myśl BPNHT: \\(M_0: \\mu = 0\\) i \\(M_1: \\mu \\sim U(-\\infty,+\\infty)\\). \\(U(a,b)\\) oznacza rozkład jednostajny w przedziale między \\(a,b\\).\r\nDokładny czynnik Bayesa dla tych dwóch modeli, dane jest wzorem1:\r\n\\[BF_{10} = \\frac{\\sqrt{2\\pi}\\sigma}{\\sqrt{n}}e^{(\\frac{\\overline{\\text{Y}}^2n}{2\\sigma})}\\] Gdzie \\(\\overline{\\text{Y}}\\) to średnia w próbie. Już we wzorze możemy zobaczyć, że ta wartość będzie coraz większa, gdy \\(n\\) rośnie. Zobaczmy na wykresie jak wyglądają wartości czynnika Bayesa dla ustalonych wartości \\(\\overline{\\text{Y}}\\):\r\n\r\n\r\nBF = function(sample_size,sample_mean, sd){\r\n  (sqrt(sample_size)/(sqrt(2*pi)*sd))*exp(((sample_mean^2)*sample_size)/(2*sd^2))\r\n}\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x))+ \r\n  stat_function(fun = BF,args = list(sample_mean = 0.6,sd = 5), aes(color = \"0.6\")) + \r\n  stat_function(fun = BF,args = list(sample_mean = 0.75,sd = 5), aes(color = \"0.75\")) +\r\n  stat_function(fun = BF,args = list(sample_mean = 0.9,sd = 5), aes(color = \"0.9\")) +\r\n  xlim(0,100) +\r\n  xlab(\"N\") + \r\n  ylab(bquote(BF[10])) + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"Sample Mean\", values = c( \"blue\", \"green\", \"red\")) +\r\n  geom_hline(yintercept = 1) +\r\n  theme_light()\r\n\r\n\r\n\r\nOk, im większa próba, tym większe wsparcie dla hipotezy alternatywnej. Choć spojrzeliśmy tylko na dwa partykularne modele, ta własność utrzymuje się generalnie w całym BPNHT (Johnson & Rossell, 2010). Czy to źle? Niekoniecznie, ale to znaczy, że podobnie jak w przypadku p-value, tym mniejszy efekt jest potrzebny by odrzucić hipotezę zerową, im większa próba. Dlatego też ciężko jest wnioskować o braku efektu, ponieważ nie wiemy czy czynnik Bayesa faworyzuje model zerowy, ponieważ jest lepszy, czy dlatego, że nie ma wystarczających dowodów by go odrzucić.\r\nNie oznacza jednak, że statystyka Bayesowska nam na to nie pozwala. Jeśli rozkład posteriori parametru jest silnie skoncentrowany blisko zera, możemy mówić, że pod warunkiem danych mamy dużą pewność braku efektu lub marginalnego efektu.\r\nI co z tym wszystkim zrobić?\r\nDogłębne omówienie wyżej wymienionych problemów możecie znaleźć w artykule “A Review of Issues About Null Hypothesis Bayesian Testing” (Tendeiro & Kiers, 2019).\r\nAlternatywna rzeczywistość, w której, w miejsce statystyki częstościowej, statystyka Bayesowska zdominowała naukę. Grafika skradziona ze slajdów Richarda Moreya.Z jednej strony czynnik Bayesa pozwala na bezpośrednie porównanie prawdopodobieństwa wyprodukowania danych przez cały model, a nie tylko punktowe wartości parametrów. Jednocześnie w naturalny sposób karze złożoność modelu, co w innych miarach dopasowania modelu takich jak na przykład AIC, próbujemy robić estymując złożoność poprzez liczbę wolnych parametrów.\r\nJednakże, jak widzimy, czynnik Bayesa jest miarą, która również słabe strony, nie jest więc cudownym lekiem na statystyczne bolączki. Z tego miejsca mamy dwie możliwości. Możemy używać czynnika Bayesa rozważnie, czyli:\r\nOstrożnie dobierać rozkłady a priori parametrów modelu\r\nPrzeprowadzać analizę wrażliwość (sensitivity analysis), tzn. sprawdzić jak bardzo wartość czynnika Bayesa ulega zmianie, gdy użyjemy innych rozkładów a priori parametrów. Niestety, konsekwencją tego, że czynnik Bayesa jest trudny do policzenia, jest to żmudny proces.\r\nNie raportować czynnika Bayesa samotnie, ale także miary rozkładu parametrów a posteriori.\r\nAlbo\r\nunikać porównywania modeli. Zamiast tego, na pierwszym miejscu skupić się na tworzeniu wiarygodnych modeli wspieranych przez teorię, przełożyć nacisku na estymację parametrów, tzn. zamiast porównywać interesujący nas model do modelu zerowego z wartością parametru równą 0, skupić się na ewaluacji rozkładu a posteriori parametrów. Wykorzystać Posterior Predictive Testsing, które może nam wskazać problemy z naszym modelem, bez konieczności porównywania go z innym. Wykorzystywać porównywanie modeli wtedy, gdy mamy dwa lub więcej podyktowane teorią konkurujące modele pewnego zjawiska np. pamięci, lecz nie po to by w przy pomocy BPNHT testować zasadność pojedynczego modelu. Co istotne, do tego ostatniego możemy posłużyć się statystykami prostszymi do obliczenia, jak np. DIC albo WAIC.\r\nZakończenie\r\nDowiedzieliśmy się czym jest czynnik Bayesowski i przy okazji omówiliśmy kontrowersje wokół niego. Mamy już intuicję na temat statystyki Bayesowskiej, estymacji modeli i wnioskowania. Fajnie, by było wreszcie coś policzyć po Bayesowsku co nie jest prostym modelem liniowym. Dlatego w następnej części przyjrzymy się jednej z ciekawszych możliwości statystyki Bayesowskiej - modelom hierarchicznym.\r\nNa koniec dodam, że czynnik Bayesowski, p-value ani żadna inna statystyka nie jest rozwiązaniem kryzysu replikacyjnego. Nie dlatego, że te miary obarczone są wadami, tylko dlatego, że prawdziwe powody leżą gdzie indziej. Badacze czasami naiwnie stosują statystykę, czasami recenzenci wymagają od badaczy stosowania utartych, choć nieadekwatnych procedur. Dodatkowo system ewaluacji pracowników naukowych wymaga od nich by publikowali dużo, a czasopisma naukowe wymagają by publikowali badania świadczące o istnieniu efektu, raczej niż o jego braku. Bez zaadresowania tych problemów, nie sądzę by jakakolwiek statystyczny test rozwiązał problem replikacji w nauce.\r\n\r\n\r\n\r\nGelman, A., & Rubin, D. B. (1995). Avoiding model selection in bayesian social research. Sociological Methodology, 25, 165–173.\r\n\r\n\r\nJohnson, V. E., & Rossell, D. (2010). On the use of non-local prior densities in bayesian hypothesis tests. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(2), 143–170.\r\n\r\n\r\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430), 773–795.\r\n\r\n\r\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a bayesian perspective. Psychonomic Bulletin & Review, 25(1), 178–206.\r\n\r\n\r\nMacKay, D. J. C. (2003). Model comparison and occam’s razor. In Information theory, inference and learning algorithms. Cambridge university press.\r\n\r\n\r\nMorey, R. D., & Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. Psychological Methods, 16(4), 406.\r\n\r\n\r\nTendeiro, J. N., & Kiers, H. A. (2019). A review of issues about null hypothesis bayesian testing. Psychological Methods, 24(6), 774.\r\n\r\n\r\nDowód możecie znaleść w suplemencie B do tego artykułu (Tendeiro & Kiers, 2019)↩︎\r\n",
    "preview": "posts/2023-01-12-bayes-factor/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-01-19T12:30:40+01:00",
    "input_file": "bayes-factor.knit.md"
  },
  {
    "path": "posts/2022-12-26-poznajcie-jags/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część 2.5: Poznajcie JAGS",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-12-26",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nJAGS\r\nJAGS in JASP\r\nBonus - diagramy Bayesowskie\r\n\r\nDzisiejszy wpis będzie krótki. Pokażę wam jak zrobić to co robiliśmy w poprzedniej części przy użyciu oprogramowania, które pozwoli nam na tworzenie i estymowanie (przy użyciu MCMC) nawet bardzo skomplikowanych modeli w łatwy sposób. Możliwości jest kilka, ja przedstawie wam JAGS (Just Another Gibbs Sampler). Jest to samodzielne oprogramowanie, które można używać bezpośrednio lub za pomocą języków programowania takich jak R, a także graficznego programu statystycznego JASP. Pobrać możecie go tutaj.\r\nJAGS\r\nPowtórzmy naszą bayesowską regresję z tym razem przy użyciu JAGS. Stwórzmy zbior danych:\r\n\r\n\r\nlibrary(rjags)\r\nlibrary(coda)\r\nlibrary(MCMCvis)\r\n\r\nx = rnorm(100,0,10)\r\ny = 5 + x + rnorm(100,0,10)\r\ndata = data.frame(y,x)\r\n\r\n\r\nMusimy zdefiniować kod naszego modelu. JAGS używa dosyć intuicyjnego kodowania BUGS. Model regresji będzie wyglądał tak:\r\n\r\n\r\nmod = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(y)) {\r\n    y[i] ~ dnorm(a + b * x[i], sigma^-2)\r\n  }}\"\r\n\r\n\r\nOperator ~ oznacza, że parametr po lewej stronie dany jesr rozkładem po prawej. Ponieważ w naszym kodzie tylko zmienna Y występuje w danych, JAGS automatycznie rozpozna, że zdefiniowany rozkład Y jest funkcją wiarygodności. Zauważcie, że gdy definuje b jako wartość losowaną z rozkładu normalnego \\(N(0,100)\\), jako drugi argument podałem \\(100^{-2}\\). JAGS zamiast odchylenia standardowego dla rozkładu normalnego przyjmuje precyzję (precision), czyli odwrotność wariancji. Po więcej szczegółów polecam zajrzeć do dokumentacji.\r\n\r\n\r\n# Parametry, które chcemy śledzić. \r\nparams = c(\"a\",\"b\",\"sigma\")\r\n\r\n## Hiperparametry\r\nn.adapt = 100\r\n# Liczba iteracji adaptacji\r\nni = 3000\r\n# Liczba iteracji \"wypalania\"\r\nnb = 3000\r\n# Liczba próbek z rozkładu post priori\r\nnt = 1\r\n# Odchudzanie - 1 oznacza, bierzemy każdą próbkę z rozkładu post priori\r\nnc = 3\r\n# liczba łańcuchów\r\n\r\n# Inicjacja modelu\r\njmod = jags.model(file = textConnection(mod), data = data, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\n\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 100\r\n   Unobserved stochastic nodes: 3\r\n   Total graph size: 412\r\n\r\nInitializing model\r\n\r\n# Wypalanie\r\nupdate(jmod, n.iter=ni, by=1)\r\n\r\n# Losowanie próbek z rozkładu post priori\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\nZauważcie, że Jags miał tylko 3000 iteracji wypalania, podczas gdy mój kod z poprzedniej części potrzebował ponad 16 razy więcej. JAGS używa kombinacji różnych agorytmów MCMC, ponado dokonuje za nas tunningu hiperparametrów (takich jak na przykład step size w Metropolis-Hasting). Dlatego, oprócz wypalania i losowania, mamy jeszcze adaptację. Obejrzmy sobie rozkłady post priori:\r\n\r\n\r\nplot(post)\r\n\r\n\r\n\r\nPoliczmy statystyki:\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:6100\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n        Mean     SD Naive SE Time-series SE\r\na      6.568 1.1387 0.012003       0.012646\r\nb      1.001 0.1134 0.001195       0.001240\r\nsigma 11.547 0.8289 0.008738       0.009381\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%    25%    50%    75%  97.5%\r\na      4.3108  5.806  6.584  7.348  8.801\r\nb      0.7762  0.925  1.000  1.077  1.223\r\nsigma 10.0485 10.966 11.501 12.077 13.308\r\n\r\nA także sprawdźmy czy łańcuchy się zbiegły:\r\n\r\n\r\ngelman.diag(post)\r\n\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\na              1          1\r\nb              1          1\r\nsigma          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\n\r\nStatystyka Gelmana-Rubina wskazuje, że łańcuchy się zbiegły. Sprawdźmy jeszcze czy aby na pewno stastystyka nie osiągneła takich wartości przez przypadek.\r\n\r\n\r\ngelman.plot(post)\r\n\r\n\r\n\r\nNa wykresie widzimy statystykę Gelmana Rubina liczną dla każdych 50 iteracji następujących po sobie. Dzięki temu możemy sprawdzić czy nasze próby wylosowane są z rozkładów, które naprawdę się zbiegły. Widzimy, że dla początkowych wartości łancuchów statystyka ma wyższe wartośći, co może sugerować, że powinniśmy zastosować dłuższy interwał wypalania (choć w naszym przypadku statystyka nie przekracza nigdzie wartości 1.15, więc bybyłby to raczej zabieg kosmetyczny).\r\nSprawdźmyy autokorelację:\r\n\r\n\r\nautocorr.diag(post)\r\n\r\n                  a           b         sigma\r\nLag 0   1.000000000 1.000000000  1.0000000000\r\nLag 1   0.052107838 0.036227109  0.0698947033\r\nLag 5   0.004006012 0.006658263 -0.0043778557\r\nLag 10  0.005318083 0.015748954  0.0007391561\r\nLag 50 -0.000887632 0.009502967 -0.0113052665\r\n\r\nAutokorelacja praktycznie nie występuje.\r\nJAGS poradził sobie dużo lepiej z regresją liniową, niż mój zabawkowy kod.Ponadto jego składnia jest relatywnie prosta. Dlatego z niego będziemy korzystać w następnych częściach tutorialu.\r\nPrzykładowe kody JAGS dla wielu modeli możecie znaleść tutaj.\r\nJAGS in JASP\r\nJeśli ktoś nie jest wielkim fanem robienia statystyk za pomocą języków programowania, JAGS jest kompatybilny z graficznym programem statystycznym JASP.\r\n\r\nBonus - diagramy Bayesowskie\r\nWarto wspomnieć, że do wizualizacji modeli bayesowskich często używa się grafów (ja do ich budowy używam biblioteki daft w Pythonie). W przypadku naszego modelu:\r\n\r\nimport daft\r\nimport matplotlib.pyplot as plt \r\n\r\npgm = daft.PGM(observed_style=\"inner\")\r\n\r\npgm.add_node(\"alpha\", r\"$\\alpha$\", 0.5, 2)\r\npgm.add_node(\"beta\", r\"$\\beta$\", 1.5, 2)\r\npgm.add_node(\"sigma\", r\"$\\sigma$\", 2.5, 2)\r\npgm.add_node(\"x\", r\"$x_i$\", 2, 1, observed=True)\r\npgm.add_node(\"y\", r\"$y_i$\", 1, 1, observed=True)\r\n\r\npgm.add_edge(\"alpha\", \"y\")\r\npgm.add_edge(\"beta\", \"y\")\r\npgm.add_edge(\"x\", \"y\")\r\npgm.add_edge(\"sigma\", \"y\")\r\n\r\npgm.add_plate([0.5, 0.5, 2, 1], label=r\"$i = 1, \\ldots, N$\", shift=-0.1)\r\n\r\npgm.render()\r\nplt.show() \r\n\r\n\r\n\r\n\r\nDiagram pokazuje, że rozkład zmiennej \\(y_i\\) definują 3 nieobserwowalne parametry (pojedyńcze okręgi) i jedna obserwowalna zmienna (pogrubiony okrąg).\r\nW przypadku modeli bayesowskich zapewno dosyć często będziecie się spotykać z takimi graficznymi opisami modeli. Do grafu dołączane są zwykle definicje parametrów. W naszym przypadku:\r\n\\[\\alpha \\sim Uniform(-1000, 1000)\\] \\[ \\beta \\sim N(0, 100) \\] \\[ \\sigma \\sim Uniform(0.000001,1000)\\] \\[ y_i \\sim N(a + b * x_i, \\sigma)\\]\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-26-poznajcie-jags/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-01-19T12:06:00+01:00",
    "input_file": "poznajcie-jags.knit.md"
  },
  {
    "path": "posts/2022-10-30-tutorial-bayes-ii/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część II: Estymacja modeli bayesowskich",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-10-30",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWstęp\r\nMCMC\r\nPrzykład wizualny\r\nPrzykład praktyczny - Bayesowska Regresja Liniowa\r\n\r\nDiagnostyka\r\nGelman-Rubin Convergence Diagnostic\r\nAutokorelacja\r\nPosterior predictive check\r\n\r\nPodsumowanie\r\n\r\n\r\n\r\n.MathJax_Display, .MJXc-display, .MathJax_SVG_Display {\r\n    overflow-x: auto;\r\n    overflow-y: hidden;\r\n}\r\n\r\n\r\nPost będzie aktualizowany.\r\n\r\nWstęp\r\nW poprzedniej części dowiedzieliśmy się na czym polega wnioskowanie bayesowskie. W następnych częściach będziemy budować modele bayesowskie i będziemy używać do tego dedykowanych pakietów i oprogramowania. Dlatego dzisiaj dowiemy się jak od wewnątrz wygląda estymacja rozkładów post priori, gdy nie jest to możliwe metodą analityczną.\r\nZałóżmy, że zdefiniowaliśmy sobie jakąś funkcję wiarygodności i rozkłady a priori parametrów \\(\\theta\\). Chcemy numerycznie przybliżyć rozkład \\(P(\\theta|y)\\)\r\n\\[P(\\theta|y) \\propto P(y|\\theta)P(\\theta)\\]\r\nIstnieją dwie najpopularniejsze metody numerycznej estymacji modeli bayesowskich - MCMC i Variational Bayes.\r\nVariational Bayes to metoda, która zmniejsza wymagania obliczenowe przyjmując pewne upraszczające założenia, dzięki czemu można ją efektywnie stosować do dużych zbiorów danych.\r\nMCMC (Markov Chain Monte Carlo) jest metodą która jest dokładniejsza, ale bardziej czasochłonna i to ją dziś wam przedstawię.\r\nMCMC\r\nZastanówmy się przez chwilę nad problemem znalezienia rozkładów post priori. Gdybyśmy szukali najlepszych estymat punktowych parametrów, jak w częstościowym wnioskowaniu, moglibyśmy ewaluować naszą funkcję \\(P(\\theta|y)\\) i szukać takich \\(\\theta\\) dla których przyjmuje najwyższą wartość.\r\nMoglibyśmy to na przykład zrobić w ten sposób. Dla funkcji \\(f(x)\\) zaczynamy z losową wartością \\(x\\). W następnym kroku dodajemy małą losową wartość \\(\\Delta\\) do \\(x\\) i sprawdzamy czy \\(f(x + \\Delta)\\) jest większe. Jeśli tak, \\(x + \\Delta\\) staje się naszym \\(x\\). Jeśli nie, powtarzamy porzednie kroki. Zatrzymujemy się wtedy, gdy \\(x\\) przestanie się zmieniać.\r\nTen algorytm ma pewną wadę, ponieważ odszuka on najbliższe lokalne maksimum, które nie musi być globalnym. Do tego jeszcze wrócimy.\r\nW podejsciu bayesowskim szukamy rozkładu \\(P(\\theta|y)\\). Moglibyśmy go przybliżyć losując próbę z niego. Ale jak wylosować próbę z rozkładu, którego nie znamy? Możemy najpier poszukać środka rozkładu za pomocą wyżej opisanej metody. Gdy już go znajdziemy, możemy zasymulować losowanie w prosty sposób. Jeśli \\(P(\\theta|y)\\) ma wartość 0.2 w pewnym miejscu rozkładu, a w 0.1 w innym, to wiemy że pierwsza wartość \\(\\theta\\) musi występować średnio dwa razy częściej.\r\nMożemy więc przyjąć następującą strategię:\r\nZainicjować losowo pierwszą wartość \\(\\theta\\) i obliczyć \\(P(\\theta|y)\\).\r\nPrzesunąć się o losową wartość \\(\\Delta\\) (losowaną z rozkładu normalnego, którego SD jest wielkością przesunięcia - step size) i obliczyć \\(P(\\theta + \\Delta|y)\\).\r\nJeśli \\(P(\\theta + \\Delta|y)\\) jest większe od \\(P(\\theta|y)\\) przyjmujemy \\(\\theta + \\Delta\\) jako losową próbkę z rozkładu.\r\nJeśli \\(P(\\theta + \\Delta|y)\\) jest mniejsze od \\(P(\\theta|y)\\) losujemy wartość między z przedziału [0,1].\r\nPrzyjmujemy \\(\\theta + \\Delta\\) jako losową próbkę z rozkładu jeśli wylosowana wartość jest mniejsza od \\(\\frac{P(\\theta + \\Delta|y)}{P(\\theta|y)}\\).\r\nPowtarzamy procedurę od punktu 2, jeśli wylosowana wartość jest większa od \\(\\frac{P(\\theta + \\Delta|y)}{P(\\theta|y)}\\).\r\nPrzyjąć \\(\\theta+\\Delta\\) jako nową wartośc \\(\\theta\\) i powtórzyć procedurę od punktu 2.\r\nTa wersja MCMC to algorytm Metropolisa-Hastingsa. Żeby być rzetelnym, prawdopodobnie musiałbym zacząć od przedstawienia wam czym jest proces stochatyczny, łańcuchy Markova, rozkład stacjonarny, et cetera. Ale ponieważ chcę się skupić na pokazaniu jak wygląda estymacja MCMC w praktyce, pominę to. Dowód i opis matematyczny działania algorytmu Metropolisa-Hastingsa możecie znaleźć tutaj.\r\nSkąd wiemy, że losujemy z właściwej przestrzeni parametrów rozkładu? Żeby wyjaśnić ten termin pójrzmy na wykres poniżej.\r\n\r\n\r\n\r\nJak widzimy, ten rozkład prawdopodbieństwa ma dwa maksima. Chcielibyśmy, by rozkład post priori był odwzorowany jak najdokładniej. Jednak nasz algorytm może w utknąć makismum po lewej, dając nam błędne przybliżenie rozkładu.\r\nCzy nasz algorytm jest w stanie poradzić sobie z tym? Odowiedź brzmi: Wykonując nieskończoną liczbę iteracji, wartości zbiegną do właściwej przestrzeni rozkładu. Dzieje się tak dlatego, że nasz algorytm może poruszać się także po mniejszych wartościach \\(P(\\theta|y)\\), jest więc w stanie opuścić lokalne maksima.\r\nMy jednak nie dysponujemy nieskończoną liczbą iteracji. Dlatego algorytm powtarzamy \\(n\\) razy. Każdą instancję nazywamy łańcuchem, a ponieważ każdy łańcuch rozpoczyna w innym losowym miejscu przestrzeni parametrów, jeśli łańcuchy się zbiegną, możemy domniemywać, że odnaleźliśmy właściwą przestrzeń i zacząć losować próbki z naszego rozkładu.\r\nPrzykład wizualny\r\nZobaczmy to na przykładzie, który kiedyś podpatrzyłem i wydaje mi się idealny do zaprezentowania działania MCMC. Weźmy sobie pewną funkcję, która jako argumenty bierze \\(x\\) i \\(y\\).\r\n\r\n\r\nf = function (x,y) {\r\n  return(20*exp(-0.2*sqrt((x^2+y^2)/2))+exp(0.5*(cos(2*pi*x)+cos(2*pi*y))))\r\n}\r\n\r\nlibrary(lattice)\r\npoints = matrix(nrow = 61, ncol = 61, seq(-3,3,0.1)) \r\nfilled.contour(x = points[,1], y = points[,1], z = f(points, t(points)), nlevels=20)\r\n\r\n\r\n\r\nJak widzimy, nasza funkcja ma wiele lokalnych maksimów, ale tylko jedno globalne na środku. Sprawdźmy jak poradzi sobie nasz algorytm. Uwaga, funkcja, której używamy nie jest rozkładem prawdopodbieństwa, dlatego algorytm wygląda trochę inaczej niż w podpunktach przedstawionych wcześniej.\r\n\r\n\r\njmp = 0.2 # step size\r\niter = 10000 #liczba iteracji algorytmu\r\nchain = 3 # liczba łańcuchów\r\n\r\nsamples =  array(NA, c(10000,3,3)) # Pusty tensor na nasze próbki\r\n\r\n# Wartości początkowe łańcuchów\r\nsamples[1,1,1] = 2.3 \r\nsamples[1,2,1] = 1.98\r\nsamples[1,1,2] = -2.3  \r\nsamples[1,2,2] = -1.98\r\nsamples[1,1,3] = 1 \r\nsamples[1,2,3] = -2.5\r\n\r\nfor (j in 1:chain){\r\nsamples[1,3,j] = f(samples[1,1,j], samples[1,2,j]) # ewaluujemy naszą funkcję dla wartości\r\n# początkowych łańcuchów\r\nn = 1\r\n\r\nwhile(n <= (iter-1)) { #Iterujemy dopóki nie zbierzemy 'iter' próbek\r\n  new_x = rnorm(1, samples[n,1,j], jmp) # przemieszczamy się o losową wartość dla wartości x\r\n  new_y = rnorm(1, samples[n,2,j], jmp) # przemieszczamy się o losową wartość dla wartości y\r\n  \r\n  if (exp(-12*(f(samples[n,1,j], samples[n,2,j])-f(new_x, new_y))) > runif(1,0,1)) {\r\n  # Sprawdzamy czy nowe wartości zwracają wyzszą wartość naszej funkcji. Poniweważ funkcja nie\r\n  # jest rozkładem prawdopodobieństwa nie możemy policzyć stosunku prawdopodobieńst. Zamiast\r\n  # tego używamy funkcji eksponencjalnej różnicy pomnożonej przez ujemną stałą.  \r\n    \r\n    n = n + 1\r\n    samples[n,1,j] = new_x\r\n    samples[n,2,j] = new_y\r\n    samples[n,3,j] = f(new_x, new_y)\r\n\r\n  }   \r\n}\r\n}\r\n\r\n\r\nZobaczmy jak poradziły sobie nasze łańcuchy.\r\n\r\n\r\nfilled.contour(x=points[,1], y=points[,1], z =f(points,t(points)), nlevels=20, plot.axes = {\r\n  axis(1); axis(2); lines(samples[1:10000,1:2,1], col =\r\n  \"black\");lines(samples[1:10000,1:2,2], col = \"blue\"); lines(samples[1:10000,1:2,3], col = \r\n  \"green\") })\r\n\r\n\r\n\r\nŁańcuchy znajdowały na swojej drodze lokalne maksima, w których pozostawały na jakiś czas, po czym zmierzały do następnych. Ostatecznie wszystkie zbiegły się do lokalnego maksimum.\r\nDobrze. Wiemy, że nasz algorytm znalazł globalne maksimum, ponieważ znamy funkcję, której ekstremum szukamy. Jednak w przeciwieństwie do powyższego przykładu, zwykle nie znamy funkcji generującej rozkład post priori - dopiero staramy się go estymować. Jak więc sprawdzić czy znaleźliśmy własciwą przestrzeń losowania? Spójrzmy na pierwsze 200 interacji dla argumentu \\(x\\).\r\n\r\n\r\nx_samples = as.data.frame(samples[1:iter,1,1:3])\r\nx_samples$iteration = 1:dim(samples)[1]\r\ncolnames(x_samples) = c(1,2,3,\"Iteration\")\r\nx_samples_burn_in = x_samples[1:200,]\r\nx_samples_in_space = x_samples[2000:2500,]\r\nx_samples_burn_in = pivot_longer(x_samples_burn_in,cols =1:3, names_to = \"chain\", values_to = \"x\")\r\nx_samples_in_space = pivot_longer(x_samples_in_space,cols =1:3, names_to = \"chain\", values_to = \"x\")\r\n\r\nggplot(x_samples_burn_in, mapping = aes(x = Iteration, y = x, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWidzimy, że każdy łańcuch oscyluje wokół innej wartości. Teraz spójrzmy na iteracje między 2000 a 2500.\r\n\r\n\r\nggplot(x_samples_in_space, mapping = aes(x = Iteration, y = x, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWidzimy, że wszytkie łańcuchy się zbiegły. Co prawda, nie daje nam to całkowitej pewności, że łańcuchy odnalazły właściwą przestrzeń.\r\nDziałanie algorytmu MCMC dzieli się na dwie części: wypalanie (burn-in) i losowanie z rozkładu post priori. Wypalamy łańcuchy dopóki się nie zbiegną, wtedy możemy rozpocząć estymacje rozkładu post priori z próbek z łańcuchów. Im większej liczby łańcuchów użyjemy, tym silniejszą mamy przesłankę (gdy wszystkie się zbiegną), że odnaleźliśmy właściwą przestrzeń losowania.\r\nPrzykład praktyczny - Bayesowska Regresja Liniowa\r\nNabyliśmy już intuicję, jak działa MCMC. Zabierzmy się więc za prawdziwy statystyczny problem.\r\nPowiedzmy, że chcemy policzyć bayesowską regresję liniową zmiennej \\(y\\) ze względu na \\(x\\):\r\n\\[y_i = \\alpha + \\beta x_i + \\epsilon{_i}\\] gdzie \\(\\alpha\\) to stała, \\(\\beta\\) to współćzynnik regresji a \\(\\epsilon{_i}\\) to błąd.\r\nWytwórzmy sobie przykładowe dane:\r\n\r\n\r\nx = rnorm(100,0,10)\r\ny = 5 + x + rnorm(100,0,10)\r\n\r\n\r\nW ujęciu Bayesowskim będziemy szukali rozkładów post priori interesujących nas parametrów. Naszą zmienną zależną zamodelujemy w następujący sposób:\r\n\\[ y_i \\sim N(\\alpha + \\beta x_i, \\sigma)\\]\r\nCo oznacza, że wartość zmiennej zależnej u każdej obserwacji jest wylosowana z rozkładu normalnego o średniej \\(\\alpha + \\beta x_i\\) i odchyleniu standardowym \\(\\sigma\\). Jednocześnie, jak widzimy, jest to nasza funkcja wiarygodności \\(P(y|\\theta)\\) - prawdopodbieństwo otrzymania \\(y\\) pod warunkiem parametrów \\(\\alpha\\), \\(\\beta\\) i \\(\\sigma\\) zapisanych w skrócie jako wektor \\(\\theta\\). Musimy zdefiniować jeszcze rozkłady a priori dla parametrów \\(\\theta\\) i możemy przystąpić do obliczania rozkładów post priori interesującyh nas parametrów. W tym przykładzie użyjemy nieinformatywnych rozkładów a priori. Zamodelujemy \\(\\alpha\\) i \\(\\sigma\\) rozkładami jednostajnymi, a \\(\\beta\\) rozkładem normalnym o średniej 0.\r\nZaimplementujmy algorytm Metropolisa-Hastingsa by wyestymować rozkłady parametrów regresji.\r\n\r\n\r\n# Definujemy naszą funkcję wiarygodności, prior i posterior. Zwrócie uwagę, że funkcję\r\n# zwracają nam logarytm naturalny gęstości prawdopodobieństwa. Jest to użyteczna \r\n# transformacja, ponieważ komputery nie radzą sobie dobrze z wartościami bardzo bliskimi 0. \r\n\r\nlikelihood <- function(parameters){\r\n  a=parameters[1]; b=parameters[2]; sigma=parameters[3]\r\n  sum(dnorm(y,a +b*x, sigma, log = TRUE))\r\n}\r\n\r\nprior <- function(parameters){\r\n  a=parameters[1]; b=parameters[2]; sigma=parameters[3]\r\n  sum(dunif(a,-1000,1000,log = TRUE), dnorm(b,0,100,log = TRUE),   \r\n  dunif(sigma,0.000001,1000,log = TRUE))\r\n}\r\n\r\nposterior <- function(parameters) {likelihood(parameters) + prior(parameters)}\r\n\r\nchain = 3 # Liczba łańcuchów\r\nn.iter <- 300000 # Liczba iteracji\r\nresults <- array(NA, c(n.iter,3,3)) # Pusty tensor na nasze próbki\r\n\r\nn.burn = 50000 # Liczba iteracji, którą odrzucimy w ramach wypalania\r\n\r\nfor (j in 1:chain){\r\n\r\nparameters <- c(runif(1,-50,50), rnorm(1,0,10),runif(1,0.000001,100)) # Losujemy wartości początkowe dla łańcucha\r\nresults[1,,j] <- parameters\r\nnaccepted = 2\r\n\r\n  while(naccepted <= n.iter){\r\n    \r\n    candidate <-  rnorm(3, mean = parameters, sd = 0.1) # przemieszczamy się o losową wartość\r\n    # parametrów od wartości obecnych parametrów \r\n    \r\n    ratio <- exp(posterior(candidate) - posterior(parameters)) # Odejmujemy ponieważ log(a/b) \r\n  # = log(a) - log(b). Różnicę wkladamy do funckcji eksponencjalnej by z logarytmów otrzyamć\r\n  # stosunek prawdopodobieństw. \r\n    \r\n    if (runif(1) < ratio) {parameters <- candidate \r\n    results[naccepted, ,j] <- parameters\r\n    naccepted = naccepted + 1}\r\n  }}\r\n\r\nresults <- results[(n.burn+1):n.iter,,] # Usuwamy 'n.burn' pierwszych iteracji\r\n\r\nall_chains = data.frame()\r\n\r\nfor (i in 1:chain){\r\n  \r\n  all_chains = rbind(all_chains,data.frame(results[,,i], chain = as.character(i), Iteration = (n.burn+1):n.iter))\r\n  \r\n  }\r\n\r\ncolnames(all_chains) = c(\"a\", \"b\", \"sigma\",\"chain\", \"Iteration\")\r\n\r\n\r\nMamy to! Sprawdźmy czy nasze łańcuchy się zbiegły.\r\n\r\n\r\nlibrary(gridExtra)\r\np1 <- ggplot(all_chains, mapping = aes(x = Iteration, y = a, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\np2 <- ggplot(all_chains, mapping = aes(x = Iteration, y = b, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\np3 <- ggplot(all_chains, mapping = aes(x = Iteration, y = sigma, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\ngrid.arrange(p1, p2,p3, nrow = 3)\r\n\r\n\r\n\r\nWizualna inspekcja wskazuje, że łańcuchy zbiegły sie wystarczająco (póżniej poznamy ilościowe wskaźniki). Zobaczmy rozkłady post piori.\r\n\r\n\r\np1 <- ggplot(all_chains, mapping = aes(x = a))+\r\n  geom_histogram(fill = \"white\",color=\"black\",) +\r\n  theme_minimal()\r\np2 <- ggplot(all_chains, mapping = aes(x = b))+\r\n  geom_histogram(fill = \"white\",color=\"black\",) +\r\n  theme_minimal()\r\np3 <- ggplot(all_chains, mapping = aes(x = sigma))+\r\n  geom_histogram(fill = \"white\",color=\"black\") +\r\n  theme_minimal()\r\ngrid.arrange(p1, p2, p3, nrow = 1)\r\n\r\n\r\n\r\nPoliczmy statystyki rozkładów.\r\n\r\n\r\nsummary(all_chains[,1:3])\r\n\r\n       a                b              sigma       \r\n Min.   :0.6338   Min.   :0.4456   Min.   : 7.213  \r\n 1st Qu.:4.2037   1st Qu.:0.8558   1st Qu.: 9.315  \r\n Median :4.8798   Median :0.9194   Median : 9.779  \r\n Mean   :4.8822   Mean   :0.9195   Mean   : 9.819  \r\n 3rd Qu.:5.5547   3rd Qu.:0.9830   3rd Qu.:10.274  \r\n Max.   :9.3682   Max.   :1.3589   Max.   :13.426  \r\n\r\nPorównajmy z klasyczną regresją.\r\n\r\n\r\nm = lm(y~x)\r\ndata.frame(a = m$coefficients[1], b = m$coefficients[2], sigma = sd(m$residuals), row.names = \"Parameters\")\r\n\r\n                  a         b    sigma\r\nParameters 4.919473 0.9192618 9.606116\r\n\r\nDiagnostyka\r\nGdy już mamy nasz model, musimy sprawdzić czy został wyestymowany poprawnie oraz w jakim zakresie jest w stanie odtworzyć zaobserwowane dane.\r\nGelman-Rubin Convergence Diagnostic\r\nStatystyka Gelmana-Rubina sprawdza ilościowo czy łańcuchy się zbiegły. Opiera się na stosunku wariancji między łańcuchami do wariancji wewnątrz łańcuchów (Gelman & Rubin, 1992).\r\n\\[\\begin{eqnarray*}\r\n\\bar{x}_j\r\n& = &\r\n\\frac{1}{N}\\sum_{t=1}^N x_t^{(j)}\\hspace{2em}\\text{(Średnia łańcucha)}\\\\\r\n\\bar{x}_\\cdot\r\n& = &\r\n\\frac{1}{J}\\sum_{j=1}^J \\bar{x}_j\\hspace{2em}\\text{(Średnia łańcuchów)}\\\\\r\nB\r\n& = &\r\n\\frac{N}{J-1}\r\n\\sum_{j=1}^J (\\bar{x}_j-\\bar{x}_\\cdot)^2\\hspace{2em}\\text{(Wariancja pomiędzy łańcuchami)}\\\\\r\ns^2_j\r\n& = &\r\n\\frac{1}{N-1}\r\n\\sum_{t=1}^N (x_t^{(j)}-\\bar{x}_j)^2\\hspace{2em}\\text{(Wariancja wewnątrz łańcucha)}\\\\\r\nW\r\n& = &\r\n\\frac{1}{J}\\sum_{j=1}^J s_j^2\\hspace{2em}\\text{(Średnia wariancja wewnątrz łańcuchów)}\r\n\\end{eqnarray*}\\]\r\nGdzie \\(N\\) to liczba iteracji, a \\(J\\) to liczba łańcuchów. Statystyka Gelmana Rubina jest dana wzorem:\r\n\\[R = \\frac{\r\n\\frac{N-1}{N}W + \\frac{1}{N}B\r\n}{W}\\]\r\nZwykle uznaje się, że łańcuchy się zbiegły, jeśli R < 1.15. Statystykę liczy się dla każdego parametru. W przypadku naszej regresji:\r\n\r\n\r\nBW = pivot_longer(all_chains,cols = 1:3, names_to = \"parameter\", values_to = \"Value\")\r\nBW = BW %>% group_by(parameter,chain) %>% summarise(Mean = mean(Value), Var = var(Value), n =n())\r\nW = BW %>% group_by(parameter) %>% summarise(W = mean(Var)/3) \r\nB = BW %>% group_by(parameter) %>% summarise(B = var(Mean)) \r\n\r\nR = (W$W*((n.iter - n.burn - 1)/(n.iter - n.burn)) + B$B)/W$W\r\nnames(R) = c(\"a\",\"b\",\"sigma\")\r\nkable(R,col.names = \"R\")\r\n\r\n\r\nR\r\na\r\n1.001088\r\nb\r\n1.000062\r\nsigma\r\n1.003041\r\n\r\nKażdy parametr spełnia kryterium Gelmana-Rubina.\r\nAutokorelacja\r\nJak wspominałem już wielokrotnie wcześniej, MCMC ma symulować losowanie z rozkładu a posteriori. Wyobraźmy sobie taką sytuację - losujemy 1000 osób, by estymować rozkład wzrostu w populacji. Obserwacje są od siebie niezależne, wylosowanie osoby A nie ma wpływu na prawdopodobieństwo wylosowania osoby B. W przypadku łańcuchów nie jest to prawdą. Zauważmy, że wylosowanie obserwacji B, zależy od tego jaką wartośc ma obserwacja A.\r\nW MCMC poruszamy się od obserwacji do obserwacji różniących się o jakąś małą losową wartość. W następstwie w łańcuchach występuje autokorelacja. Jeśli jest duża, może zbiasować nasz rozkład a posteriori, ponieważ pewne wartości będą nadreprezentowane, a efektywna liczba prób z rozkładu post priori będzie mniejsza niż N.\r\nAutokowariancja dla opóźnienia (lag) \\(t\\) jest zdefiniowana tak:\r\n\\[aCov(t) = \\frac{1}{N - t} \\sum_{n=1}^{N-t} (\\theta_n - \\mu_f)\\,(\\theta_{n+t}-\\mu_\\theta)\\]\r\ngdzie\r\n\\[\\mu_\\theta = \\frac{1}{N}\\sum_{n=1}^N \\theta_n\\]\r\na \\(\\theta_n\\) to wartość wylosowana przez łańcuch w iteracji \\(n\\).\r\nAutokorelacja dana jest więc:\r\n\\[aCor(t) = aCov(t)/aCov(0)\\]\r\nSprawdźmy autokorelację dla pierwszego łańcucha i parametru \\(\\beta\\).\r\n\r\n\r\nacf(results[,2,1],lag.max = 1000)\r\n\r\n\r\n\r\nWyglada to dobrze. Slina autokorleacja występuje tylko dla bardzo małego opóźnienia. Sprawdźmy teraz autokorelacje parametru \\(\\sigma\\).\r\n\r\n\r\nacf(results[,3,1],lag.max = 1000)\r\n\r\n\r\n\r\nW przypadku \\(\\sigma\\) wygląda to gorzej. Możecie się spotkać z tym, że niektórzy badacze by zmniejszyć autokorelacje, stosują odchudzanie (thinning), polegające na tym, do naszej próby rozkładu post priori bierzyemy co \\(n\\)-tą wartość z łańcucha.\r\n\r\n\r\nthinned_results = results[seq(1, n.iter - n.burn, by = 10),,]\r\nacf(thinned_results[,3,1],lag.max = 2000)\r\n\r\n\r\n\r\nJednakże, odchudzanie nie jest najlepszą strategią. Historycznie odchudzanie było stosowane z chęci zaoszczędzenia pamięci komputerów. Choć zmniejsza autokorelację, odchudzanie zmniejsza precyzję naszych wyników (Link & Eaton, 2012).\r\nZastanówmy się dlaczego. Chcielibyśmy by średnia wartości naszego łańcucha była jak najbliższa prawdziwej średniej rozkładu.\r\nJak pamiętamy, wariancja dla rozkładu średnich niezależnych prób losowych wynosi (zauważcie, że to inna \\(\\sigma\\) niż wcześniej):\r\n\\[\\sigma^2 = \\frac{1}{N}\\mathrm{Var}(\\theta)\\]\r\nJednak, jeżeli obserwacje są skorelowane, wariancja wynosi:\r\n\\[\\sigma^2 = \\frac{\\tau}{N}\\mathrm{Var}(\\theta)\\]\r\nDlaczego tak jest, to na razie pominiemy (wymagało by to dłuższego zatrzymania się nad tym problemem, niemniej być może do tego wrócimy). \\(\\tau\\) to zintegrowana funkcja autokorelacji (integrated autocorrelation function) dana wzorem:\r\n\\[ \\tau =1 + 2\\sum^N_{t= 1} \\mathrm{aCor(t)}\\] Wobec tego \\(\\frac{N}{\\tau}\\) to efektywna liczba próbek (ESS), a \\(\\tau\\) to liczba iteracji jakie musi minąć łańcuch zanim “zapomni” gdzie sie zaczął.\r\nPopatrzmy na wykres autokorelacji dla \\(\\beta\\). Do pewnego \\(t\\) autokorelacja jest malejącą funkcją, po przekroczeniu \\(t\\) zaczyna oscylować wokół 0. Ponieważ autokorelacje mogą być tylko pozytywne gdy \\(N\\) dąży do nieskonczoności, obserwowane empirycznie negatywne korelacje na pewno wynikają z szumu (Geyer, 1992). Sumowanie ich by otrzymać zmniejszy nam estymatę \\(\\tau\\). Rownież te małe dodatnie korelacje są efektem szumu.\r\nZwróćmy uwagę na wzór na autokowariancję. Im większy lag \\(t\\) tym mniejsza liczebność próbki (wynosi \\(N-t\\)), z której liczymy autokowariancę. W związku z tym im większe \\(t\\) tym bardziej zaszumione estymaty autokowariancji otrzymujemy. Jednym ze sposóbw na obejście tego problemu jest estymowanie \\(\\tau\\) w następujący sposób:\r\n\\[ \\tau =1 + 2\\sum^M_{t= 1} \\mathrm{aCor(t)}\\] Gdzie \\(M\\) jest ostatnią największą wartością \\(t\\) dla której wszystkie \\(t < M\\) mają dodatnią autokowariancję.\r\nPoliczmy efektywną liczbę próbek dla parametru \\(\\sigma\\) i łańcucha 1.\r\n\r\n\r\nESS = function(data,parameter,chain){\r\n  \r\n  tmp = data[,parameter,chain]\r\n  ACF = acf(tmp,plot = F,lag.max = 10000, type = \"covariance\")$acf/var(tmp)\r\n  tau =  2*sum(ACF[1:(which(ACF<0)[1]-1)]) - 1\r\n  length(tmp)/tau\r\n}\r\n\r\nESS(results,3,1)\r\n\r\n[1] 1174.496\r\n\r\nTeraz policzmy ESS dla odchudzonego łańcucha.\r\n\r\n\r\nESS(thinned_results,3,1)\r\n\r\n[1] 1166.745\r\n\r\nEES zmniejszyło się względem nieodchudzonego łańcucha. Generalnie, znacznie lepszą metodą redukowania wpływu autokorelacji, jest użycie większej liczby dłuższych łańcuchów. Odchudzanie należy stosować tylko jeśli mierzymy się z bardzo dużą autokorelacją. Reguła kciuka została zaproponowana w tej książce (Christensen et al., 2010) - odchudzać jeśli duża autokorelacja występuje powyżej lag > 30. W naszym przypadku było to więc niejako uzasadnione. Nie należy jednak stosować odchudzania jako rutyny w MCMC.\r\nPosterior predictive check\r\nWreszcie, gdy upewnimy się, że losoujemy z właściwej przestrzeni parametrów, i że nasze próbki przynajmnie udają, że są od siebie niezależne, czas na sprawdzenie czy nasz model właściwie modeluje to, co powinien modelować (model fit). Posterior predictive check polega na losowaniu z posterior predictive distribution, który jest rozkładem prawdopodbieństwa uzyskania nowych danych, pod warunkiem zebranych przez nas danych:\r\n\\[P(y^*|y)=\\int P(y^*|\\theta)P(\\theta|y)d\\theta\\] gdzie \\(y^*\\), to nowe dane. W przypadku MCMC estymacja rozkładu \\(P(y^*|y)\\) składa się z dwóch kroków: Wylosować wartość \\(\\theta\\) z rozkładu post priori. Wstawić wylosowane \\(\\theta\\) do \\(P(y^*|\\theta)\\) i wylosować \\(y^*\\).\r\nJak wygląda \\(P(y^*|\\theta)\\)? Podobnie jak nasza funkcja wiarygodności, z tą różnicą, że tym razem nie obliczamy wiarygodności uzyskania \\(n\\) próbek z zadanego rozkadu, lecz po po prostu losujemy z owego rozkładu.\r\nMy sprawdzimy nasze dopasowanie modelu poprzez analizę wizualną. Wyplotujmy sobie histogram z naszych danych obok rozkładu generowanego przez model.\r\n\r\n\r\npredicted <- NULL\r\n\r\nfor (j in 1:length(x)){\r\nfor (i in 1:chain){\r\npredicted <- c(predicted,rnorm(dim(results)[1], results[dim(results)[1],1,i] +\r\nresults[dim(results)[1],2,i]*x[j], results[dim(results)[1],3,i]))\r\n}}\r\n\r\npredicted = as.data.frame(predicted)\r\ny = as.data.frame(y)\r\nggplot() + \r\n  geom_histogram(data = y,mapping = aes(x = y,y = ..density..),\r\n                 colour = 1, fill = \"white\") +\r\n  geom_density(data = predicted, aes(x =predicted, y = ..density..)) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWszystko wydaje się być w porządku. Problemy z posterior predictive check może świadczyć o tym, że wybraliśmy złą funkcją wiarygodności lub/i rozkłady a priori.\r\nPodsumowanie\r\nUff, przebrneliśmy przez estymację bayesowską. Na szczęście od teraz będziemy stosować pakiety, które zrobią to za nas. Co ważne, zaprezentowany przeze mnie dziś kod raczej nie nadaje się do prawdziwej analizy. Dedykowane oprogramowanie przeprowadza losowanie MCMC w sposób bardziej optymalny (i skomplikowany). Warto jednak poznać na czym polega metoda estymacji, której często będziemy używać.\r\n\r\n\r\n\r\nChristensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). Bayesian ideas and data analysis: An introduction for scientists and statisticians (p. 146). CRC press.\r\n\r\n\r\nGelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science, 457–472.\r\n\r\n\r\nGeyer, C. J. (1992). Practical markov chain monte carlo. Statistical Science, 473–483.\r\n\r\n\r\nLink, W. A., & Eaton, M. J. (2012). On thinning of chains in MCMC. Methods in Ecology and Evolution, 3(1), 112–115.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-30-tutorial-bayes-ii/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-12-26T21:37:56+01:00",
    "input_file": "tutorial-bayes-ii.knit.md"
  },
  {
    "path": "posts/2022-07-31-tutorial-bayes/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Częśc I: Nabywanie intuicji",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-08-14",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nMały wstęp\r\nPrawdopodobieństwo\r\nTwierdzenie Bayesa\r\nDyskretny przykład\r\n\r\nPewność pomiaru\r\nCzęstościowcy\r\nBayesowcy\r\n\r\nPrzykład praktyczny\r\nPodsumowanie\r\n\r\n\r\n\r\n.MathJax_Display, .MJXc-display, .MathJax_SVG_Display {\r\n    overflow-x: auto;\r\n    overflow-y: hidden;\r\n}\r\n\r\nMały wstęp\r\nPomyślałem sobie, że napiszę taki mały tutorial statystyki Bayesowskiej.\r\nPróba wytłumaczenie czegoś komuś jest jedną z najlepszych metod nauki i weryfikacji swojej własnej wiedzy. W przeciwieństwie do rozważań we własnym umyśle, gdy tłumaczy się innym jakieś zagadnienie, nie można nie odpowiedzieć na pewne pytania, czy niektóre rzeczy zgrabnie pominąć :)\r\nTutorial nie będzie na pewno wyczerpującym podejściem do tematu. Jeśli ktoś potrzebuje metodycznego wprowadzenia do metod bayesowskich, możne znaleźć je świetnej książce “Bayesian Data Analysis” (Gelman et al., 2021). Tutorial jest raczej pewnego rodzaju przyśpieszonym kursem pozwalający na późniejsze samodzielne eksplorowanie i zabawę statystyką Bayesowską. Forma tutorialu wynika z podejścia w akademii, które dla rozpoczynających karierę naukowców można sprowadzić do zdania: najpierw badaj, potem zastanawiaj się jak zbadać.\r\nOsobiście preferuję używanie języków programowania i obliczenia w tym tekście będą wykonywane za pomocą R, ale wiele z pokazanych tu rzeczy można także wykonać w graficznym programie statystycznym JASP. W zrozumieniu zawartych tu treści pomoże, jeśli uczestniczyłeś/łaś wcześniej w jakimś kursie statystyki.\r\nI jeszcze kilka słów wstępu o samej statystyce bayesowskiej. Istnieje obiegowa opinia, że statystyka Bayesowska jest trudniejsza od klasycznej. Osobiście wydaje mi się, że “prostota” klasycznej statystyki wynika z jej dogmatyzmu. Istnieją instrukcje na zastosowanie odpowiednich testów do danych, utarły się schematy, do których należy się stosować, mimo, że czasami są arbitralne. Statystyka Bayesowska wymaga więcej namysłu od badacza, ponieważ nie istnieją tak ostre „reguły kciuka” jak w statystyce klasycznej.\r\nO statystyce bayesowskiej mówiło się ostatnio, jako rozwiązaniu kryzysu replikacyjnego. Mam pewną obawę, że statystyka bayesowska może tak samo popaść w dogmatyzm, jeśli potraktuje się ją mechanicznie jak klasyczną statystykę. Nie jestem twardym Bayesowcem, to znaczy nie uważam, że należy porzucić statystykę częstościową (tą zwykłą) na rzecz Bayesowskiej. Obie mają pewne zalety i wady. Podzielam zdanie Kevina Rosa, że dobry współczesny statystyk korzysta z każdego z podejść w zależności od potrzeb.\r\nPrawdopodobieństwo\r\nWszyscy intuicyjnie czujemy czym jest prawdopodobieństwo. Jeśli chcemy sprawdzić czy moneta nie jest obciążona i częściej wypada reszka niż orzeł, najprostszym sposobem jest dokonanie “eksperymentu losowego” i sprawdzenie ile razy wypadnie reszka spośród wszystkich prób. Im więcej razy rzucimy monetą, tym bardziej będziemy pewni otrzymanego wyniku. W statystyce klasycznej wychodzimy z podobnego założenia, formalnie zdefiniowanego tak:\r\n\\[ \\lim_{n\\to\\infty} P(A) = \\frac{k_n(A)}{n} \\]\r\ngdzie \\(k_n(A)\\) to liczba zdarzeń sprzyjających zdażeniu \\(A\\) a \\(n\\) to liczba prób. Słownie możemy przedstawić to założenie tak: Gdy liczba prób dąży do nieskończoności, częstość wystąpień zdarzeń \\(A\\) pośród wszystkich prób będzie się równało prawdopodobieństwu wystąpienia zdarzenia \\(A\\). Innymi słowy im więcej razy rzucimy monetą, tym większe mamy szanse na otrzymanie takiego stosunku reszek do orłów, który jest bliski prawdziwemu prawdopodobieństwu otrzymania reszki.\r\nW statystyce Bayesowkiej na prawdopodobieństwo patrzy się trochę inaczej. Zakłada się, że prawdopodobieństwo jest miarą pewności, którą możemy przypisać pewnym modelom (hipotezom). Wnioskowanie Bayesowskie można porównać do tego, jak wnioskują ludzie. Mamy jakieś oczekiwania, następnie dostajemy dane i rewidujemy nasze oczekiwania.\r\nWnioskowanie Bayesowskie opiera się na twierdzeniu Bayesa:\r\n\\[  P(Hipoteza|Dane) = \\frac{P(Dane|Hipoteza)P(Hipoteza)}{P(Dane)}\\]\r\n\\(P(Hipoteza)\\) nazywane prawdopodobieństwem a priori, oznacza uprzednią wiedzę, przed zebraniem danych, które hipotezy są bardziej prawdopodobne. Innymi słowy są to nasze oczekiwania, na przykład, że gdy odkręcimy kran, najprawdopodobniej poleci z niego woda. Również brak wiedzy/oczekiwań jest prawdopodobieństwem a priori. Wtedy wszystkie hipotezy można zamodelować jako równie prawdopodobne.\r\nCzłon \\(P(Dane|Hipoteza)\\) (wiarygodność, likehood) oznacza prawdopodobieństwo otrzymania danych, że pod warunkiem wygenerowania ich w zgodzie z daną hipotezą (wygenerowania ich przez określony model). Można powiedzieć, że jest to sposób w jaki aktualizujemy nasze przekonania - tzn. nasz model rzeczywistości (po odkręceniu kranu coś z niego leci).\r\nWreszcie człon \\(P(Hipoteza|Dane)\\) jest prawdopodobieństwem a posteriori, czyli prawdopodobieństwem hipotezy w zależności od posiadanych danych. Innymi słowy prawdopodobieństwo a posteriori mówi nam jak bardzo zmieniły się nasze przekonania a priori pod wpływem danych. Czyli jeśli z kranu popłynęło wino, to zrewidowaliśmy nasze oczekiwania względem tego kranu.\r\n\\(P(Dane)\\) jest parametrem skalującym (stałą), bez specjalnej interpretacji. O jego roli przekonamy się nieco dalej w tekście.\r\nWszystko to wygląda fajnie, ale już na tym etapie może budzić zamieszanie. Czym są owe prawdopodobieństwa? Żeby to dobrze zrozumieć, przypomnimy sobie czym jest prawdopodobieństwo warunkowe, a następnie wyprowadzimy wzór Bayesa.\r\nTwierdzenie Bayesa\r\nPrawdopodobieństwo zdarzenia \\(A\\) w zależności od zdarzenia B równa się prawdopodobieństwu wystąpienia zdarzeń \\(A\\) i \\(B\\) podzielonym na prawdopodobieństwo zdarzenia \\(B\\):\r\n\\[P(A|B) = \\frac{P(A \\land B)}{P(B)}\\] By obliczyć prawdopodobieństwo palenia papierosów pod warunkiem bycia mężczyzną musimy po prostu podzielić liczbę mężczyzn palących papierosy przez liczbę wszystkich mężczyzn.\r\nZauważmy, że powyższe równanie możemy przekształcić. Najpierw pomnóżmy obie strony równania przez \\(P(B)\\).\r\n\\[  P(A|B)P(B) = {P(A \\land B)}\\] Teraz podzielmy obie strony przez \\(P(A)\\).\r\n\\[  \\frac{P(A|B)P(B)}{P(A)} = \\frac{P(A \\land B)}{P(A)} = P(B|A)\\] To co otrzymaliśmy, jest twierdzeniem Bayesa:\r\n\\[  P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\]\r\nDyskretny przykład\r\nBy uświadomić sobie przydatność twierdzenia Bayesa wykonajmy małe ćwiczenie. Załóżmy, że opracowaliśmy test pozwalający na wykrycie pewnej choroby z 99% prawdopodobieństwem u osób faktycznie chorych. Możemy zapisać to w następujący sposób \\(P(Pozytywny|Choroba) = 0.99\\). Test jednak daje też wynik fałszywie pozytywny u 5% osób zdrowych \\(P(Pozytywny|\\neg Choroba) = 0.05\\). Wiemy także, że nosicielem choroby jest 1 procent populacji \\(P(Choroba) = 0.01\\). Chcielibyśmy się dowiedzieć jakie jest prawdopodobieństwo choroby, jeśli wynik jest pozytywny \\(P(Choroba|Pozytywny)\\). By je obliczyć potrzebujemy jeszcze prawdopodobieństwa otrzymania wyniku pozytywnego. Uzyskać je możemy za pomocą prawa sumy.\r\n\r\np.comment {\r\n-moz-border-radius: 6px;\r\n-webkit-border-radius: 6px;\r\nbackground-color: #f0f7fb;\r\nbackground-image: url(css-box-icon-3.png);\r\nbackground-position: 9px 0px;\r\nbackground-repeat: no-repeat;\r\nborder: solid 1px #3498db;\r\nborder-radius: 6px;\r\nline-height: 18px;\r\noverflow: hidden;\r\npadding: 15px 60px;\r\n}\r\n\r\n\r\n\r\nPrawo całkowitego prawdopodobieństwa  Jeśli \\(B_1\\), \\(B_2\\) … \\(B_n\\) są parami rozłączne, a ich prawdopodobieństwo sumy zdarzeń wynosi 1, to dla dowolnego zdarzenia \\(A\\) zachodzi wzór:  Dla wartości dyskretnych. \\[P(A)=\\sum_{B}P(A \\land B) \\] Dla wartosci ciągłych. \\[ P(A) = \\int_{}P(A \\land B)dB\\]\r\n\r\n\r\nCzyli, by obliczyć intersujące nas prawdopodobieństwo musimy podstawić odpowiednie wartości do wzoru:\r\n\\[P(Choroba|Pozytywny) =\\frac{P(Pozytywny|Choroba)P(Choroba)}{P(Pozytywny) = P(Pozytywny|Choroba)P(Choroba) + P(Pozytywny|\\neg Choroba)P( \\neg Choroba)}\\]\r\nPo podstawieniu do wzoru\r\n\\[\\frac{0.99*0.01}{0.99*0.01 + 0.05*0.99} = 0.166 \\]\r\nTylko 16% procent osób z pozytywnym testem faktycznie będzie miało chorobę. Powyższy przykład obrazuje czemu czasami profilaktyczne badania mogą powodować więcej problemów niż ich brak. Jeśli osoby byłby wysyłane na badanie bez innych wskazań świadczących o chorobie, ponad 85% osób zdrowych, otrzymałoby fałszywy wynik świadczący o chorobie.\r\nPewność pomiaru\r\nRozważmy teraz następujący problem. Mierzymy IQ 15 osób i chcemy się dowiedzieć z jaką pewnością możemy mówić o średniej IQ z próby jako estymacie średniej IQ w populacji.\r\nCzęstościowcy\r\nW statystyce klasycznej policzylibyśmy błąd standardowy i przedział ufności zgodnie z paradygmatem Null Hypothesis Significance Testing (NHST). Dla uwidocznienia różnic pomiędzy podejściem Bayesowskim a częstościowym, przypominajmy sobie na jakich założeniach opiera się NHST.\r\nZgodnie z Centralnym Twierdzeniem Granicznym średnia niezależnych zmiennych losowych o takim samym rozkładzie dąży o rozkładu normalnego gdy liczba zmiennych dąży do nieskończoności. Średnia takiego rozkładu jest równa średniej w populacji, a jej wariancja wynosi \\(\\frac{D^2}{n}\\), gdzie \\(D^2\\) to wariancja w populacji. Zwizualizujmy taki rozkład dla średnich z wielkości próby równej 15. Wiemy, że średnia IQ wynosi 100, a odchylenie standardowe 15.\r\n\r\n\r\nlibrary(tidyverse)\r\nbreaks <- qnorm(c(0, .025, .2, .5, .8, .975, 1),100, 15/sqrt(15))\r\nggplot(data.frame(x = c(85, 115)), aes(x)) +\r\nscale_fill_brewer(\"x\") +\r\nstat_function(fun = dnorm,n = 1000, args = list(mean = 100, sd = 15/sqrt(15)),geom = \"area\",\r\n  colour = \"gray30\", alpha = 0.7, aes(fill = after_stat(x) |> cut(!!breaks),\r\n  group = after_scale(fill))) +\r\n  geom_vline(xintercept = c(100 - 15/sqrt(15)*qnorm(.975), 100 + 15/sqrt(15)*qnorm(.975))) +\r\n  labs(x = \"Mean IQ\", y = \"Density\") +\r\n  scale_fill_discrete(name = \"P(X < x)\", labels = c('0.025', '0.2', '0.5', '0.8', '0.975','1'))\r\n\r\n\r\n\r\nStworzyliśmy w ten sposób estymowany rozkład wszystkich możliwych średnich IQ z piętnastoosobowych prób. Ten rozkład mówi nam, że losując 15 osobową próbę mamy 95% prawdopodobieństwo na wylosowanie takiej próby, w której średnie IQ znajdzie się w przedziale ufności oznaczonym przez czarne pionowe kreski na wykresie. 95% jest tu wartością umowną, badacz może przyjąć dowolny poziom ufności (Poziom ufności to \\(1-\\alpha\\), gdzie \\(\\alpha\\) to prawdopodbieństwo błędu I rodzaju).\r\nZwykle gdy dokonujemy pomiaru, nie znamy średniej i wariancji i dopiero estymujemy je z naszej próby. Jeśli będziemy wielokrotnie wykonywać pomiar i wyznaczać przedziały ufności, możemy spodziewać się, że w 95% w przedziałach ufności z prób znajdzie się prawdziwa wartość średniej IQ.\r\n\r\n\r\nset.seed(999)\r\ndata <- data.frame()\r\n\r\nfor(i in 1:50) {\r\n  sample <- rnorm(15, 100, 15)\r\n  data <- rbind(data,data.frame(Mean = mean(sample), se = sd(sample)/sqrt(15)))\r\n}\r\n\r\ndata$Sample <-1:50\r\n\r\nggplot(data, aes(x = Sample, y = Mean)) +       \r\n  geom_errorbar(aes(ymin = Mean - se*qnorm(.975), ymax = Mean + se*qnorm(.975))) +\r\n  coord_flip() +\r\n  geom_hline(yintercept = 100, colour = \"red\" )\r\n\r\n\r\n\r\nNo dobrze, jaki z płynie wniosek? W idealnej sytuacji, gdy nie ma błędów systematycznych, wszystkie założenia są spełnione i próba była wylosowana w sposób reprezentatywny, kiedy wykonujemy wiele pomiarów średniej, średnio 95% przedziałów ufności dla próby powinno zawierać w sobie prawdziwą średnią.\r\nZałóżmy teraz, że wykonaliśmy pomiar IQ w 15 osobowej próbie i stwórzmy sobie takie dane:\r\n\r\n\r\niq <- c(87, 112, 83, 95, 106,105, 123, 107, 103, 98, 137, 114, 90, 90, 117)\r\ndata <- data.frame(IQ = \"Our sample\", Mean = mean(iq), se = sd(iq)/sqrt(length(iq)))\r\nggplot(data, aes(x = IQ, y = Mean)) +        \r\n  geom_point() +\r\n  geom_errorbar(aes(ymin = Mean - se*qnorm(.975), ymax = Mean + se*qnorm(.975))) +\r\n  coord_flip()\r\n\r\n\r\n\r\nJaką daje nam to pewność co do naszego pomiaru? Na ile możemy być pewni, że prawdziwa średnia leży w tym przedziale ufności? Tu zaczynają się trudności.\r\nNHST historycznie powstała do stosowania w warunkach przemysłowych, gdzie wykonujemy bardzo wiele identycznych badań, na przykład w kontroli jakości produktów. Na przykład, w Ikei produkowany jest pewien element do konstrukcji szafek. Każdy kolejny wyprodukowany egzemplarz tego elementu różni się odrobinę w długości od schematu, ale dobrze skalibrowana maszyna przycinająca elementy działa tak, że te różnice nie są na tyle duże by element nie pasował do szafki.\r\nOsoba odpowiedzialna za kontrolę jakości regularnie wykonuje pomiary długości \\(n\\) elementów każdego dnia, by sprawdzić czy średnia długość elementów nie wykracza założonego przedziału bezpiecznej różnicy w długości. Obliczył sobie ile elementów musi zmierzyć dziennie, by przedział ufności pokrywał się z tą bezpieczną różnicą przy ustalonym poziomie ufności 90%. Jeśli maszyna będzie działać poprawnie, kontroler otrzyma średnio 1 fałszywy alarm na 10 dni.\r\nZasadniczo, w idealnej sytuacji mamy \\(1-\\alpha\\) prawdopodobieństwo na wylosowanie takiej próby, której przedział ufności będzie zawierał prawdziwą średnią. Często spotykane, choć nie do końca zasadne, jest twierdzenie, w danej próbie istnieje prawdopodobieństwo równe \\(1-\\alpha\\), że w przedziale zawiera się średnia. Takie prawdopodbieństwo mamy przed wylosowaniem próby, gdy już ją mamy, nie wiemy co wylosowaliśmy.\r\nBayesowcy\r\nNo dobrze. Czas na Bayesowskie podejście do problemu. Chcemy obliczyć \\(P(Hipoteza|Dane)\\). Co jest jednak naszą hipotezą? O ile w przypadku zmiennej binarnej jak choroba i jej brak jest to proste, tutaj nasza hipoteza dotyczy wartości średniej - wartość średniej jest raczej zmienną ciągłą, może być dowolna. Dlatego \\(P(Hipoteza|Dane)\\) będzie rozkładem prawdopodobieństwa dla każdej możliwej wartości średniej.\r\nRozkład IQ jest, zgodnie z teorią, rozkładem normalnym. Dlatego \\(P(Dane|Hipoteza)\\) możemy zamodelować jako prawdopodobieństwo otrzymania danych pod warunkiem określonych wartości średniej (krtórą oznaczymy jako \\(\\mu\\)) i odchylenia standardowego (które oznaczymy jako \\(\\sigma\\)) rozkładu normalnego, które nazywamy parametrami. Gdy mamy dużo parametrów, zwykle dla wygody oznaczmy je literką \\(\\theta\\), która symbolizuje wektor parametrów.\r\nMy jednak mamy tylko dwa parametry, więc nasze \\(P(Dane|Hipoteza)\\) zapiszemy jako \\(P(y| \\mu ,\\sigma)\\), które jest rozkładem prawdopodobieństwa, mówi nam jak prawdopodobne jest wylosowanie wartości \\(y\\) z rozkładu normalnego \\(N(\\mu,\\sigma)\\). Na przykłd, gęstość prawdopodobieństwa otrzymania wartości 6 z rozkładu \\(N(4,2)\\) wynosi:\r\n\r\n\r\ndnorm(6,4,2)\r\n\r\n[1] 0.1209854\r\n\r\nZauważmy, że chcemy by nasza funkcja wiarygodności dawała prawdopodobieństwo wszystkich danych jakie mamy. Ponieważ dane (obserwacje), które mamy są losowe i od siebie niezależne, prawdopodobieństwo ich wylosowania równa się ich iloczynowi:\r\n\\[P(A \\land B) = P(A)P(B)\\] Czyli, prawdopodobieństwo wylosowania całego zbioru danych z rozkładu o określonych parametrach będzie iloczynem prawdopodobieństwa wylosowania każdej obserwacji z osobna:\r\n\\[P(y|\\mu,\\sigma) = \\prod_{i=1}^{n}P(y_i|\\mu,\\sigma) = P(y_1|\\mu,\\sigma)*P(y_2|\\mu,\\sigma)...P(y_n|\\mu,\\sigma)\\].\r\nSkoro mamy już naszą funkcję wiarygodności, potrzebujemy jeszcze naszego prawdopodobieństwa A priori \\(P(\\mu,\\sigma)\\). Zakładamy, że wartość średniej jest niezależna od wartości odchylenia standardowego, czyli \\(P(\\mu,\\sigma) = P(\\mu) P(\\sigma)\\). Tak jak już wspomniałem, prawdopodobieństwo a priori przedstawia stan wyjściowy (np. naszą wcześniejszą wiedzę).\r\nJaką wiedzę mamy na temat średniego IQ? Wiemy, że w populacji wynosi 100 (tak jest skonstruowana ta miara). Raczej nie podejrzewamy dużych odstępów od tej wartości więc możemy zamodelować \\(P(\\mu)\\) rozkładem normalnym o średniej 100 i odchyleniu 2 \\(N(100,3)\\). Wiemy też, że odchylenie standardowe wynosi 15, ale na potrzeby tego ćwiczenia udamy, że tego nie wiemy. A skoro nie mamy żadnej wcześniejszej wiedzy na temat SD, uznajemy że każda wartość SD jest tak samo prawdopodobna i zamodelujemy \\(P(\\sigma)\\) rozkładem jednostajnym (prawdopodobieństwo wylosowania każdej wartości z zadanego przedziału jest takie same).\r\nNasz wzór możemy zapisać:\r\n\\[P(\\mu ,\\sigma|y) = \\frac{P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)}{P(y)}\\]\r\nPotrzebujemy jeszcze prawdopodobieństwa \\(P(y)\\). Ponownie wykorzystujemy prawo sumy, tym razem dla zmiennych ciągłych:\r\n\\[P(y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)d\\mu d\\sigma\\]\r\nChoć ta podwójna całka może wyglądać dla niektórych przerażająco, można skonceptualizować ją sobie jako sumowanie członu \\(P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\) po wszystkich wartościach \\(\\mu\\) i \\(\\sigma\\).\r\nNa marginesie, zasadniczo nie musimy jej obliczać ponieważ jest to wartość stała, która skaluje otrzymany rozkład post priori by suma prawdopodobieństw równała się 1. Dlatego nasz wzór możemy zapisać:\r\n\\[P(\\mu ,\\sigma|y) \\propto P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\] Co oznacza, że nasz rozkład \\(P(\\mu ,\\sigma|y)\\) jest proporcjonalny do \\(P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\).\r\nTeraz możemy policzyć nasze szukany rozkład prawdopodobieństwa \\(P(\\mu ,\\sigma|y)\\). W statystyce bayesowskiej popularnością cieszą się rozwiązania numeryczne, ponieważ rozwiązania analityczne (czyli rozwiązania danego problemu czysto za pomocą wzorów) często nie istnieją.\r\nBy obliczyć nasze rozwiązanie, też posłużymy się metodą numeryczną. Będzie ona bardzo prosta, co pozwoli prześledzić dokładnie jak wykonywane są obliczenia, jednak w praktyce stosuje się trochę bardziej skomplikowane metody, które omówimy w następnych częściach tutorialu.\r\nRównania wyglądają czasami bardzo abstrakcyjnie, zwłaszcza jeśli nie można sobie dokładnie wyobrazić tego co reprezentują. Kod przydaje się w takich momentach. Po każdej wykonanej linijce można sprawdzić co nam wyszło i powiązać to z matematycznym zapisem.\r\n\r\n\r\n# Dane z naszej próby\r\niq <- c(87, 112, 83, 95, 106,105, 123, 107, 103, 98, 137, 114, 90, 90, 117)\r\n\r\n# To są nasze parametry czyli możliwe wartości parametrów mu i sigma. \r\n# Ponieważ jest to proste przybliżenie numeryczne, ograniczamy możliwe wartości średniej \r\n# między 50 a 150, dla odchylenia standardowego między 0.1 a 50.\r\npars <- expand.grid(mu = seq(50, 150, length.out = 300), \r\n                    sigma = seq(0.1, 50, length.out = 300))\r\n\r\n# Liczymy marginalne rozkłady a priori.\r\npars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 3)\r\npars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)\r\n\r\n# By uzyskać łączne prawdopodobieństwo a priori mnożymy je przez siebie.  \r\npars$prior <- pars$mu_prior * pars$sigma_prior\r\n\r\n# Obliczamy wiarygodność \r\nfor(i in 1:nrow(pars)) {\r\n  # Prawdopodobieństwo wylosowania danej obserwacji z rozkładu normalnego o parametrach mu i sigma.\r\n  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])\r\n  # Mnożymy by uzyskać wiarygodność\r\n  pars$likelihood[i] <- prod(likelihoods)\r\n}\r\n# Mnożymy wiarygodność przez prawdopdobieństwo a priori. \r\npars$probability <- pars$likelihood * pars$prior\r\n\r\n#Otrzymaną wartość dzielimy przez P(y). \r\npars$probability <- pars$probability/sum(pars$probability)\r\n\r\n# Voilà, mamy nasz rozkład prawdopodobieństwa P(mu, sigma| y)\r\nlibrary(lattice)\r\nlibrary(viridisLite)\r\ncoul <-  viridis(1000)\r\nlevelplot(probability ~ mu * sigma, data = pars, col.regions = coul)\r\n\r\n\r\n\r\nWidzimy tu prawdopodobieństwo, że dana kombinacja parametrów \\(\\mu\\) i \\(\\sigma\\) wygenerowała nasze dane. W odróżnieniu do statystyki częstościowej odpowiedzią na nasze pytanie jest nie punktowa wartość parametru, ale jego rozkład \\(P(\\theta|y)\\). Możemy skonstruować przedział, w którym zawrze się 95% prawdopodobieństwa. Nazywa się go przedziałem wiarygodności. W odróżnieniu do przedziału ufności, interpretacja przedziału wiarygodności mówi: Biorąc pod uwagę dane, które mamy, oraz prawdopodobieństwa a priori, mamy 95% prawdopodobieństwo, że parametr, który wygenerował dane jest w przedziale wiarygodności.\r\nWeźmy rozkład marginalny dla średniej za pomocą prawa sumy:\r\n\r\n\r\nmu = pars %>% group_by(mu) %>% summarise(probability = sum(probability))\r\ncs = cumsum(mu$probability)\r\nboundary_1 = mu$mu[which(cs < 0.025)[length(which(cs < 0.025))]]\r\nboundary_2 = mu$mu[which(cs >= 0.975)[1]]\r\nggplot(mu, aes(x = mu,y = probability))+\r\n  geom_line(colour = 'light blue') +\r\n  xlim(92,111) +\r\n  ylab(\"Density\") +\r\n  geom_area(alpha = 0.75, fill = 'light blue') +\r\n  geom_vline(xintercept = c(boundary_1, boundary_2))\r\n\r\n\r\n\r\nZastanówmy się co by się stało gdybyśmy ponowili nasze badanie. W przypadku statystyki częstościowej musielibyśmy powtórzyć całe postępowanie (tak, technicznie możemy zebrać dane z obu badań i policzyć z nich przedział ufności czy inną statystykę, ale nie byłoby to zgodne z założeniami testów statystycznych). W przypadku bayesowskim możemy potraktować nasz rozkład post priori \\(P(\\mu|y)\\) jako rozkład a priori \\(P(\\mu)\\) w następnym badaniu.\r\nŚrednia IQ w naszej bayesowskiej analizie wyszła nieco mniejsza niż w częstościowej, ponieważ zastosowaliśmy informatywny rozkład a priori. Tu właśnie mamy do czynienia z elementem “subiektywnym.” Możliwość arbitralnego doboru wyjściowego przekonania co do stanu rzeczy, była dla statystyków częstościowych czymś nie do przyjęcia. Jak się jednak przekonamy, dobór prawdopodobieństwa a prori nie jest aż taką arbitralną decyzją, a rozkłady a priori posiadają także niesubiektywistyczną interpretację.\r\nNo dobrze, wynikiem analizy częstościowej i bayesowskiej jest jakiś przedział. Może interpretacje się różnią, ale czy w praktyce nie wychodzi na to samo? Otóż nie. Przedziału wiarygodności używamy w różnych sytuacjach, ale statystyka bayesowska nie sprowadza się do bayesowskiego p-value.\r\nPrzykład praktyczny\r\nZałóżmy, że korzystamy z internetowej księgarni, która ma system oceny książek przez czytelników. Dla uproszczenia załóżmy, że internauci mogą jedynie polecić książkę lub ją nie polecić. Wybraliśmy sobie trzy książki, które nas interesują, ale stać nas na tylko jedną. Chcemy więc dokonać finalnego wyboru sugerując się ocenami innych. Książkę A poleca 8 z 10 czytających, książkę B poleca 35 z 50, a książkę C poleca 60 z 100.\r\nOdsetek poleceń książki w stosunku do wszystkich głosów, możemy potraktować jako ocenę jakości książki. Chcemy więc, by ta wartość była jak największa. Ale jednocześnie chcemy nasza miara jakości była jak najbardziej pewna. A jaka jest najbardziej pewna? Ta która ma najwięcej głosów (danych). Jak więc wybrać książkę, która ma największe prawdopodobieństwa bycia dobrą lekturą? Zauważmy, że prawdopodobieństwo wylosowania \\(k\\) ocen pozytywnych z \\(n\\) wszystkich ocen danej książki, jeśli książka ma prawdopodobieństwo \\(p\\) otrzymania recenzji pozytywnej możemy zamodelować rozkładem dwumianowym.\r\n\r\nRozkład dwumianowy  Rozkład dwumianowy modeluje prawdopodobieństwo uzyskania \\(k\\) sukcesów z \\(n\\) prób, gdy prawdopodobieństwo sukcesu wynosi \\(p\\). Dany jest wzorem: \\[ P(k)  =  {{n}\\choose{k}} \\cdot p^k(1-p)^{n-k}\\]\r\n\r\n W statystyce klasycznej najlepszym estymatorem \\(p\\) jest po prostu \\(\\frac{k}{n}\\). Nie uwzględnia to jednak niepewności pomiaru. My jednak chcielibyśmy podejść do problemu bayesowsko. Musimy więc zdefiniować wszystkie potrzebne zmienne.\r\nZmienną, której rozkład chcemy poznać jest \\(p\\), dlatego \\(p\\) będzie naszą hipotezą. Dane to recenzje pozytywne i negatywne. Mamy już dobrego kandydata na funkcję wiarygodności. Prawdopodobieństwo uzyskania \\(k\\) poleceń na \\(n\\) ocen pod warunkiem konkretnej wartości \\(p\\) zamodelujemy rozkładem dwumianowym.\r\n\\[P(k,n|p) = {{n}\\choose{k}} \\cdot p^k(1-p)^{n-k}\\]\r\nPotrzebujemy jeszcze prawdopodobieństwa a priori \\(P(p)\\). Zauważmy, że ponieważ szukane przez nas \\(p\\) jest prawdopodobieństwem, może przyjmować dowolne wartości z przedziału <0,1>. Do modelowania prawdopodobieństwa prawdopodobieństwa często wykorzystuje się rozkład Beta.\r\n\r\nRozkład Beta  Rozkład Beta to rozkład prawdopodobieństwa, którego nośnik (przedział dla którego funkcja zwraca wartości większe od 0) to <0,1>. Rozkład ma dwa parametry kształtu \\(\\alpha\\) i \\(\\beta\\). Dany jest wzorem: \\[ P(x)  =  \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\] W mianowniku mamy funkcję Beta. Dla wygody zapisuje się jako \\(B(\\alpha,\\beta)\\).  Wartość oczekiwana rozkładu Beta to: \\[E(x) = \\frac{\\alpha}{\\alpha+ \\beta}\\]  Na wykresie kształt rozkładu dla różnej wartości parametrów. \r\n\r\nA więc \\(P(p)\\) możemy zamodelować rozkładem Beta.\r\n\\[ P(p)  =  \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\\]\r\nCo wiemy parametrach tego rozkładu? Prawdopodobnie nic, ale możemy zastosować metodę zwaną empirycznym Bayesem. To znaczy, wyestymujemy rozkład a priori z danych. Nie jest to ortodoksyjne podejście Bayesowskie, które zakłada zdefiniowanie rozkładów a priori przed spojrzeniem na dane. Niemniej, człowiek musi sobie jakoś radzić.\r\nKsięgarnia internetowa posiada oceny wszystkich książek, jakie są w jej posiadaniu. Załóżmy, że mamy dostęp do tych danych (zasymulujmy je) i stwórzmy histogram wszystkich ocen.\r\n\r\n\r\nset.seed(123)\r\nx = rbeta(10000,10,10)\r\nhist(x)\r\n\r\n\r\n\r\nNa histogramie widzimy jak często pojawiają się dane oceny. Rozkład jest skupiony symetrycznie wokół wartości 0.5, które występują najczęściej. Im wartości bardziej oddalone od 0.5 tym rzadziej występują. Oznacza to, że książki bardzo dobre, albo bardzo złe są rzadsze od przeciętnych.\r\nTen rozkład jest sensownym wyborem rozkładu a priori. Mówi nam jak prawdopodobne jest, że wylosujemy książkę o danej ocenie, zanim zobaczymy ocenę konkretnej pozycji.\r\nWykorzystajmy ten rozkład by dobrać wartości \\(\\alpha\\) i \\(\\beta\\). Wyestymujmy parametry rozkładu z danych:\r\n\r\n\r\nfit <- function(pars,x) {-sum(log(dbeta(x,pars[1],pars[2])))}\r\nstart <- c(1,1)\r\nnames(start) <-c('a','b')\r\nrecov <- nlminb(start, fit, x = x, lower = -Inf, upper = Inf)\r\nround(recov$par, 2)\r\n\r\n   a    b \r\n9.95 9.96 \r\n\r\nFantastycznie. Mamy wszystko czego potrzebujemy by obliczyć nasz rozkład:\r\n\\[P(p|k,n) = \\frac{P(k,n|p)P(p)}{P(k,n)} \\] Zdradzę wam teraz, że rozwiązaniem jest takie, że \\(P(p|k,n)\\) jest rozkładem Beta o parametrach \\(\\alpha = \\alpha_0 + k\\) i \\(\\beta = \\beta_0 + n - k\\), gdzie \\(\\alpha_0\\) i \\(\\beta_0\\) to parametry rozkładu a priori.\r\nAnalityczną forma \\(P(p|k,n)\\) istnieje. Rozkład Beta jest zgodnym rozkładem a priori  (conjugate prior) dla rozkładu dwumianowego. To znaczy, że w tym wypadku nie musimy przybliżać rozwiązania metodami numerycznymi, ale możemy je wprost obliczyć.\r\nJeśli kogoś interesują obliczenia, które nie są trudne, a mogą pomóc zrozumieć jak rozkłady a priori, post priori i wiarygodność są powiązane, zapraszam do rozwinięcia tekstu poniżej.\r\nKliknij mnie\r\nPodstawmy wszystkie rozkłady pod nasze równanie:\r\n\\[P(p|k,n) =\\frac{{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}}{\\int{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}dp} \\]\r\nNie wygląda zbyt zachęcająco, prawda? Ale spokojnie, rozwiązanie tego równania jest bardzo proste. Weźmy wszystkie człony równania, które nie zależą od \\(p\\) i zapiszmy je jako \\(C\\).\r\n\\[P(p|k,n) = C*p^k(1-p)^{n-k}p^{\\alpha-1}(1-p)^{\\beta-1} = C* p^{\\alpha-1 +k}(1-p)^{\\beta-1 + n - k}\\] Gdzie:\r\n\\[ C = \\frac{{{n}\\choose{k}}\\frac{1}{B(\\alpha,\\beta)}}{\\int{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}dp} \\]\r\nNo dobra, teraz mała sztuczka. Zauważmy teraz, że rozkład Beta jest proporcjonalny do \\(p^{\\alpha-1}(1-p)^{\\beta-1}\\):\r\n\\[\\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\\propto p^{\\alpha-1}(1-p)^{\\beta-1}\\]\r\nZauważmy też, że nasz w szukanym prze nas rozkładzie, \\(C\\) nie zależy od \\(p\\), jest więc jakąś wartością stałą, skalującą rozkład. \\[P(k,n|p) \\propto p^{\\alpha-1 + k}(1-p)^{\\beta-1 + k - n}\\]\r\nPrzyjmijmy, że \\(\\alpha = \\alpha_0 +k\\) i \\(\\beta = \\beta_0 + n - k\\), gdzie \\(\\alpha_0\\) i \\(\\beta_0\\) to parametry \\(\\alpha\\) i \\(\\beta\\) z powyższego wzoru. Teraz już powinniśmy zauważyć, że nasz rozkład jest proporcjonalny do rozkładu Beta. A ponieważ nasz \\(P(p|k,n)\\) jest rozkładem prawdopodobieństwa, to jego suma/całka musi równać się 1. Rozkładem proporcjonalnym do rozkładu Beta, którego suma/całka wynosi 1, jest właśnie rozkład Beta, a więc:\r\n\\[ C = \\frac{1}{B(\\alpha,\\beta)}\\]\r\nNasz rozkład a posteriori \\(P(p,|k,n)\\) jest rozkładem Beta o parametrach \\(\\alpha = \\alpha_0 + k\\) i \\(\\beta = \\beta_0 + n - k\\).\r\nWiemy już, że rozkłady \\(P(p|k,n)\\) dla każdej książki to rozkłady Beta o następujących parametrach:\r\n\\[A_p \\sim Beta(17.95, 11.96)\\] \\[B_p \\sim Beta(39.93, 19.93)\\] \\[C_p \\sim Beta(64.93, 44.93)\\]\r\nZobaczmy je na wykresie razem z wykresem a priori:\r\n\r\n\r\nlibrary(tidyverse)\r\nfun.1 = function(x){dbeta(x,17.95, 11.96)}\r\nfun.2 = function(x){dbeta(x,44.95, 24.96)}\r\nfun.3 = function(x){dbeta(x,69.95, 49.96)}\r\nfun.4 = function(x){dbeta(x,9.95, 9.96)}\r\ncolors <- c(\"A\" = \"blue\", \"B\" = \"green\", \"C\" = \"red\", \"a priori\" = \"black\")\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \r\n  stat_function(fun = fun.1, aes(color = \"A\")) + \r\n  stat_function(fun = fun.2, aes(color = \"B\")) +\r\n  stat_function(fun = fun.3, aes(color = \"C\")) +\r\n  stat_function(fun = fun.4, aes(color = \"A priori\")) +\r\n  xlab(\"p\") + ylab(\"Density\") + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"Rozkład\", values = colors) + \r\n  xlim(0,1) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nCzarna linia to wykres a priori. Symbolizuje naszą wiedzę na początku, jak prawdopodobne jest wybranie losowej książki o danej ocenie. Pozostałem linie pokazują jak zmieniły się nasze przekonania w stosunku do konkretnych książek po zobaczeniu ich oceny.\r\nMamy rozkłady, ale chcemy podjąć decyzję, a do tego potrzebujemy mieć punktowe estymaty \\(p\\). Możemy ją otrzymać na kilka sposobów. Możemy wziąć takie \\(p\\), które ma największe prawdopodobieństwo. Inną popularną estymatą jest średnia, ponieważ minimalizuje ona kwadratową funkcję błędu \\(l(p, \\hat{p}) = E((p - \\hat{p})^2)\\). Ponadto gdy nasz rozkład post priori jest symetryczny, obie te wartości są równe.\r\nPoliczmy sobie średnie rozkładów post priori.\r\n\\[E(A_p) = 0.6\\] \\[E(B_p) = 0.64\\] \\[E(C_p) = 0.58\\]\r\nJak widzimy, książka B ma największe prawdopodobieństwo bycia najlepszą książką. Co tu się właściwie zadziało? Wszystkie prawdopodobieństwa są mniejsze, niż gdybyśmy wzięli po prostu częstościową estymatę \\(\\frac{k}{n}\\).\r\nEstymata bayesowska oceny jest mniejsza od częstościowego odpowiednika dla każdej z książek. Dzieje się tak dlatego, że rozkład a priori wskazuje, że prawdopodobieństwa bliższe 0.5 - średniej rozkładu a priori są bardziej prawdopodobne i “ściąga” rozkłady post priori w swoją stronę. Jednocześnie bayesowska ocena książki C najmniej się różni od estymaty częstościowej, dlatego, że ocena książki C składa się z największej ilości głosów (obserwacji).\r\nŻeby to zwizualizować, załóżmy teraz, że wszystkie trzy książki mają 0.8 pozytywnych ocen, ale różnią się liczebnością jak w poprzednim przykładzie i popatrzmy na wykres:\r\n\r\n\r\nlibrary(tidyverse)\r\nfun.1 = function(x){dbeta(x,17.95, 11.96)}\r\nfun.2 = function(x){dbeta(x,49.95, 19.96)}\r\nfun.3 = function(x){dbeta(x,89.95, 29.96)}\r\nfun.4 = function(x){dbeta(x,9.95, 9.96)}\r\ncolors <- c(\"A\" = \"blue\", \"B\" = \"green\", \"C\" = \"red\", \"a priori\" = \"black\")\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \r\n  stat_function(fun = fun.1, aes(color = \"A\")) + \r\n  stat_function(fun = fun.2, aes(color = \"B\")) +\r\n  stat_function(fun = fun.3, aes(color = \"C\")) +\r\n  stat_function(fun = fun.4, aes(color = \"A priori\")) +\r\n  xlab(\"p\") + ylab(\"Density\") + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"Rozkład\", values = colors) + \r\n  xlim(0,1) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nIm więcej mamy danych (obserwacji), tym mniej istotne staje się prawdopodobieństwo a priori. Jednocześnie im bardziej informatywny rozkład a priori (im węższy, im więcej gęstości prawdopodobieństwa jest skupione wokół danej wartości) tym więcej danych trzeba by rozkład post priori różnił się od niego.\r\nPodsumowanie\r\nW statystyce częstościowej dane są modelowane jako zmienne losowe, a parametry traktowane są jako stałe. W analizie staramy się oszacować najlepszą możliwą punktową wartość tego parametru. Jak zauważyliśmy, analiza Bayesowska zwraca nam rozkład prawdopodobieństwa parametru, którego szukamy. Rozkład reprezentuje nasz poziom pewności co do wartości parametru. W podejściu Bayesowskim dane są stałe, a parmetry są zmiennymi losowymi. To znaczy, że szukane przez nas parametry reprezentujemy jako rozkład statystyczny, pod warunkiem zebranych przez nas danych. Gdy otrzymujemy nowe dane, możemy aktualizować naszą pewność co do prawdopodobieństwa konkretnych wartości parametrów.\r\nMam nadzieję, że pokazanymi tu przykładami udało mi się pokazać na czym polega wnioskowanie Bayesowskie. W następnej części zajmiemy się budowaniem i estymowaniem modeli Bayesowskich.\r\n\r\n\r\n\r\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2021). Bayesian data analysis. Chapman; Hall/CRC.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-31-tutorial-bayes/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-12-24T13:48:40+01:00",
    "input_file": "tutorial-bayes.knit.md"
  },
  {
    "path": "posts/2022-05-13-mixedmotivation/",
    "title": "Dlaczego liniowe modele mieszane?",
    "description": "Mała zachęta do stosowania mieszanych modeli liniowych.",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-05-14",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nDzisiejszy wpis będzie taką mini motywacją do zainteresowania się mieszanymi modelami liniowymi (aka hierarchicznymi/wielopoziomowymi modelami liniowymi).\r\nRozważmy hipotetyczny eksperyment. Badane osoby oglądają różne zdjęcia, a po po obejrzeniu oceniają jak bardzo zdjęcie było pobudzające. W trakcie badania uczesnicy mają na palcu wskazującym i środkowym elektrody mierzące aktywność skórno-galwaniczną (mikropocenie).\r\nNastępnie możemy zadać pytanie czy większa amplituda sygnału z elektrod jest związana z większym pobudzeniem. Jeśli pytamy o to, czy osoby z średnio wyższą amplitudą sygnału mają wyższą średnią ocenę (efekt międzyobiektowy), możemy po prostu uśrednić pomiary na osobę i skorelować je ze sobą.\r\nJeśli jednak zapytamy czy średnio im większy sygnał w pojedynczym pomiarze, tym wyższa ocena (efekt wewnątrzobiektowy), takie podejście może nie zadziałać. Czemu? Zasymulujmy sobie takie dane.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(lme4)\r\nlibrary(parameters)\r\nlibrary(datawizard)\r\nlibrary(cowplot)\r\n\r\n\r\nbetween_effect = 0\r\nwithin_effect = 0.5\r\nbetween_sd = 2\r\nwithin_sd = 0.1\r\n\r\nsignal_intercepts = rnorm(100,0,1) \r\nresponse_intercepts = rnorm(100,0, between_sd) + between_effect * signal_intercepts\r\n\r\ndata =  data.frame()\r\nfor (participant in 1:100){\r\n  \r\n  signal = rnorm(100,4,0.1)  \r\n  response =  within_effect*signal + rnorm(100,0,within_sd)\r\n  response = response + response_intercepts[participant]\r\n  signal = signal + signal_intercepts[participant]\r\n  data =  rbind(data, data.frame(signal,response,participant))\r\n  \r\n}\r\ndata_averaged = data %>% group_by(participant) %>% summarise(mean_signal = mean(signal), mean_response = mean(response))\r\n\r\ncat(\"Korelacja pojedynczych pomiarów\",cor(data$signal, data$response))\r\n\r\n\r\nKorelacja pojedynczych pomiarów -0.0215814\r\n\r\ncat(\"Korelacja średnich pomiarów na osobę\",cor(data_averaged$mean_signal, data_averaged$mean_response))\r\n\r\n\r\nKorelacja średnich pomiarów na osobę -0.02477697\r\n\r\nNie obserwujemy znaczących korelacji w żadnym wypadku. Zerknięcie na wykres wyjaśni tajemnicę:\r\n\r\n\r\nplot1 <- ggplot(data, aes(x = signal, y = response, colour =\r\n  as.character(participant))) + geom_point() + theme_bw() + \r\n  theme(legend.position=\"none\")\r\n\r\nplot2 <- ggplot(data_averaged, aes(x = mean_signal, y = mean_response)) +\r\n  geom_point() + theme_bw()\r\n\r\nplot_grid(plot1, plot2, labels = \"AUTO\")\r\n\r\n\r\n\r\n\r\nJeśli spojrzymy na wykres po lewej, zobaczymy różnokolorowe zgrupowane punkty. Te kolory to poszczególni badani. Jeśli się im przyjrzymy zauważymy, że są lekko przechylone w prawo, co sugeruje związek liniowy. Jednak badani są rozrzuceni po całym wykresie (duża wariancja międzyobiektowa), ponieważ występuje duże zróżnicowanie pomiędzy ich bazowymi amplitudami i ocenami. Wykres ich średnich pokazuje brak związku, ponieważ te bazowe wartości nie są ze sobą związane. Jak więc wykryć związek?\r\nA gdybyśmy tak od wartości naszych zmiennych odjęli średnią dla danego badanego?\r\n\r\n\r\ndata = data %>% group_by(participant) %>% mutate(signal_demeaned = signal - mean(signal), \r\nresponse_demeaned = response - mean(response))\r\n\r\nggplot(data, aes(x = signal_demeaned, y = response_demeaned, colour = as.character(participant))) +\r\ngeom_point() + theme_bw() +  theme(legend.position=\"none\")\r\n\r\n\r\n\r\n\r\nLepiej, co nie? Jak więc uwzględnić to w modelu liniowym?\r\nRegresja liniowa ma postać:\r\n\\[ Y_i = \\alpha + \\beta X_i\\] gdzie \\(a\\) to stała, a \\(\\beta\\) to współczynnik regresji.\r\nNasz model musimy zmodyfikować tak by brał pod uwagę średnie amplitudy badanych.\r\n\\[ Y_{ij} = \\alpha + \\beta X_i + b_j z_i\\]\r\ngdzie \\(z_j\\) to zmienna binarna oznaczająca czy dany pomiar należy do badanego \\(j\\), a \\(b_j\\) to współczynnik regresji dla \\(z_j\\). Ponieważ \\(z_i\\) przyjmuje dla badanego \\(j\\) wartość 1, a dla wszystkich innych 0, równanie możemy przepisać:\r\n\\[ Y_{ij} = (\\alpha +b_j) + \\beta X_i\\] Jak widzimy teraz nasz model uwzględnia odchylenia od stałej dla każdego badanego.\r\nZerknijmy na model mieszany:\r\n\r\n\r\nmodel = lmer(response ~ signal + (1| participant), data = data)\r\nmodel_parameters(model)\r\n\r\n\r\n# Fixed Effects\r\n\r\nParameter   | Coefficient |   SE |        95% CI | t(9996) |      p\r\n-------------------------------------------------------------------\r\n(Intercept) |       -0.02 | 0.19 | [-0.40, 0.36] |   -0.09 | 0.926 \r\nsignal      |        0.51 | 0.01 | [ 0.49, 0.52] |   50.43 | < .001\r\n\r\n# Random Effects\r\n\r\nParameter                   | Coefficient\r\n-----------------------------------------\r\nSD (Intercept: participant) |        1.89\r\nSD (Residual)               |        0.10\r\n\r\nW tabeli Fixed Effects mamy estymaty stałej \\(a\\) i współczynnika regresji \\(\\beta\\). Niżej w tabeli Random Effects widzimy estymaty odchylenia standardowego stałych \\(b\\) (wariancji międzyobiektowej) i błędu (w tym wypadku wariancji wewnątrzobiektowej). Jak widzimy współczynnik dla sygnału jest pozytywny i wynosi około 0.5. Dzięki dodaniu dodatkowych stałych \\(b_j\\) nasz model liczy teraz efekt wewnątrzobiektowy.\r\nAle czasami chcielibyśmy by liczył także efekt międzyobiektowy. Spójrzmy na taki przykład:\r\n\r\n\r\n\r\nJak widzimy na wykresie, występuje zarówno efekt międzyobiektowy, jak i wewnątrzobiektowy. Ponadto, te efekty mają przeciwny znak.\r\nPrzykładem z życia takiej sytuacji jest szybkość pisania na klawiaturze. Im szybciej średnio dana osoba pisze na klawiaturze tym rzadziej popełnia błędy. Jednak każda osoba, im relatywnie szybciej (względem swojej średniej) pisze, tym więcej błędów popełnia.\r\nModel mieszany wykryje tylko efekt wewnątrzobiektowy. Jeśli chcielibyśmy wykryć oba efekty, musimy wykonać pewną sztuczkę i rozbić zmienną niezależną na dwie (Bell et al., 2019): \\(X_{between}\\) = średnia dla danego badanego i \\(X_{within} = X - X_{between}\\).\r\n\r\n\r\ndata <- cbind(\r\n  data,\r\n  demean(data, select = c(\"signal\"), group = \"participant\")\r\n)\r\n\r\nmodel = lmer(response ~ signal_within + signal_between + (1| participant), data = data)\r\nmodel_parameters(model)\r\n\r\n\r\n# Fixed Effects\r\n\r\nParameter      | Coefficient |   SE |         95% CI | t(395) |      p\r\n----------------------------------------------------------------------\r\n(Intercept)    |       11.71 | 3.19 | [ 5.44, 17.99] |   3.67 | < .001\r\nsignal within  |        0.93 | 0.05 | [ 0.83,  1.03] |  17.86 | < .001\r\nsignal between |       -1.88 | 0.39 | [-2.65, -1.11] |  -4.81 | < .001\r\n\r\n# Random Effects\r\n\r\nParameter                   | Coefficient\r\n-----------------------------------------\r\nSD (Intercept: participant) |        5.27\r\nSD (Residual)               |        1.00\r\n\r\nDzięki temu nasz model estymuje zarówno efekt wewnątrzobiektowy, jak i międzyobiektowy.\r\nPoniżej możecie zobaczyć jak zmiana poszczególnych parametrów w symulacji (kod z początku wpisu) wpływa na to jak wyglądają dane i jak radzi sobie model mieszany.\r\n\r\n\r\n\r\n\r\nMożliwość estymacji efektów wewnątrzobiektowych i międzyobiektowych to jedna z zalet mieszanych modeli liniowych. Posiadają one jeszcze inne, ciekawe właściwości. Niemniej, tu zakończymy tą małą zachętę do stosowania mieszanych modeli liniowych.\r\nJeśli ktoś jest zainteresowany szczegółami, zarówno matematycznymi jak i bardziej praktycznymi, zachęcam do skorzystania z bazy wiedzy, gdzie znajdują się odnośniki do dobrych tutoriali i kursów omawiających tę klasę modeli.\r\n\r\n\r\n\r\nBell, A., Fairbrother, M., & Jones, K. (2019). Fixed and random effects models: Making an informed choice. Quality & Quantity, 53(2), 1051–1074.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-13-mixedmotivation/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-10-14T17:55:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-05-tranistive/",
    "title": "Większe czy równe?",
    "description": "O nieprzechodności testów statystycznych.",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2022-05-05",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nWspomnę jeszcze nie raz, że nie jestem fanem p-value. Nie dlatego, że jest ona złym sposobem oceny wniosków jakie wyciągamy z danych, ale z powodu tego, jak jest używana i nadużywana. Poruszyłem ten temat tutaj.\r\nDzisiejszy wpis będzie o sytuacji jaką możemy napotkać gdy obcujemy z danymi. Mamy taki zbiór danych z trzema zmiennymi: wartości jakiejś cechy, grupa do której obserwacja należy i unikalny identyfikator. Przeprowadzamy analizę wariancji.\r\n\r\n\r\nlibrary(ez)\r\nlibrary(knitr)\r\nlibrary(broom)\r\n\r\ndata = data.frame(value = c(1,2,3,3,4,5,5,6,7), group = c(\"a\",\"a\",\"a\",\"b\",\"b\",\"b\",\"c\",\"c\",\"c\"), id = 1:9)\r\n\r\nkable(ezANOVA(data = data, dv = value, between = group, wid = id)$ANOVA)\r\n\r\n\r\nEffect\r\nDFn\r\nDFd\r\nF\r\np\r\np<.05\r\nges\r\ngroup\r\n2\r\n6\r\n12\r\n0.008\r\n*\r\n0.8\r\n\r\nIstotny efekt! Wykonujemy więc porównania posthoc.\r\n\r\n\r\nkable(tidy(pairwise.t.test(data$value, data$group), p.adjust.method = 'holm'))\r\n\r\n\r\ngroup1\r\ngroup2\r\np.value\r\nb\r\na\r\n0.0996505\r\nc\r\na\r\n0.0081410\r\nc\r\nb\r\n0.0996505\r\n\r\nZwizualizujmy także efekt grupy.\r\n\r\n\r\nezPlot(data = data, wid = id, between = group, dv = value, x = group)\r\n\r\n\r\n\r\n\r\nMamy do czynienia z sytuacją, w której \\(c\\) jest istotnie większe od \\(a\\), natomiast \\(b\\) nie jest istotnie różne od \\(a\\) i od … \\(c\\). I co teraz? Popularna wykładnia mówi: p-value < 0.05 - efekt jest (średnie się różnią), p-value > 0.05 - efektu nie ma (średnie się nie różnią). Jednak patrząc na wszystkie 3 testy moglibyśmy dojść do wniosku, że efekt zarówno jest jak i go nie ma.\r\nOczywiście nie wydarzyło się tu nic niezwykłego. Różnica średnich pomiędzy \\(c\\) i \\(b\\) oraz \\(a\\) i \\(c\\) jest za mała, by przy tej wariancji wykazać istotny efekt. Wnioski otrzymane za pomocą testów statystycznych są nieprzechodne. Przechodność to taka własność, która mówi nam, że jeśli \\(a<b\\) i \\(b<c\\) to \\(a<c\\).\r\nJak zachować się w takiej sytuacji? Prawdopodobnie najrozsądniejszym wyjściem jest przedstawienie wyniku \\(a\\) i \\(c\\), oraz stwiedzenie, że nie mamy wiedzy, by orzekać o \\(b\\) (patrząc na przedział ufności na wykresie, widzimy, że może być zarówno bliżej \\(a\\) jak i \\(c\\)).\r\nGdybyśmy mieli do czynienia z sytuacją w której \\(a,b,c\\) się nie różnią statystycznie albo \\(a,b\\) się nie różnią statystycznie, a \\(c\\) jest od nich istotnie większe moglibyśmy szybko przedstawić nasze wnioski, bez zbędnej konsternacji.\r\nStwierdzenie, że coś jest istotnie różne jest użyteczną heurystyką tego, że coś jest naprawdę różne, jednak prosty przykład przedstawiony powyżej pokazuje nam, że nie zawsze otrzymane wyniki będą spójne. Warto pamiętać, że p-value jest tylko miarą wskazującą jak bardzo dane nie pasują do danego modelu (w tym przypadku modelu, w którym dana para średnich jest równa).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-05-tranistive/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-10-14T17:55:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-28-behrens/",
    "title": "Jak mózg monitoruje zmienność środowiska?",
    "description": "Modelowanie niepewności w procesie uczenia się wartości bodźców.",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-04-28",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nModelowanie procesów poznawczych to taka dziedzina, dzięki której możemy sięgnąć do czarnej skrzynki, jaką jest umysł. Jednym procesów, który możemy zamodelować, jest uczenie się. Choć my, ludzie, potrafimy nauczyć się czynności poprzez obserwację, czy nawet werbalny komunikat, dzielimy ze wszystkimi zwierzętami starszy (ale nie mniej przydatny) mechanizm. Mowa o warunkowaniu.\r\nPodstawowa zasada warunkowania jest prosta: Jeśli po wykonaniu jakiejś czynności otrzymujemy rezulatat pozytywny, zwiększa się prawdopodobieństwo powtórzenia tej czynności, jeśli negatywny, zmniejsza się.\r\nJednym z modeli warunkowania jest model Rescola-Wagner (Sutton & Barto, 2018), który wygląda tak:\r\n\r\n\r\n\r\nModel jest bardzo prosty. Przewidywaną wartością bodźca \\(V\\) w czasie \\(i\\) jest wartość bodzica w czasie \\(i-1\\) plus błąd predykcji, czyli różnica pomiędzy rezultatem (np. wartością nagrody), a przewidywaną wartością bodźca w czasie \\(i-1\\) , pomnożoną przez stałą \\(a\\). Od wielkości współczynnika uczenia (learning rate) \\(a\\) zależy jak szybko agent aktualizuje wartość bodźca.\r\nModel Rescola-Wagner jest dość prosty i nie odda wszystkich zjawisk zachodzących u ludzi i zwierząt podczas warunkowania. Jednak jego prostota pozwala szybko zrozumieć ideę stojącą za tą klasą modeli - uczenia ze wzmocnieniem (reinforcement learning models).\r\nTakie modele wykorzystywane są w kognitywistyce i ekonomii (uczenie się i podejmowanie decyzji), a także w biologii (uczenie ze wzmocnieniem to jedna z podstawowych zasad działania neuronów dopaminergicznych) i robotyce. Ja chciałym się skupić na badaniu przeprowadzonym przez zespół Timothego Behrensa, które wyjątkowo mi się spodobało (Behrens et al., 2007).\r\nBadani wykonywali proste zadanie, w którym mieli wybierać wiekokrotnie jedną z dwóch opcji. W pierwszej fazie (120 prób) każda z możliwości była związana ze stałym prawdopodbieństwem uzyskania nagrody. Następnie, w drugiej fazie prawdopodobieństwo uzyskania nagrody zmieniało się co 30-40 prób.\r\nCałą procedurę badani wykonywali w funkcjonalnym rezonansie magnetycznym (fMRI). Dzięki użyciu neurobrazowania, można nie tylko sprawdzić jak dobrze dany model uczenia ze wzmocniem wyjaśnia nam obserwowalne dane (w tym wypadku decyzje badanych), ale możemy sprawdzić czy mózg oblicza wartość bodźca i błąd predykcji w podobny sposób jak model (i jakie ośrodki w mózgu w tym współuczestniczą).\r\nBehrensa i jego zespół zainteresował problem zmienności środowiska. Jeśli środowisko jest stabilne (prawdopodobieństwa przypisane wyborom/opcjom nie zmieniają się), współczynnik uczenia się powinien być niższy, ponieważ nawet jeśli preferowana przez nas opcja akurat nie dostarczy nagrody, jest to raczej wynikiem przypadku. Natomiast jeśli prawdopodobieństwa związane z wyborami zmieniają się (a co za tym idzie zmienia się wartość bodźców), musimy szybko aktualizować wartości bodźców, inaczej z uporem wybieralibyśmy te, których wybranie w przeszłosci dawało często nagrodę.\r\nMoglibyśmy tak zmodyfikować model Rescola-Wagner, by współczynnik uczenia nie był stały, ale zmieniał się w czasie. Uzyskalibyśmy wtedy informację jak dana osoba dostosowuje swój współczynik uczenia. Nie wiedzielibyśmy jednak dlaczego to robi. Wielkość współczynnika uczenia będzie także zależała od innych czynników.\r\nNa przykład, gdy nie mamy wcześniejszych informacji na temat wartości bodźców, współczynnik musi być wysoki, byśmy mogli szybko nabyć jakieś przekonania, na podstawie których podejmiemy decyzję. Natomiast, wraz z postępem czasu powinien maleć (wynika to z chartakterystyki uczenia się - wraz z postępem czasu efekty uczenia maleją).\r\nJeśli chcielibyśmy wiedzieć jak bardzo współczynnik zmienia się w zależności od niestabilności (volatility), a także czy jej monitorowanie jest odzwierciedlone w sygnale fMRI (co interesowało Behrensa), musimy mieć jej jakąś estymatę.\r\nBehrens stworzył więc algorytm - Bayesowskiego Obserwatora, którego zadaniem było przewidywanie prawdopodobieństw i monitorowanie niestabilności. Uzyskał wartość niestabilności w czasie \\(i\\). Dzięki temu odkrył, że w monitorowaniu zmienności środowiska uczestniczy pewien obszar mózgu - przedni zakręt obręczy.\r\nPonieważ lubię się pobawić takimi zabawkami, zaimplementowałem go (kod można znaleźć tutaj). Wygenerowałem zmienną \\(y\\) przyjmującą wartości 0 (brak nagrody) i 1 (nagroda). Przez pierwsze 120 obserwacji prawdopodobieństwo nagrody wynosiło 90%, następnie przez 220 prób prawdopodobieństwo zmieniało się między 80% a 20% co 30 lub 40 prób. W ostatnich 120 obserwacjach prawdopodbieństwo wynosiło 10%.\r\nModel radzi sobie tak:\r\n\r\n\r\n\r\nPrzy pomocy naszego idealnego obserwatora możemy policzyć teraz niestabilność środowiska. Przy odrobinie wysiłku można zaadaptować model do innych sytuacji, bądź rozwinąć go, by używać niestabilności środowiska jako zmiennej w modelach podejmowania decyzji czy uczenia się.\r\nEstymowaną niestabilność środowiska moglibyśmy nazwać też niepewnością pierwszego poziomu. Warto wspomnieć, że istnieje model uczenia ze wzmocnieniem - Hierarchical Gaussian Filter, który uwzględnia niepewność decyzyjną \\(n\\) poziomów (właściwie tyle ile chcemy), czyli modeluje też niestabiność niestabilności - i tak dalej (Mathys et al., 2014).\r\nJak działa model?\r\nTwierdzenie Bayesa\r\nBayesowskie modele opierają się na twierdzeniu Bayesa (lub inaczej mówiąc, wzorze na prawdopodobieństwo warunkowe):\r\n\\[ P(parameters|data) = \\frac{P(data|parameters)P(parameters)}{P(data)}\\] Człon \\(P(data|parameters)\\) (wiarygodność, likehood) oznacza prawdopodobieństwo, że dany parametr wygenerował zaobserwowane dane (czyli na przykład prawdopodbieństwo tego, że jeśli prawdopodobieństwo nagrody wynosi 80%, jakie jest prawdopodobieństwo, że zaobserwujemy 1 - czyli nagrodę).\r\n\\(P(parameters)\\) nazywane prawdopodobieństem a priori, oznacza jakie jest prawdopodobieństwo danego parametru. Często opisuje się go jako wcześniejsze doświadczenie, ponieważ możemy wiedzieć, że wystąpienie jakiejś wartości parametru jest mniej prawdopodobne, niż innej (bądź ma pewnien określony rozkład statystyczny). Jeśli tego nie wiemy, \\(P(parameters)\\) przyjmują taką samą wartość dla wszystkich \\(P(data|parameters)\\).\r\n\\(P(data)\\) jest właściwie tylko parametrem skalującym (stałą), bez specjalnej interpretacji. Dlatego powyższy wzór możemy zapisać:\r\n\\[ P(parameters|data) \\propto P(data|parameters)P(parameters)\\] gdzie \\(\\propto\\) oznacza “jest proporcjonalny do”\r\nWreszcie człon \\(P(parameters|data)\\) jest prawdopodbieństwem a posteriori, czyli prawdopodbieństwem wartości parametru modelu przy zaobserowanych danych (na przykład, jeśli zaobserwowaliśmy 1, jakie jest prawdopodbieństwo, że prawdopodobieństwo otrzymania nagrody wynosi 66%).\r\nProces Markowa\r\nProces Markowa to proces stochastyczny, w którym prawdopodobieństwo zdarzenia w czasie \\(i\\) zależy tylko od stanu systemu w czasie \\(i-1\\). Czyli:\r\n\\[P(X_i = x|X_{i-1}, X_{i-2}  ... X_1) = P(X_i = x|X_{i-1})\\]\r\nObserwator Bayesowski\r\nModel Behrensa można zwizualizować następujący sposób:\r\n\r\n\r\n\\(y_{i+1}\\) czyli nagroda lub jej brak zależy tylko od parametru \\(r_{i+1}\\) - prawdopodobieństwa jej uzyskania. Z kolei parametr \\(r_{i+1}\\) zależy od siebie krok wcześniej \\(r_{i}\\) oraz od niestabilności \\(v_i\\), która sama zależy od \\(v_{i-1}\\) i stałego parametru \\(k\\), który moglibyśmy nazwać niestabilnością niestabilności. Jak widzimy, model Behrensa zakłada proces Markowa (wartości parametrów w czasie \\(i\\) zależą tylko od wartości parametrów w czasie \\(i-1\\)).\r\nSzukamy takich parametrów, które maksymalizują prawdopodobieństwo a posteriori \\(P(r_i,v_i,k|y_i)\\). Dzięki temu będziemy poznamy najbardziej prawdopodobne wartości parametrów \\(r\\), \\(v\\) i \\(k\\).\r\nUżywiając twierdzenia Bayesa, wzór na prawdopodobieństwo a posteriori wygląda tak:\r\n\\[P(r_i,v_i,k|y_i) \\propto \\int \\int P(y_i|r_i)P(r_{i}|r_{i-1},v_i) P(v_{i}|v_{i-1},k) P(r_{i-1},v_{i-1},k|y_{i-1})dv_{i-1} dr_{i-1}\\]\r\nPotrzebujemy do pełni szczęścia jeszcze poszczególnych prawdopodobieństw wystepujących we wzorze.\r\nPrawdopodobieństwo \\(P(y_i|r_i)\\), czyli otrzymanie nagrody, pod warunkiem prawdopodobieństwa \\(r_i\\) wynosi po prostu \\(r_i\\). Możemy więc zamodelować je rozkładem dwumianowym:\r\n\\[P(y_i|r_i) \\sim Binomial(r_i)\\]\r\nPrawdpodobieństwo \\(P(r_{i+1}|r_i,v_i)\\) Behrens zamodelował rozkładem Beta, jednak ja, kierowany lenistwem, zamodelowałem je rozkładem normalnym ograniczonym na przedziale (0,1): \\[P(r_{i+1}|r_{i},v_{i}) \\sim N^{(0,1)}(r_i,e^{v_{i}})\\]\r\nJak widzimy, prawdopodobieństwo \\(r_{i+1}\\) dane jest rozkładem normalnym o średniej \\(r_i\\) i wariancji \\(e^{v_{i+1}}\\). To jak bardzo prawdopodobny jest większy przeskok pomiędzy \\(r_i\\) do \\(r_{i+1}\\) zależne będzie od tego jak duża jest wariancja \\(e^{v_{i+1}}\\). Właśnie ona jest naszą ukrytą zmienną reprezentującą niestabilność.\r\nPrawdopodobieństwo \\(P(v_{i+1}|v_i,k)\\) również zamodelowane jest rozkładem normalnym:\r\n\\[P(v_{i+1}|v_i,k) \\sim N(v_i,e^{k})\\]\r\nCzyli wielkość przeskoków pomiędzy \\(v_{i+1}\\) i \\(v_{i}\\) zależna jest od parametru k - stałego dla wszystkich \\(i\\), który, a jakże, też zamodelowany jest rozkładem normalnym:\r\n\\[P(k) \\sim N(0,10^{10})\\]\r\nDo uzyskania rozkładów a posteriori użyłem metod ABC (Approximate Bayesian Computation), a konkretniej próbkowania Gibbsa (przy użyciu JAGS).\r\nPo więcej detali polecam zajrzeć do orginalnego artykułu.\r\n\r\n\r\n\r\nBehrens, T. E., Woolrich, M. W., Walton, M. E., & Rushworth, M. F. (2007). Learning the value of information in an uncertain world. Nature Neuroscience, 10(9), 1214–1221.\r\n\r\n\r\nMathys, C. D., Lomakina, E. I., Daunizeau, J., Iglesias, S., Brodersen, K. H., Friston, K. J., & Stephan, K. E. (2014). Uncertainty in perception and the hierarchical gaussian filter. Frontiers in Human Neuroscience, 8, 825.\r\n\r\n\r\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (pp. 346–349). MIT press.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-28-behrens/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-10-14T17:55:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-15-seria-jak-rozumie-nauk-metodologia-bada-i-statystyka/",
    "title": "Metodologia badań i Statystyka",
    "description": "Seria: Jak rozumieć naukę?",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "Refleksja"
    ],
    "contents": "\r\n\r\nContents\r\nStatystycznie rzecz biorąc\r\nJak zbadać wszystich Polaków?\r\nRóżnią się czy nie?\r\nKrysys Replikacyjny\r\n\r\nZwykle kursy z metodologii w szkołach wyższych zaczyna się od filozofii nauki. Wprowadza ona należyty kontekst, historię refleksji naukowej. Przedstawia się rolę teorii i modeli w procesie wyjaśniania rzeczywistości. Następnie wyjaśnia się założenia i metody stosowane w dziedzinie nauki, którą ktoś akurat studiuje. My te i inne tematy odłożymy sobie na potem, a serię jak rozumieć naukę zaczniemy od przyjrzenia się statystyce i metodologii badań.\r\nStatystycznie rzecz biorąc\r\nStatystyka to dziedzina nauki pozwalająca na formalne (to znaczy w języku matematyki) przedstawienie interesujących nas zjawisk. Często jest przedmiotem znienawidzonym przez studentów. Uznawana za nudną i trudną, często uczona przez wykładowców, którzy sami ją za taką uważają, co zdecydowanie nie pomaga w jej przyswajaniu. Zdecydowaliśmy się zacząć od statystyki, ponieważ jest ona wykorzystywana właściwie we wszystkich naukach empirycznych (choć niektórzy fizycy zapewne z pogardą pokręciliby głową). Dobrze podsumował to matematyk John Tukey: „Najlepszą rzeczą w byciu statystykiem jest to, że możesz grzebać w podwórku u wszystkich innych” (Lin et al., 2014).\r\nZacznijmy więc zagłębiać się w meandry statystyki. Najpierw musimy wykonać pomiar. Pomiar oznacza przypisanie cesze pewnego obiektu wartości liczbowej. Np. ile waży człowiek w kilogramach, na kogo zagłosuje w najbliższych wyborach lub jaki jest poziom jego otwartości na nowe doświadczenia. Następnie możemy sprawdzić czy pomiędzy mierzonymi zmiennymi zachodzi jakiś związek. Na przykład, gdybyśmy przyjrzeli się danym dotyczącym wieku i wzrostu ludzi do 21 roku, zauważylibyśmy, że im więcej ktoś ma lat, tym jest wyższy. Związek, w którym wartość jednej zmiennej wzrasta i wartość drugiej zmiennej także wzrasta (bądź maleje) nazywamy korelacją liniową.\r\nIstnieją oczywiście inne, bardziej subtelne związki pomiędzy zmiennymi, ale na razie zostańmy przy relacji liniowej. Pewną mantrą, którą słyszy się na zajęciach ze statystyki jest: „korelacja nie oznacza przyczynowości”. Weźmy taki przykład: okazuje się, że im więcej sprzedaje się lodów tym częściej ludzi atakują rekiny. Czy spożycie lodów w jakiś sposób zachęca rekiny do ataku? Oczywiście nie, pozorny związek pomiędzy tymi dwoma zjawiskami wyjaśnia fakt, że ludzie najczęściej kupują lody w upalne dni, a w upalne dni więcej ludzi odpoczywa na plaży, co przekłada się na częstsze ataki rekinów.\r\nGdy mamy więc dane obserwacyjne (nasze wyniki pomiarów) nie możemy mówić o przyczynowości, gdyż widzimy tylko współzmienność cech obiektów, które badamy. By wnioskować o przyczynowości musimy posłużyć się eksperymentem. W warunkach kontrolowanych dzielimy losowo badane przez nas obiekty na dwie grupy (bądź więcej), zapewniamy im możliwie jak najbardziej zbliżone warunki, a następnie w jednej z nich (zwanej grupą docelową) wykonujemy jakąś interwencję X (np. podajemy lek), a w drugiej grupie (nazywanej kontrolną) nie. Następnie w obu grupach wykonujemy pomiar. Jeśli pomiary w grupach się różnią, zakładamy, że różnicę mogła spowodować tylko interwencja X.\r\nCzęsto jednak nie możemy posłużyć się eksperymentem, bo na przykłąd: jego koszt jest zbyt wysoki, wykonanie byłoby nieetyczne, lub jest to po prostu niemożliwe. Wtedy, by odnaleźć przyczynowość, posługujemy się dodatkowymi metodami, takimi jak wykorzystanie istniejących teorii, zgodność z modelem, wyeliminowanie innych potencjalnie wpływających zmiennych, modelami zwierzęcymi czy obserwacją następstw czasowych w badaniach podłużnych. Żadna z tych metod nie daje jednak takiej pewności we wnioskowaniu o przyczynowości jak eksperyment. Pewną nadzieję na rozwiązanie tego problemu dają rozwijane od kilkunastu lat metody matematyczne pozwalające na wnioskowanie o przyczynowości z danych obserwacyjnych. Zainteresowanym tym tematem polecam książkę „Przyczyny i Skutki” Jude’a Pearl’a i Dany Mackenzie (Pearl & Mackenzie, 2021).\r\nJak zbadać wszystich Polaków?\r\nZajmijmy się teraz kolejnym istotnym problemem, mianowicie, jak możemy uogólniać (generalizować) wyniki badań. Załóżmy, że chcemy powiedzieć coś o wadze w populacji Polaków, więc przeprowadzamy badanie. Mierzymy wagę dziesięciu ludzi napotkanych na ulicy, ale od razu napotykamy problem. Otrzymaliśmy dokładne informacje o wadze tylko tych 10 ludzi. Skąd mamy pewność, że ich średnia waga jest jakkolwiek zbliżona do średniej w populacji? Może należałoby zwiększyć próbę? Sto, może tysiąc osób wystarczyłoby aby uzyskać odpowiednie przybliżenie. Taką logiką kierował się magazyn „Literary Digest”, który przeprowadził sondaż w wyborach prezydenckich w Stanach Zjednoczonych w 1936 roku. Próba wynosiła ponad dwa miliony ludzi. W sondażu zwyciężył Alf Landon z 57% poparciem. Jednak w wyborach zwyciężył Franklin Delano Roosevelt z 61% poparciem, gdy Alf Landon otrzymał jedynie 8% głosów. Jak to możliwe? „Literary Digest” losował respondentów z książek telefonicznych. Jednak w 1936 roku telefon posiadały raczej osoby zamożne, dlatego wyniki sondażu były bardzo skrzywione. Większość ludzi w dobie największego kryzysu ekonomicznego w Stanach, głosowało na prosocjalne reformy Roosevelta (Babbie, 2008).\r\nA jednak dzisiejsze sondaże są zadziwiająco dokładne. Jak się to dzieje? Otóż zachowanie matematycznych obiektów, a takimi są nasze pomiary, można policzyć. Jeśli każdy obiekt w populacji (np. Polaków) ma niezerowe, takie samo prawdopodobieństwo wylosowania do próby, możemy policzyć ile osób musimy wylosować, by z prawdopodobieństwem X wylosować próbę, w której średnia wartość cechy nie będzie różna więcej niż Y od średniej wartości cechy w populacji.\r\n\r\nUmożliwia nam to jedno z najważniejszych twierdzeń w statystyce: Centralne Twierdzenie Graniczne.\r\nW praktyce wygląda to tak, że gdybyśmy chcieli mieć 95% szansę na wylosowanie próby, w której procent respondentów głosujących na daną partię, nie różni się więcej niż o 3% od realnego poparcia w społeczeństwie, powinniśmy wylosować 1067 osób do próby. Ponieważ każdy Polak musi mieć szansę na wylosowanie, próbę powinniśmy losować ze zbioru, który zawiera wszystkich Polaków, na przykład zbioru numerów PESEL.\r\nPróbę, do której obiekty zostały przydzielone w sposób losowy z całej populacji nazywamy próbą reprezentatywną. Próbę w której wszystkie obiekty zostały wylosowane z takim samym prawdopodobieństwem nazywamy doborem prostym losowym. Istnieją jeszcze inne metody doboru próby reprezentatywnej. Wszystkie łączy to, że możemy dokładnie policzyć margines błędu dla wyników.\r\nSpotkałem się kiedyś z opinią, że jeśli ankieter stanie w centrum miasta i będzie podchodził do niektórych ludzi i prosił ich o wypełnienie ankiety, mamy do czynienia z losowym doborem do próby. Ankieter przecież nie zna tych ludzi, nie może więc dobierać ich sobie według ich poglądów. Niemniej, ludzie Ci znaleźli się w tym konkretnym miejscu, o tej konkretnej godzinie w sposób nieprzypadkowy. Przy uczelni będzie więcej studentów, w godzinach szczytu będzie więcej osób zmierzających do pracy, i tak dalej.\r\nA jednak wśród badań naukowych próby reprezentatywne to znaczna mniejszość. Jest to spowodowane wieloma czynnikami. Przede wszystkim, przeprowadzenie badania reprezentatywnego jest kosztowne i trudne. Uzyskanie dostępu do listy zawierającej dane wszystkich obywateli wymaga przejścia wielu czasochłonnych procedur. Po drugie, o ile w wypadku ludzi istnieje jakaś lista, w przypadku badań nad zwierzętami nic takiego nie ma. Biolodzy nie mają skąd wylosować reprezentatywnej próby szczurów. Chemicy i fizycy mają w tym względzie nieco łatwiej. Atom wodoru na wsi będzie identyczny z atomem wodoru w mieście.\r\nBy poradzić sobie z tym problemem stosuje się różne metody. W dniu wyborów, po zakończeniu zbierania głosów, zwykle o godzinie 21 ogłasza się sondażowe wyniki wyborów, metodą exit poll. Badanie to nie jest przeprowadzone na próbie reprezentatywnej, ponieważ ankieterzy muszą zadać pytanie o oddany głos zaraz po wyjściu wyborcy z lokalu wyborczego. A jednak znowu mamy do czynienia z niezwykłą dokładnością. Badacze wyszczególniają zmienne silnie skorelowane z preferencjami wyborczymi, na podstawie wcześniejszych badań reprezentatywnych. Takie zmienne to zwykle płeć, wielkość miejscowości zamieszkania, poziom wykształcenia czy wielkość dochodów. Respondenci są dobierani tak, by procentowo liczba osób wykształconych odpowiadała tej w populacji itd. Przy poprawnie dobranych założeniach pozwala to na dokładne oszacowanie wyników wyborów. Biolodzy współcześnie w badaniach nad zwierzętami podobnie starają się brać pod uwagę wewnątrzgatunkowe zróżnicowanie genetyczne, środowisko zwierzęcia i jego historię, by stworzyć próbę, której wyniki będą dały się generalizować (Farrar et al., 2021).\r\nJednocześnie w niereprezentatywnych próbach należy uznać, że wpływ pewnych potencjalnych źródeł zmienności jest zaniedbywalny, to znaczy nie ma znaczenia dla wyniku badań. Na przykład, że mechanizm molekularny skurczu mięśnia poprzecznie prążkowanego jest niezależny od tego czy ktoś mieszka w Warszawie czy San Francisco. Jednak czasami takie założenia są błędne. Psychologowie długo twierdzili, że badają uniwersalne mechanizmy zachowania i myślenia ludzkiego, co miało uzasadniać mało zróżnicowane próby składające się głównie ze studentów psychologii (obecnie w ramach psychologii międzykulturowej zwraca się uwagę na różnice wynikające z kultury) (Hanel & Vione, 2016). Problemy z doborem próby miały także inne dziedziny nauki, jak biologia i medycyna, co jest jednym z powodów kryzysu replikacyjnego, który omówimy w dalszej części tego tekstu.\r\nRóżnią się czy nie?\r\nNo dobrze. Jak zauważyliśmy, dobór próby do badania to nietrywialna sprawa. Załóżmy jednak, że dobraliśmy naszą próbę odpowiednio i teraz chcemy powiedzieć czy średnia waga mężczyzn i kobiet się różni. Nie wystarczy jednak tylko spojrzeć na średnią mężczyzn i kobiet z naszej próby i zobaczyć, która średnia jest większa. Ponieważ losowaliśmy naszych badanych, nawet jeśli średnia waga w populacji jest taka sama dla kobiet i mężczyzn, prawie na pewno nie otrzymamy takich samych średnich w naszej próbie (zawsze będzie między nimi jakaś różnica). Jak więc określić czy otrzymane wyniki w próbie istotnie się różnią w populacji, czy różnica jest wynikiem błędu losowego.\r\nI tu statystycy opracowali kolejną ciekawą sztuczkę. Na podstawie wyników otrzymanych w próbie jesteśmy w stanie oszacować prawdopodobieństwo tego, że jeśli w populacji mężczyźni i kobiety nie różnią się wagą, to na ile jest prawdopodobne wylosowanie próbek różniących się o określoną wartość. Jeśli jest to wysoce nieprawdopodobne (zwykle przyjmuje się, że to prawdopodobieństwo wynosi mniej niż 5%) przyjmuje się to jako argument, że różnica rzeczywiście istnieje w populacji. Prawdopodobieństwo to nazywa się wartością p (p-value), a jeśli jest mniejsze od przyjętego progu (dopuszczalnego prawdopodobieństwa otrzymania wyniku fałszywie pozytywnego), wynik nazywamy istotnym statystycznie.\r\nP-value możemy obliczyć dla większości hipotez, nie tylko o różnicy średnich, np. czy zmienne są skorelowane liniowo, szacując prawdopodobieństwo otrzymania wyniku w próbie, który w populacji nie istnieje. Nie jest to jedyny sposób weryfikacji hipotez statystycznych, niemniej z powodów historycznych oraz faktu, że obliczenie p-value jest stosunkowo proste, jest to najczęstsza metoda stosowana w praktyce.\r\nNiemniej wokół użycia p-value narosło wiele kontrowersji. Wynika to między innymi z automatycznego korzystania z tej miary przez naukowców traktujących statystykę jako mechaniczny sposób na wskazanie czy badanie potwierdza daną tezę. Z jednej z możliwości weryfikacji hipotez statystycznych p-value stała się obowiązjuącym standardem w badaniach, choć nie zawsze jej użycie jest najbardziej adekwatne (Gigerenzer, 2004).\r\nPrzeszliśmy od doboru próby do prostej analizy statystycznej otrzymanych wyników. Czy to jednak zawsze wystarczy by otrzymać rzetelną informację na temat interesującego nas efektu? Niestety sprawa jest bardziej skomplikowana. Rozpatrzmy następujący przykład, przeprowadzamy badanie na podstawie danych obserwacyjnych w którym chcemy określić wpływ aktywności fizycznej na gęstość kości. Liczymy wskaźnik korelacji liniowej, jednak ku naszemu zdziwieniu analiza pokazuje brak związku, choć z innych badań wiemy, że wraz ze wzrostem częstotliwości uprawiania sportu, gęstość kości powinna rosnąć.\r\nUżywamy więc innej metody statystycznej – regresji liniowej, która pozwala na uwzględnienie wpływu więcej niż jednej zmiennej na zmienną, która nas interesuje (gęstość kości). Do analizy używamy teraz nie tylko częstotliwości aktywności fizycznej, ale także wagę w kilogramach. Okazuje się, że obydwie zmienne są pozytywnie skorelowane z gęstością kości (im większa częstotliwość aktywności fizycznej bądź waga, tym większa gęstość kości). Jednak jeśli analizować zmienne z osobna, nie wykryjemy związku. Dzieje się tak dlatego, że częstotliwość aktywności fizycznej i waga są ze sobą negatywnie skorelowane (im częstsza aktywność, tym mniejsza waga). Oznacza to, że większa gęstość kości u osób aktywnych jest równoważona przez większą gęstość kości u osób ważących więcej, przez co wydaje się, na pierwszy rzut oka, że aktywność fizyczna nie ma związku z gęstością kości.\r\nZauważmy, że gdybyśmy przeprowadzili eksperyment, nie spotkalibyśmy się z podobnym problemem, ponieważ tylko jedna grupa uprawiałaby wzmożoną aktywność fizyczną, co zwiększyłoby średnią gęstość kości. Choć dane obserwacyjne nie pozwalają nam mówić o przyczynowości, za pomocą metod statystycznych jesteśmy w stanie odseparować wpływ poszczególnych zmiennych.\r\nNa koniec tej części warto wspomnieć o interpretacji wyników analizy statystycznej. Ponownie załóżmy, że przeprowadzamy eksperyment, w którym interesuje nas wpływ alkoholu na zachowania agresywne. Dobieramy próbę, grupie docelowej podajemy alkohol, a następnie mierzymy częstotliwość zachowań agresywnych. Podczas analizy naszych wyników widzimy, że średnia częstotliwość zachowań agresywnych jest istotnie wyższa w grupie, której podaliśmy alkohol. Z tego punktu wiedzie prosta droga do wniosku, że alkohol powoduje agresję.\r\nNie jest to jednak wniosek poprawny. Jeśli dołączymy do analizy zmienną wskazującą na tendencję do zachowań agresywnych, okaże się, że częstotliwość agresji wzrasta po alkoholu, ale proporcjonalnie do wcześniej istniejącej skłonności do agresji (Chiavegatto et al., 2010). Otrzymaliśmy zgoła inny wniosek, mówiący nam, że alkohol działa jako wyzwalacz agresji, jednak jej nie powoduje. Podczas przeprowadzania badań należy pamiętać o tym, że wniosek statystyczny (średnia ilość zachowań agresywnych wzrasta), jest czymś innym od wniosku interpretacyjnego (alkohol powoduje agresję).\r\nKrysys Replikacyjny\r\nZwróćmy uwagę teraz na większy obraz wynikający z naszych rozważań nad metodologią statystyczną. Załóżmy, że w idealnym świecie, gdzie badacze idealnie przygotowują swoje eksperymenty na doskonałych próbach, średnio 5% badań, w których badany efekt nie istnieje, będzie zawierało fałszywe wyniki. Czyli jeśli na temat jakiegoś zjawiska pojawiło się wystarczająco dużo badań, to zawsze znajdziemy artykuł przedstawiający dane za, jak i przeciw danej tezie. Dlatego jeśli ktoś przedstawia nam badanie popierające jakąś tezę, niekoniecznie oznacza to, że ma rację. W nauce ważne jest gromadzenie (akumulacja) dowodów i powtarzanie (replikacja) badań. Duże role odgrywają w tym metaanalizy czyli wtórna analiza statystyczna wyników wielu badań w celu określenia istotności dowodów przemawiających za danym zjawiskiem.\r\n\r\nUpraszczam tu sprawę, ponieważ mówię tylko o prawdopodobieństwie wyników fałszywie pozytywnych, a należałoby uwzględnić także sytuacje, w których nie wykryto efektu faktycznie istniejącego, czyli fałszywych wynikach negatywnych. Realnie prawdopodobieństwo otrzymania wyników nieprawdziwych jest większe.\r\nNo właśnie. Tak by było w idealnym świecie. Jednak realnie mamy do czynienia z efektem złudzenia publikacyjnego, czyli skłonności wydawców do publikacji artykułów zawierających wyniki świadczące o istnieniu efektu. Brak efektu nie jest sexy, bo w badaniu nie wyszło nic interesującego. Doprowadziło to sytuacji, w której badania przedstawiające jakiś efekt były znacznie częściej publikowane niż te, które go nie pokazywały. A tak jak zauważyliśmy wcześniej zawsze pewien odsetek badań fałszywie pokaże nam istnienie efektu, którego nie ma. Może prowadzić to do sytuacji, w której dobrze udokumentowany efekt faktycznie nie istnieje. Za przykład może posłużyć zjawisko zagrożenia stereotypem z psychologii społecznej polegające na obniżeniu sprawności wykonywania zadania przez osoby należące do grupy objętej negatywnym stereotypem, gdy wcześniej „przypomni” im się o istnieniu stereotypu. Na przykład jeśli kobietom (niekoniecznie wprost) powie się, że kobiety są gorsze z matematyki, a następnie przeprowadzi się test, okaże się, że wypadły gorzej niż mężczyźni.\r\nMimo licznych badań potwierdzających ten efekt, metaanalizy przeprowadzane od 2015 roku wykazały istnienie złudzenia publikacyjnego (Flore & Wicherts, 2015). Wykrycie złudzenia publikacyjnego jest możliwe dzięki temu, że wyniki badań (w formie liczb) powinny zachowywać się (statystycznie) w określony sposób. Od kilkunastu lat rozwijają się metody statystyczne pozwalające na wykrywanie tendencyjności w publikowaniu artykułów. Złudzenia publikacyjne stanowią jeden z problemów składających się na wspomniany wcześniej kryzys replikacyjny.\r\nW 2015 roku w prestiżowym czasopiśmie naukowym „Science” ukazały się wyniki badania, w którym autorzy powtórzyli 100 nigdy wcześniej niereplikowanych badań psychologicznych. Tylko w 39% z nich udało się otrzymać wynik taki jak w oryginalnych publikacjach (Open Science Collaboration, 2015). To i podobne badania, wraz z rosnącą świadomością złudzeń publikacyjnych, zapoczątkowało debatę na temat kryzysu replikacyjności i rzetelności współczesnych praktyk naukowych. Szybko okazało się, że problem nie dotyczy tylko psychologii, lecz również biologii, ekonomii i medycyny.\r\nKryzys replikacyjny ma wiele źródeł, jednak najczęściej wymienia się zjawisko “publikuj lub zgiń” (publish or perish). Współczesna polityka ewaluacji pracownika naukowego i jego dorobku opiera się na publikacjach. Im więcej jest publikacji, im częściej są cytowane przez innych badaczy, w im lepszych czasopismach są publikowane, tym lepiej. Od wyniku ewaluacji zależy zatrudnienie i finansowanie badań naukowca. Presja publikacyjna wywierana na badaczy w połączeniu z tendencyjnością publikowania tylko “interesujących” wyników sprawiła, że niekiedy popełniają oni pewne metodologiczne nadużycia. Jakie to nadużycia? Pomoże nam to wyjaśnić martwy łosoś w rezonansie magnetycznym.\r\nZespół neuronaukowców pod kierownictwem Craiga Bennetta przygotowywał się do przeprowadzenia badania dotyczącego przetwarzania w mózgu emocjonalnych zdjęć przy użyciu funkcjonalnego rezonansu magnetycznego (fMRI). Urządzenie to działa jak trójwymiarowa kamera pozwalająca na rejestrowanie zmian w natężeniu pola magnetycznego w voxelach (trójwymiarowych odpowiednikach pikseli). W zależności od tego czy krew jest utlenowana czy odtlenowana ma inne właściwości magnetyczne, pozwala to sprawdzać poziom utlenowania krwi w różnych obszarach mózgu. Im większą aktywność wykonuje dany region mózgu, tym więcej tlenu zużywa. Pozwala to na sprawdzenie jakie obszary mózgu są bardziej aktywne w różnych warunkach eksperymentalnych.\r\nBennett przed wykonaniem właściwych badań zdecydował się na przetestowanie procedury wkładając do skanera trzykilogramowego martwego łososia. W trakcie pomiaru wyświetlano na specjalnym ekranie zdjęcia, tak jak miałoby to miejsce podczas prawdziwego badania, po czym zadawano łososiowi pytania, jakie emocje prezentują osoby na zdjęciach. Po przeanalizowaniu danych okazało się, że mózg martwego łososia wykazuje zwiększoną aktywność pewnych regionów podczas oglądania zdjęć, niż w spoczynku (Bennett et al., 2009).\r\nWynikało to ze zjawiska znanego w statystyce jako problem wielokrotnych porównań. W badaniach przy użyciu fMRI porównuje się aktywność w każdym voxelu pomiędzy warunkami. Voxeli w skanie fMRI jest kilkaset tysięcy. Wykonując tyle porównań prawdopodobieństwo, że wyjdzie nam choć jeden wynik fałszywie pozytywny wynosi niemal 100%. Prawdopodobieństwo, że w każdym pojedynczym porównaniu wystąpi wynik fałszywie pozytywny nadal wynosi 5%, ale jeśli średnio 5% wokseli pokaże nam różnice pomiędzy warunkami, to wykryjemy aktywność nawet u martwego łososia (przez szum wynikający z fluktuacji pola magnetycznego). By poradzić sobie z tym problemem stosuje się współcześnie odpowiednie poprawki, które utrzymują niższe prawdopodobieństwo otrzymania wyniku fałszywie pozytywnego.\r\nCo ma więc martwy łosoś do zjawiska “publikuj lub giń?” Naukowcy w swoich badaniach zwykle zbierają więcej danych, niż te które są potrzebne do weryfikacji a priori postawionych hipotez (pytań badawczych, które stanowiły motywację do przeprowadzenia badania). Dodatkowe dane zbiera się w celu kontrolowania wpływu zmiennych potencjalnie wpływających na efekt, jak wspominaliśmy powyżej. Ponadto, z uwagi, że zwykle przeprowadzenie badania jest kosztowne i czasochłonne zbiera się jak najwięcej danych w celach eksploracyjnych (dodatkowych analiz nie wynikających bezpośrednio z hipotez). Presja publikacyjna, razem z tendencją wydawnictw do publikowania artykułów potwierdzających efekt niż mu zaprzeczających, sprawia, że jeśli badacze nie potwierdzą swoich założonych hipotez, szukają w danych jakiegoś istotnego efektu, by zwiększyć szansę na publikację.\r\nW tym momencie powracamy do problemu wielokrotnych porównań. Istnieją oczywiście odpowiednie statystyczne środki zaradcze, niestety czasami badacze by zwiększyć sexapill artykułów przedstawiają uzyskane istotne wyniki tak, jakby odpowiadały na ich pytanie badawcze postawione przed jego przeprowadzeniem badania. Pomijają także w tekście analizy, które nie wykrywały efektu. Nieodpowiedni dobór próby, tendencyjność publikacyjna i presja kładziona na badaczy składa się na kryzys replikacyjny. Od kilku lat stosuje się coraz częściej środki zaradcze w postaci częstszej publikacji replikacji, czasopism, które publikują tylko artykuły mówiące o braku efektu czy prerejestracji - przedstawieniu do wiadomości publicznej w formie krótkiego artykułu, hipotez badawczych i metodologii przed zaczęciem badania.\r\nCzy kryzys replikacyjny oznacza, że nie możemy ufać nauce? Nie. Nauka nadal pozostaje najlepszym narzędziem do poznawania otaczającego nas świata i jest procesem samokrytycznym, to znaczy metody i założenia stosowane w nauce są poddawane ciągłej krytyce przez środowisko naukowe. Należy tu dodać, że kryzys replikacyjny może być związany z umasowieniem nauki. Liczba osób otrzymujących doktoraty stale się zwiększa, i to w państwach, w których liczba ludności spada. Standaryzacja metod oceny pracowników naukowych jest potrzebna, jednak w obecnej formie może nasilać omawiany problem.\r\nNa tym kończymy pierwszy wpis serii “Jak rozumieć naukę.” Dotknęliśmy tylko wierzchołka góry lodowej jeśli chodzi o zagadnienia statystyczne i metodologiczne w nauce, jednak mam nadzieję, że ten tekst będzie stanowił interesujące wprowadzenie do zagadnień metodologicznych. Osobom zainteresowanym poszerzeniem swojej wiedzy polecamy przejrzenie zasobów w bazie wiedzy.\r\n\r\n\r\n\r\nBabbie, E. (2008). Podstawy badań społecznych (pp. 209–210). PWN.\r\n\r\n\r\nBennett, C. M., Miller, M. B., & Wolford, G. L. (2009). Neural correlates of interspecies perspective taking in the post-mortem atlantic salmon: An argument for multiple comparisons correction. Neuroimage, 47(Suppl 1), S125.\r\n\r\n\r\nChiavegatto, S., Quadros, I., Ambar, G., & Miczek, K. (2010). Individual vulnerability to escalated aggressive behavior by a low dose of alcohol: Decreased serotonin receptor mRNA in the prefrontal cortex of male mice. Genes, Brain and Behavior, 9(1), 110–119.\r\n\r\n\r\nFarrar, B. G., Voudouris, K., & Clayton, N. S. (2021). Replications, comparisons, sampling and the problem of representativeness in animal cognition research. Animal Behavior and Cognition, 8(2), 273.\r\n\r\n\r\nFlore, P. C., & Wicherts, J. M. (2015). Does stereotype threat influence performance of girls in stereotyped domains? A meta-analysis. Journal of School Psychology, 53(1), 25–44.\r\n\r\n\r\nGigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587–606.\r\n\r\n\r\nHanel, P. H., & Vione, K. C. (2016). Do student samples provide an accurate estimate of the general public? PloS One, 11(12), e0168354.\r\n\r\n\r\nLin, X., Genest, C., Banks, D. L., Molenberghs, G., Scott, D. W., & Wang, J.-L. (2014). Past, present, and future of statistical science (p. 44). CRC Press.\r\n\r\n\r\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251).\r\n\r\n\r\nPearl, J., & Mackenzie, D. (2021). Przyczyny i skutki rewolucyjna nauka wnioskowania przyczynowego. Copernicus Center Press.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-seria-jak-rozumie-nauk-metodologia-bada-i-statystyka/naser-tamimi-yG9pCqSOrAg-unsplash.jpg",
    "last_modified": "2022-03-26T19:41:29+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-13-o-odpowiedzialnoci-za-czyny-w-dobie-lotw-na-marsa/",
    "title": "O odpowiedzialności za czyny w dobie lotów na Marsa",
    "description": "Opowieść o wolnej woli",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": {}
      }
    ],
    "date": "2021-09-13",
    "categories": [
      "Refleksja"
    ],
    "contents": "\r\n\r\nContents\r\nBezwolne maszyny\r\nZłudzenie konieczne\r\nNie ma odpowiedzialności, nie ma kary\r\nEpilog\r\n\r\nW Polsce (i na świecie) nie brakuje przykładów postaw społecznych sprzecznych współczesnej wiedzy naukowej. Za przykład mogą posłużyć postawy anty-GMO, anty-szczepieniowe, katastrofa klimatyczna, które są prawdopodobnie najbardziej znane. Jednak poza nimi wiele aspektów państwa i społeczeństwa funkcjonuje “obok” refleksji naukowej. Przyjrzyjmy się mniej popularnemu zagadnieniu - problematyce wolnej woli.\r\nBezwolne maszyny\r\nW XIX wieku fizyka święciła triumfy, zwłaszcza w dziedzinach elektryczności i magnetyzmu. Émilie du Châtelet i James Joule niezależnie sformułowali zasadę zachowania energii. Michael Faraday odkrył zjawisko indukcji elektromagnetycznej, Georg Ohm opisał związek pomiędzy natężeniem a napięciem prądu elektrycznego. James Maxwell połączył elektryczność i magnetyzm w jedną dziedzinę - elektromagnetyzm. Emil du Bois-Reymond odkrył, że informacja w układzie nerwowym przekazywana jest przez sygnały elektryczne. XIX wiek obfitował również w nowe wynalazki powstałe dzięki rozwijającej się wiedzy, m.in.: oświetlenie elektryczne, kolej parowa, telegram, telefon czy pierwsze aparaty fotograficzne.\r\nNiezwykłe dokonania naukowe oraz zdolność fizyki do dokładnego przewidywania zjawisk przyrodniczych zmusiła współczesnych do refleksji nad naturą rzeczywistości. Według ówczesnej wiedzy, rzeczywistość fizyczna rządzona była przez stałe i uniwersalne prawa fizyki. Znając je i dysponując odpowiednią aparaturą, można było przewidzieć z absolutną dokładnością dowolne zjawisko fizyczne, pomniejszoną tylko o błąd pomiaru.\r\nPopularną metodą rozwiązywania sporów jest rzut monetą. To prosty, a przede wszystkim losowy sposób na decyzję pomiędzy dwoma możliwościami. Jednak zgodnie z prawami fizyki klasycznej, losowość rzutu monetą nie wynika z jakiejś rzeczywistej losowości. Przypadek jest tylko niedostatkiem informacji. Biorąc pod uwagę wszystkie parametry fizyczne: siłę wyrzutu, prędkość kątową, wiatr, grawitację etc, można by obliczyć czy moneta upadnie awersem czy rewersem do dołu. Fizycy twierdzili, że wszystkie zdarzenia w świecie fizycznym, a więc i działanie człowieka, są powiązane związkiem przyczynowo-skutkowym - to znaczy, obecne zdarzenia miały być zdeterminowane przez zdarzenia przeszłe.\r\nPierre-Simon Laplace w roku 1814 dokonał pewnego eksperymentu myślowego: jeśli wyobrazimy sobie istotę - demona, który zna wszystkie obecne parametry fizyczne każdego atomu we wszechświecie, będzie on w stanie przewidzieć wszystko co się zdarzy w przyszłości. Oznaczałoby to, że wszechświat jest całkowicie zdeterminowany, istnieje tylko jedna przyszłość, do której wszyscy zmierzamy, a nasze działania są tylko skutkami zdarzeń z dalekiej przeszłości.\r\nTaki obraz świata nie był przychylny koncepcji wolnej woli - idei ciężkiej do zdefiniowania, a w uproszczeniu oznaczającej, że człowiek, pomijając ograniczenia fizyczne, jest w stanie całkowicie niezależnie decydować o swoich czynach. Jedyną przyczyną jego zachowania jest świadoma decyzja,która sama nie ma przyczyny - przynajmniej w świecie fizycznym. Termin ten ma silne ugruntowanie zarówno w tradycji chrześcijańskiej, jak i filozoficznej. Kartezjańska opozycja umysłu i materii na lata zakorzenił się tak w myśli powszechnej jak i naukowej. Obie tradycje odwołują się do niematerialnego bytu - duszy bądź umysłu, który nieograniczony zasadami obejmującymi materię jest zdolny do ‘’wolnej’’ decyzji. Choć współcześnie rzadziej widać odwołania do niematerialnego umysłu, wciąż widać wpływ dualizmu kartezjańskiego np. w opozycji myślenia racjonalnego i emocji (Damasio, 2011). Idea wolnej woli jest dla nas naturalna i większość z nas doświadcza poczucia sprawczości objawiającego się przekonaniem, że to my jesteśmy przyczyną działania np. ruchu ręki.\r\nKoncept wolnej woli krytykowany był również od strony filozoficznej. Artur Schopenhauer zauważył, że ludzie w swojej naiwności dowodzą wolności swej woli twierdząc: “mogę robić co chcę.” Schopenhauer zapytał wtedy: skoro można robić co się chce, czy można chcieć co się chce? Nawet jeśli przyjęlibyśmy odpowiedź twierdzącą, pytanie można rozszerzyć do: „czy można chcieć tego, co chce się chcieć?”. W ten sposób Schopenhauer pokazał, że nie mamy wpływu na nasze ‘’chcenia’’, które są przyczyną naszych działań (Schopenhauer, 1991).\r\nOdkrycie fizyki kwantowej, że rzeczywistość na poziomie subatomowym jest probabilistyczna, zachwiała wiarą w determinizm fizyczny. Była to idea tak niepojęta dla wielu fizyków, że, jak stale powtarza fizyk Andrzej Dragan w swoich wykładach, wielu odkrywców efektów kwantowych nie wierzyło w swoje odkrycia aż do śmierci. Sam problem losowości i determinizmu jest niezwykle ciekawy. Badania wykorzystujące twierdzenie Bella wykazują, że stan kwantowych cząstek splątanych nie wynika z ich historii, tzn. stanu jaki miały wcześniej, lecz w sposób probabilistyczny objawia się podczas pomiaru. Eksperymenty nie dają całkowitej pewności (jak to zwykle bywa w nauce) o istnieniu losowości na kwantowym poziomie. Wynika to z problemów metodologicznych, takich jak: możliwe wady eksperymentu czy niepewności pomiarowe. Niemniej, wyniki tych badań zdają się mocno sugerować prawdziwość postulatów fizyki kwantowej1.\r\nNiektórzy myśliciele dopatrywali się w zjawiskach kwantowych mechanizmów umożliwiających istnienie wolnej woli oraz świadomości. Roger Penrose uważa, że zjawiska kwantowe zachodzące w mózgu generują świadomość. Z kolei Johnjoe McFadden uważa, że za wolną wolę i świadomość odpowiada pole elektromagnetyczne wytwarzane przez pracujący mózg. Należy tu poczynić pewną uwagę. Mechanika kwantowa jest najbardziej podstawową teorią naukową jaką posiadamy, dotyczy zachowania mikrocząstek, z których zbudowane są wszystkie większe obiekty fizyczne i podlegają jej prawom. W takim ujęciu wszystko jest “kwantowe.” Gdy jednak mowa o kwantowych efektach w mózgu, zwykle chodzi o nietrywialne efekty kwantowe takie jak tunelowanie, splątanie czy superpozycja. Efekty te potrzebują odpowiednich warunków do zaistnienia. Obliczenia pokazują, że te efekty kwantowe w mózgu ulegają dekoherencji zbyt szybko, by wpływać lub zarządzać pracą neuronów (Seife, 2000)2. Nawet jeśli dopuścimy pewną losowość w pracy mózgu, problem z wolną wolą pozostał zasadniczo ten sam. Nawet jeśli istnieje pewna losowość w działaniu umysłu, to nie umysł ją generuje. Jeśli decyzja jest dziełem przypadku, nie może być wolna, ponieważ jest przypadkowa. Ponadto pojawia się też pytanie - jak taka losowość miałaby się manifestować. Na poziomie neuronalnym, gdyby losowe neurony losowo generowałyby potencjały czynnościowe, zakłóciłoby to pracę mózgu. Losowość musiałaby być niezwykle precyzyjna. Nie znajdziemy jej jednak też na poziomie świadomym, gdyż ludzie nie są w stanie np. wygenerować losowych sekwencji liczb (Figurska et al., 2008; Schulz et al., 2012).\r\nPoszukiwania kwantowej świadomości przypominają pod pewnymi względami problem homunkulusa. Hipotetyczny homunkulus to mały zarządca w naszym mózgu, który “ogląda” informacje dostarczane przez zmysły, integruje je i podejmuje decyzje. W toku historii wielokrotnie poszukiwano owego homunkulusa czy to w postaci niematerialnej duszy czy jakiegoś konkretnego obszaru w mózgu, który odpowiada za świadomość. Nie odnaleziono jednak niczego takiego, a świadomość wydaje się wyłaniać ze zsynchronizowanej pracy całego mózgu. W naukowym obrazie świata, zawieszeni pomiędzy deterministycznymi i stochastycznymi procesami, nie znajdziemy miejsca dla wolności przez duże “W.”\r\nHomunkulus\r\nProblemem organu zarządczego jest to, że jego działeniem również powinno coś zarządzać, i tak ad infinitum.\r\nZłudzenie konieczne\r\nMimo to, wielu ludzi - w tym naukowców, nie godzi się z takim stanem rzeczy. Doświadczenie sprawczości jest tak potężne, że stanowi samoistny argument za istnieniem wolnej woli. Hannah Arendt pisała “uznaję wewnętrzne świadectwo »ja chcę« za dostateczny dowód realności fenomenu woli” (Arendt, 1996), jakoby samo istnienie subiektywnego poczucia woli byłoby wystarczające, by udowodnić istnienie wolnych decyzji. Mgliste stwierdzenie, że struktura rzeczywistości, w której się znajdujemy, zaprzecza możliwości istnienia takiego fenomenu, może być dla wielu nieprzekonująca, gdy tak żywo czujemy naszą własną sprawczość. Niektórzy myśliciele, by znaleźć empiryczne dowody za lub przeciw wolnej woli, zdecydowali poszukać ich w prężnie rozwijającej się dziedzinie zajmującej się badaniem umysłu - szeroko pojętej neuronauki.\r\nNajsłynniejszymi eksperymentami dotyczącymi wolnej woli są eksperymenty Libeta i później Haynesa, którzy wykazali, że w przypadku prostych decyzji, takich jak naciśnięcie jednego z dwóch guzików, badacz jest w stanie przewidzieć na podstawie aktywności mózgu co wybierze badany, zanim uświadomi on sobie swoją decyzję (Soon et al., 2013). Choć te eksperymenty są najbardziej znane, ponieważ explicite poruszały tematykę wolnej woli, nie znajdziemy wielu badań studiujących wolną wolę. Wydaje się wręcz (pozornie), że na temat wolnej woli neuronauka ma niewiele do powiedzenia. Jest to podyktowane faktem, że na pytanie o wolną wolę (choć nie zostało zadane) została już udzielona odpowiedź. Założenia metodologiczne neuronauk i uzyskana dzięki nim wiedza, spójna z resztą naszej wiedzy o świecie, wykluczają istnienie fenomenu, który by jej jawnie przeczył. Mózg jest tylko (i aż) biologicznym komputerem. Laptop, na którym piszę ten tekst też potrafi podejmować decyzje, niemniej nie posiada z tego tytułu wolnej woli. Wydaje się, że wolna wola jest pojęciem, który ciężko wpisać we współczesną naukę.\r\n\r\nCelowo nie rozwijam problemu świadomości, który, moim zdaniem, jest bardziej skomplikowany niż problematyka wolnej woli. Na szczęście ta ostatnia nie wymaga prób odpowiedzenia na pytanie “czym jest świadomość?”\r\nMózg determinuje nasze decyzje poprzez obliczenia, a owe obliczenia determinowane są przez interakcje genów i środowiska. Bez odwołania do metafizycznych efektów, nie ma miejsca na ów wolny element. Niemniej, filozof Phillipe Meyer w książce “Złudzenie konieczne” zauważa, że chociaż neuronaukowcy udowodnili materialność funkcjonowania mózgu w każdym aspekcie, wielu naukowców, w tym niektórzy neuronaukowcy, odrzucają czysto materialny charakter pracy mózgu i to niekoniecznie w oderwaniu od swojej pracy naukowej (Meyer, 1998). John Eccles, laureat nagrody Nobla z fizjologii i medycyny (za badania nad synapsami), napisał artykuł w którym twierdził, że „[mózg odznacza się] wrażliwością innego rodzaju niż jakikolwiek instrument fizyczny” oraz że „umysł osiąga połączenie z mózgiem za pośrednictwem przestrzennoczasowych pól oddziaływania, które stają się aktywne dzięki tej wyjątkowej […] funkcji pobudzonej kory mózgowej”. Znany neuronaukowiec Michael Gazzaniga tak skomentował poglądy Eccelsa: “No, no! Przecież to czyste wudu, opisane wyszukanym językiem. Eccles zastąpił Kartezjuszową szyszynkę tajemniczą wrażliwością pobudzonej kory mózgowej. Dwieście lat po Kartezjuszu kontynuował kartezjańską tradycję dualizmu, mimo że spędzał sześćdziesiąt godzin tygodniowo na badaniu i rejestrowaniu aktywności neuronów i choć we wszystkich innych sprawach był gorliwym wyznawcą determinizmu. To po prostu niepojęte.” (Gazzaniga, 2020).\r\nWydaje się, że mamy do czynienia tutaj przykładem desperackiej próby ocalenia tego, w co dość długo wierzyliśmy, że nas ludzi wyróżnia. Wspomniany już Kartezjusz wierzył, że zwierzęta są maszynami napędzanymi przez skomplikowane mechanizmy (Descartes, 1980). Nie chciał jednak dopuścić, że podobnie może być w wypadku ludzi. Mimo oporów wobec materialistycznych i mechanicystycznych wyjaśnień działania ludzkiego umysłu, neuronauki zdają się sukcesywnie pokazywać, że nasza świadomość, poczucie sprawczości i ciągłości “ja,” są sprytnymi złudzeniami.\r\nBy zobaczyć jakie problemy nastręcza pojęcie wolnej woli, przyjrzyjmy się kilku badaniom naukowym. Michael Gazzaniga badał pacjentów po komisurotomi - chirurgicznym rozszczepieniu półkul mózgu wykonanym w celu złagodzenia ciężkich ataków epilepsji. W wyniku tego obie półkule dostawały te same dane wejściowe z obszarów podkorowych, ale działają niezależnie od siebie. Półkule nie dzielą informacji wzrokowej, prawe pole widzenia dochodzi tylko do lewej półkuli, a lewe do prawej. Jednocześnie u większości ludzi obszary związane z mową mieszczą się w lewej półkuli, słowa produkowane są bez udziału prawej półkuli. Pozwala to na dostarczanie różnych informacji obu półkulom: “Pokazaliśmy pacjentowi dwa obrazki: w prawej połowie jego pola widzenia umieściliśmy wizerunek kurzej łapy, tak aby lewa półkula mózgu widziała tylko ten obrazek, a w lewej połowie – obraz zaśnieżonego podwórka, tak aby półkula prawa nie widziała niczego poza nim. Następnie położyliśmy przed pacjentem kilkanaście rysunków, widocznych dla obu półkul mózgu, i poprosiliśmy, żeby wybrał spośród nich obrazki kojarzące mu się z tym, co przed chwilą zobaczył. Jego lewa ręka wskazała szuflę (która stanowiła najlepszą odpowiedź na widok zaśnieżonego podwórka), a prawa – kurę (była to najbardziej trafna reakcja na rysunek kurzej łapy). Kiedy zapytaliśmy, dlaczego wybrał właśnie te obrazki, jego ośrodek mowy zlokalizowany w lewej półkuli odparł: „Och, to bardzo proste. Kurza łapa kojarzy mi się z kurą,” z łatwością wyjaśniając to, co wiedział. Zobaczył kurzą łapę. Następnie, spojrzawszy na swoją lewą dłoń wskazującą szuflę, mężczyzna dodał bez wahania: „A szufla jest potrzebna do posprzątania kurnika”[…] Interesujący wydawał się fakt, że lewa półkula nie odpowiedziała: „Nie wiem”, co byłoby zgodne z prawdą. Zamiast tego wymyśliła odpowiedź pasującą do sytuacji. Konfabulowała, składając informacje, którymi dysponowała, w sensowną odpowiedź. Nazwaliśmy ten lewopółkulowy proces interpretatorem.\" (Gazzaniga, 2013)\r\nGazzaniga kontynuował badania nad interpretatorem. Jest to moduł wyspecjalizowany w tworzeniu spójnej narracji, umożliwia również tworzenie abstrakcyjnych relacji przyczynowo-skutkowych. Pozwala, na podstawie różnych przesłanek, wyciągać logiczne wnioski, wychodzące poza czystą percepcję rzeczywistości. Lewopółkulowy interpretator nieustannie szuka potencjalnych przyczyn różnych zdarzeń, jednak struktura, znajdująca się prawdopodobnie w prawym płacie ciemieniowym, hamuje działanie interpretatora, kiedy ten zaczyna tworzyć historie zbytnio odstające od rzeczywistości. Gdy półkule są rozdzielone, sygnał hamujący nie dochodzi do lewej półkuli, a interpretator może nieskrępowanie snuć swoje historie. Prowadzi to do systematycznych konfabulacji pacjentów z rozdzielonymi półkulami, ponieważ nawet najmniej prawdopodobne wyjaśnienia dostają się do świadomości jako wiarygodne przyczyny. Przykładowo, gdy Gazzaniga zaprezentował prawej półkuli pacjentki przerażający film, ta zaaktywizowała układ współczulny, wprowadzając fizjologiczną reakcję strachu - przyśpieszone bicie serca, potliwość i aktywność mięśniową. Ponieważ obszary podkorowe przekazują informacje o aktywności układu współczulnego prawej i lewej półkuli, lewa półkula “wiedziała,” że organizm jest przestraszony, nie wiedziała jednak dlaczego. Interpretator na podstawie dostępnych informacji - obecności Gazzanigi w pomieszczeniu - natychmiast wytworzył wyjaśnienie tego stanu: to doktor Gazzaniga jest przerażający.\r\nDalsza praca zespołu Gazzanigi pokazała role lewopółkulowego interpretatora u ludzi zdrowych. Sprawia on, że mamy poczucie spójnej narracji w naszych działaniach. Interpretator jest jednak tak dobry jak informacje, które do niego docierają, a wyniki eksperymentów pokazują, że docierają do niego rezultaty, czy też wyniki działań innych modułów mózgu, nie ma on jednak dostępu do ich obliczeń. Przykładem jest coś, co nazywamy pamięcią procesualną. W odróżnieniu do pamięci deklaratywnej, czyli takiej, do której mamy świadomy dostęp, pewne czynności potrafimy wykonywać doskonale, ale nie jesteśmy właściwie w stanie wytłumaczyć jak to robimy. Przykładem może być jazda na rowerze. Wyjaśnienie komuś jak należy jeździć na rowerze jest skazane na porażkę. Jest to wbrew pozorom bardziej skomplikowane niż pedałowanie i kręcenie kierownicą. Wymaga poczucia równowagi i odpowiedniego balansowania ciałem. Jednak osoby, które potrafią jeździć na rowerze wcale nie czują, by coś takiego robiły.\r\nInnym przykładem może być wiedza ekspercka. Ludzie specjalizujący się w pewnych zadaniach, potrafiący wykonywać je z niesamowitą biegłością, często nie mają wglądu w to jak to robią. Gazzaniga zaprosił do laboratorium mistrza szachowego Patricka Wolffa, który miał za zadanie odtworzyć układ pionków na szachownicy, oglądając go przez 5 sekund. Wolff był w stanie to zrobić, jeśli układ, który oglądał był sensowny z szachowego punktu widzenia. Jeśli jednak pionki były ustawione losowo, radził sobie z podobną skutecznością jak każdy inny człowiek. Nie miał więc świetnej pamięci wzrokowej, lecz jego moduł zajmujący się grupowaniem percepcyjnym nauczony wieloletnią grą w szachy błyskawicznie rozpoznawał złożone wzorce szachowe. Sam Wolff był tego nieświadomy, nie był w stanie powiedzieć jak udaje mu się osiągnąć dobry wynik w jednym przypadku i przeciętny w drugim. Jego interpretator dostawał tylko informacje o końcowym wyniku (odtworzeniu szachownicy), nie miał jednak informacji jak udało się to osiągnąć.\r\nInformacje przekazywane do świadomości przez mózg są selektywne. Nie wszystko co mózg zauważy i przetworzy dostanie się do naszej świadomości, czy też jakby powiedział Gazzaniga, do interpretatora. Jeśli badanym zostanie zaprezentowana przez kilka-kilkanaście milisekund przestraszona twarz, badani jej świadomie nie zauważą, jednak dostrzeżemy zwiększoną aktywność ciała migdałowatego - struktury przetwarzającej bodźce zagrażające (Whalen et al., 1998). Jest to kolejny z przykładów, że nie mamy świadomego dostępu do pewnych informacji przetwarzanych przez nasz mózg.\r\nMożna powiedzieć, że mózg regularnie nas “oszukuje.” Jeśli dotkniemy palcem do nosa, jednocześnie poczujemy dotyk w nosie i w palcu, mimo że sygnał z palca dotrze do mózgu znacznie później. Mózg tworzy własną reprezentację czasu przekazywaną świadomości. Badani, u których po wykonaniu spontanicznej akcji zaaplikowano silny impuls magnetyczny na pole przedruchowe, postrzegali, że intencja wykonania ruchu pojawiła się wcześniej niż gdy nie otrzymali impulsu (Lau et al., 2007).\r\nSposób przetwarzania informacji przez mózg ma krytyczne znaczenie nie tylko dla postrzegania sprawczości u siebie samych, ale i u innych ludzi. Zespół Rebecki Saxe zakłócił badanym działanie pewnego obszaru mózgu - prawego styku skroniowo-ciemieniowego, za pomocą impulsu magnetycznego. Następnie przedstawiła im cztery historie, w których Grace podała swojemu przyjacielowi cukierniczkę myśląc, że jest tam cukier/trucizna, gdy naprawdę był tam cukier/trucizna. Badani, którzy nie otrzymali impulsu, negatywnie oceniali Grace, gdy ta myślała, że w cukierniczce jest trucizna, niezależnie od tego czy faktycznie tam była. Badani którzy impuls otrzymali, łagodniej oceniali Grace, gdy ta myśląc, że podaje truciznę, podała cukier (Young et al., 2010). Badani, u których praca styku skroniowo-ciemieniowego została zakłócona, skupiali się na skutkach działań Grace, nie biorąc pod uwagę jej intencji. Prawy styk ciemieniowo-skroniowy jest częścią obwodu nerwowego zajmującego się teorią umysłu - zdolnością pozwalającą między innymi na przejęcie perspektywy innych osób. Sposób w jaki badani dokonują oceny moralnej Grace zależy od pracy tego obwodu. Gdyby mieli zadecydować czy i jaką karę wymierzyć Grace, informacje przetwarzane przez styk skroniowo-ciemieniowy miałby spore znaczenie dla tej decyzji.\r\nNie ma odpowiedzialności, nie ma kary\r\nPowyższe przykłady miały za zadanie pokazać, że odczuwana przez nas rzeczywistość jest sprytną iluzją, która nie zawsze przystaje do rzeczywistości fizycznej. Choć najbardziej spektakularnie widać to u ludzi z uszkodzeniami mózgu, postrzeganie ludzi “zdrowych” też jest wynikiem pracy mózgu, pracy niezależnej od nas samych. Wydaje się, że taki brutalny mechanicyzm odziera świat z jakiegokolwiek sensu. Jednak wiedza o tym, że “wolna wola” nie istnieje, raczej nie zmieni sposobu w jaki zachowujemy się na co dzień. Ma to jednak znaczenie dla praktyk społecznych. Jednym z przykładów może być system penitencjarny. System penitencjarny stawia sobie różne cele: odizolowanie jednostek niebezpiecznych od reszty społeczeństwa, resocjalizacji czy karze. Kara opiera się na pojęciu odpowiedzialności, jest odpłatą (sankcjonowaną zemstą) za przestępstwo.\r\nOkoło 180 lat temu stworzono regułę M’Naghtena - precyzyjne pojęcie niepoczytalności. Stało się to podczas procesu Daniela M’Naghtena w Wielkiej Brytanii, oskarżonego o zabójstwo i uniewinnionego z powodu choroby psychicznej. Wyrok ten wywołał ogromne kontrowersje, w wyniku czego powołano komisję, która miała określić dokładne kryteria niepoczytalności. Osoba, która nie jest świadoma, że popełnia przestępstwo, nie wie, że to co zrobiła jest “złe” w chwili popełnienia czynu nie może podlegać karze. Był to przełom społeczny, zdecydowano bowiem, że istnieją wyjątki od odpowiedzialności (Maroń, 2018).\r\n\r\nNie był to jednak pierwszy wyrok biorący pod uwagę poczytalność sprawcy. Istniał precedens (Rex v. Arnold z 1724) zwany “standardem dzikiej bestii,” jednak dopiero po sprawie M’Naghtena zestandaryzowano wskaźniki niepoczytalności.\r\nIdea ta, choć z oporem, na stałe zagościła w zachodnich systemach prawnych. Powoli pojawiła się w innych obszarach społecznych - sto pięćdziesiąt lat po sprawie M’Naghtena Kościół Katolicki - dla którego pojęcie wolnej woli i odpowiedzialności za czyny są niezwykle ważne, również uznał owy argument. Jak możemy przeczytać w Katechizmie: “Ciężkie zaburzenia psychiczne, strach lub poważna obawa przed próbą, cierpieniem lub torturami mogą zmniejszyć odpowiedzialność samobójcy” (Katechizm Kościoła Katolickiego, 2015), uznając, że depresja zmniejsza opdowiedzialnośc za czyn uważany jako grzech ciężki.\r\nChoć ówcześnie idea, że człowiek może nie odpowiadać za swoje czynny była myślą nowatorską, funkcjonuje do dziś w stanie praktycznie nie zmienionym, mimo znacznego postępu nauki. Znaleźliśmy się współcześnie w miejscu, gdzie dzielimy ludzi na zdolnych i tych niezdolnych do odpowiedzialności. Kryterium rozróżnienia polega na poprawności funkcjonowania mózgu. Problem w tym, że według wiedzy neuronaukowej ciężko jest mówić o odpowiedzialności kogokolwiek (Farah, 2005). Przytoczone wcześniej przykłady miały pokazać, że zachowanie zależne jest od pracy mózgu, na którą jednostka nie ma wpływu. Neurobiolog Robert Sapolsky poddał pod wątpliwość obecną definicje niepoczytalności skupiającą się na nieświadomości sprawcy, że popełnia czyn zabroniony. Istotnym zespołem ośrodków mózgów odpowiedzialnych za hamowanie zachowań impulsywnych jest kora przedczołowa. Osoby z zaburzonym jej funkcjonowaniem doskonale zdają sobie sprawę z swoich zachowań, nie są jednak w stanie się od nich powstrzymać (Sapolsky, 2006).\r\nWśród wielu neuronaukowców i przedstawicieli innych dziedzin istnieje pogląd, że kara oparta na odpowiedzialności, która jest sankcjonowaną prawnie zemstą, jest co najmniej wątpliwa. Richard Dawkins stwierdził “Odpłata jako zasada moralna jest niezgodna z naukowym poglądem na ludzkie zachowanie. Jako naukowcy wierzymy, że ludzkie mózgi, chociaż mogą nie działać w taki sam sposób, jak komputery stworzone przez człowieka, są tak samo rządzone prawami fizyki. Kiedy komputer działa nieprawidłowo, nie karzemy go. Odnajdujemy problem i naprawiamy go, zwykle poprzez wymianę uszkodzonego komponentu, sprzętowego lub programowego” (Dawkins, 2006). Joshua Green i Jonathan Cohen uważają, że należy odrzucić koncept retrybucji, a zamiast tego skupić się na odseparowaniu od społeczeństwa (jeśli to potrzebne) i resocjalizacji. Postulują, że wraz ze zrozumieniem przyczyn działań przestępców, powinniśmy zmienić stosunek do nich (Greene & Cohen, 2004). Jest to logiczną konsekwencją faktu, że retrybucje wobec osób psychicznie chorych uważamy za niehumanitarne (ponieważ nie mają wpływu na to co robią), to zgodnie z wiedzą neuronaukową nie powinniśmy stosować retrybucji wobec nikogo.\r\nNiekoniecznie jednak brak “wolnej woli” człowieka implikuje konieczność zrezygnowania z retrybucyjnej funkcji kary. Psychiatra Sally Satel i psycholog Scott O. Lilienfeld w książce “Pranie mózgu” argumentują, że niezależnie od statusu wolnej woli, ludzie mają poczucie wolności wyboru i działają podług niego, a kara ma ewolucyjną funkcję społeczną (Satel & Lilienfeld, 2017). Już małe dzieci odbierają zachowanie innych ludzi w kategoriach intencjonalności i mają silne wrodzone zachowania moralne, włącznie z karaniem źle zachowujących się jednostek (Hamlin, 2013). Eksperymenty przeprowadzone przez ekonomistów behawioralnych pokazały, że wielu ludzi dyscyplinuje jednostki zachowujące się “nie-fair,” nawet jeśli były tylko biernymi obserwatorami takiego zachowania i są w stanie poświęcić na to własne zasoby (Fehr & Fischbacher, 2004). Pozwala to ludziom na osiąganie celów wymagającej współpracy wielu osób i minimalizacji “oszustów.”\r\nPonadto Satel i Lilienfeld przywołują eksperyment psycholożki i prawniczki Kenworthey Bilz, w którym wykazała, że nieukaranie sprawcy gwałtu obniża status społeczny ofiary (Bilz, 2016). Sugerują, że brak moralnego zadośćuczynienia przy użyciu proporcjonalnej kary może prowadzić do zachwiania równowagi społecznej. Przywołują hipotezę “sprawiedliwego świata” sformułowaną przez Melvina Lernera. Jest to błąd poznawczy polegający na tendencji do wiary, że ludzie zasługują na nieszczęścia i sukcesy, które ich spotykają. W eksperymencie przeprowadzonym przez Lernera badani obserwowali aktorkę wykonującą zadanie pamięciowe, jednocześnie otrzymującą bolesne impulsy elektrycznie (udawane, o czym badani nie wiedzieli). Gdy badani mogli zdecydować o przerwaniu eksperymentu lub dowiadywali się, że aktorka otrzyma wynagrodzenie pieniężne oceniali ją znacznie wyżej, niż w sytuacji gdy biernie przyglądali się sytuacji bez możliwości wpływu na nią (Lerner & Miller, 1978). Lerner doszedł do wniosku, że widok osoby cierpiącej, która nie ma szans na rekompensatę skłania ludzi do deprecjonowania ofiary. Satel i Lilienfeld sugerują, że rezygnacja ze sprawiedliwej odpłaty byłaby negatywna w skutkach zarówno dla ofiary jak i moralności społecznej.\r\nInni wyrażają bardziej umiarkowany pogląd, że kwestia wolnej woli jest w gruncie rzeczy nieistotna dla prawa, dostrzegają jednak, że włączenie wiedzy o człowieku do procesu wymierzania sprawiedliwości może poskutkować bardziej empatycznym traktowaniem przestępców (przynajmniej tych o których wiemy, że mają znacznie zakłócone procesy podejmowania decyzji), przy zachowaniu psychologicznej potrzeby retrybucji ofiar i społeczeństwa (Goodenough & Prehn, 2004). Jest to próba pewnego kompromisu, wskazująca na potrzebę zmian w systemie prawnym, niemniej nie polegająca na odrzuceniu retrybucyjnej funkcji kary.\r\nEpilog\r\nChoć odpłata jest głęboko zakorzeniona w ludzkiej kulturze i biologii, nie oznacza to, że nie jesteśmy w stanie zmienić naszego podejścia. Człowiek wielokrotnie wprowadzał zmiany społeczne, jak wówczas mówiono, niezgodne z jego naturą czy uderzające w porządek społeczny. Jeśli jako gatunek mielibyśmy działać tak jak aktualnie postrzegamy rzeczywistość, niewiele byśmy osiągnęli. Green i Cohen podają przykład tego jak nasza percepcja fizycznej rzeczywistości rozmija się z tym jak jest faktycznie. Przykładem może być czas. Czas jest względny, zależny od pola grawitacyjnego i prędkości z jaką układ odniesienia się porusza. Jednak nasze codzienne doświadczenie mówi nam, że czas jest uniwersalny i płynie liniowo. Ma to silne przyczyny ewolucyjne. Czy to oznacza, że jesteśmy na zawsze ograniczeni przez nasze liniowe postrzeganie czasu? Zdecydowanie nie. Wykorzystujemy zdobytą wiedzę w tworzeniu rzeczywistości np. w systemach GPS. Podobnie jest w przypadku omawianego problemu. Możemy wykorzystać wiedzę, którą mamy, by zmienić nasze zachowania społeczne.\r\n\r\n\r\n\r\nArendt, H. (1996). Wola (p. 28). Czytelnik.\r\n\r\n\r\nBilz, K. (2016). Testing the expressive theory of punishment. Journal of Empirical Legal Studies, 13(2), 358–392.\r\n\r\n\r\nDamasio, A. R. (2011). Błąd kartezjusza: Emocje, rozum i ludzki mózg. Dom Wydawniczy\" Rebis\".\r\n\r\n\r\nDawkins, R. (2006). Let’s all stop beating basil’s car. https://www.edge.org/response-detail/11416\r\n\r\n\r\nDescartes, R. (1980). Rozprawa o metodzie (pp. 72–73). Państwowy Instytut Wydawniczy.\r\n\r\n\r\nFarah, M. J. (2005). Neuroethics: The practical and the philosophical. Trends in Cognitive Sciences, 9(1), 34–40.\r\n\r\n\r\nFehr, E., & Fischbacher, U. (2004). Third-party punishment and social norms. Evolution and Human Behavior, 25(2), 63–87.\r\n\r\n\r\nFigurska, M., Stańczyk, M., & Kulesza, K. (2008). Humans cannot consciously generate random numbers sequences: Polemic study. Medical Hypotheses, 70(1), 182–185.\r\n\r\n\r\nGazzaniga, M. S. (2013). Kto tu rządzi-ja czy mój mózg?: Neuronauka a istnienie wolnej woli (pp. 74–76). Smak Słowa.\r\n\r\n\r\nGazzaniga, M. S. (2020). Instynkt świadomości jak z mózgu wyłania się umysł? (pp. 89–90). Smak Słowa.\r\n\r\n\r\nGoodenough, O. R., & Prehn, K. (2004). A neuroscientific approach to normative judgment in law and justice. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 359(1451), 1709–1726.\r\n\r\n\r\nGreene, J., & Cohen, J. (2004). For the law, neuroscience changes nothing and everything. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 359(1451), 1775–1785.\r\n\r\n\r\nHamlin, J. K. (2013). Moral judgment and action in preverbal infants and toddlers: Evidence for an innate moral core. Current Directions in Psychological Science, 22(3), 186–193.\r\n\r\n\r\nKatechizm kościoła katolickiego. (2015). 2280, 2282.\r\n\r\n\r\nLau, H. C., Rogers, R. D., & Passingham, R. E. (2007). Manipulating the experienced onset of intention after action execution. Journal of Cognitive Neuroscience, 19(1), 81–90.\r\n\r\n\r\nLerner, M. J., & Miller, D. T. (1978). Just world research and the attribution process: Looking back and ahead. Psychological Bulletin, 85(5), 1030.\r\n\r\n\r\nMaroń, G. (2018). Zabójstwo „z rozkazu boga” a niepoczytalność sprawcy w świetle orzecznictwa sądów USA ( murder on the command of god versus a perpetrator’s insanity in the light of the case law of u.s. courts). 2018.\r\n\r\n\r\nMeyer, P. (1998). Złudzenie konieczne (pp. 187–188). Państwowy Instytut Wydawniczy.\r\n\r\n\r\nSapolsky, R. M. (2006). The frontal cortex and the criminal justice system. Law and the Brain, 227.\r\n\r\n\r\nSatel, S., & Lilienfeld, S. O. (2017). Pranie mózgu. Uwodzicielska moc (bezmyślnych) neuronauk (pp. 191–228). CiS.\r\n\r\n\r\nSchopenhauer, A. (1991). O wolności ludzkiej woli (pp. 16–17). bis.\r\n\r\n\r\nSchulz, M.-A., Schmalbach, B., Brugger, P., & Witt, K. (2012). Analysing humanly generated random number sequences: A pattern-based approach. PloS One, 7(7), e41531.\r\n\r\n\r\nSeife, C. (2000). Cold numbers unmake the quantum mind. Science, 287(5454), 791–791.\r\n\r\n\r\nSoon, C. S., He, A. H., Bode, S., & Haynes, J.-D. (2013). Predicting free choices for abstract intentions. Proceedings of the National Academy of Sciences, 110(15), 6217–6222.\r\n\r\n\r\nWhalen, P. J., Rauch, S. L., Etcoff, N. L., McInerney, S. C., Lee, M. B., & Jenike, M. A. (1998). Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge. Journal of Neuroscience, 18(1), 411–418.\r\n\r\n\r\nYoung, L., Camprodon, J. A., Hauser, M., Pascual-Leone, A., & Saxe, R. (2010). Disruption of the right temporoparietal junction with transcranial magnetic stimulation reduces the role of beliefs in moral judgments. Proceedings of the National Academy of Sciences, 107(15), 6753–6758.\r\n\r\n\r\nCo interesujące, pojawia się tu dodatkowy problem. W wielu dziedzinach nauki wykorzystujemy założenie o niezależności pomiarów, który uzyskujemy zwykle dzięki np. generatorom liczb pseudolosowych czy podwójnie ślepej próbie. Zwykle to wystarcza by uzyskać rzetelne wyniki. Jeśli jednak chcemy zbadać fundamentalną właściwość jaką jest samo istnienie losowości, nie możemy wykorzystać liczb pseudolosowych do ustawienia parametrów urządzeń, ponieważ liczby pseudolosowe są deterministyczne. Prawdziwą losowość uzyskuje się podczas pomiarów efektów kwantowych, ale tu zakładamy losowość zjawisk kwantowych, by udowodnić losowość zjawisk kwantowych. Fizycy starają się poradzić sobie z tym problemem na wiele sposobów, np. mierząc fotony pochodzące z gwiazd oddalonych o 600 lat świetlnych czy prosząc internautów o głosowanie w wyborze parametrów. Zabezpiecza to przez zarzutem prostego łańcucha deterministycznego, nie wyklucza jednak superdeterminizmu, to znaczy możliwości, że takie, a nie inne zachowanie kwantowych cząsteczek podczas pomiarów wynika z mechanizmu przyczynowo-skutkowego, którego nie rozumiemy. Wykład o twierdzeniu Bella autrostwa Michała Ecksteina - https://www.youtube.com/watch?v=eRDxsl06f30&t=194s↩︎\r\nInteresujący przegląd zagadnień związanych z zjawiskami kwantowymi w biologii można znaleźć w książce Paula Davisa “Demon w maszynie.”↩︎\r\n",
    "preview": "posts/2021-09-13-o-odpowiedzialnoci-za-czyny-w-dobie-lotw-na-marsa/pexels-suzy-hazelwood-1422673.jpg",
    "last_modified": "2022-03-26T19:41:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-12-odwied-baz-wiedzy/",
    "title": "Odwiedź Bazę Wiedzy!",
    "description": {},
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {}
      }
    ],
    "date": "2021-09-12",
    "categories": [
      "Sortownia"
    ],
    "contents": "\r\n\r\n\r\np.note\r\n{\r\n     -moz-border-radius: 6px;\r\n     -webkit-border-radius: 6px;\r\n     background-color: #f0f7fb;\r\n     background-image: url(css-box-icon-3.png);\r\n     background-position: 9px 0px;\r\n     background-repeat: no-repeat;\r\n     border: solid 1px #3498db;\r\n     border-radius: 6px;\r\n     line-height: 18px;\r\n     overflow: hidden;\r\n     padding: 15px 60px;\r\n}\r\n\r\nOdwiedź Bazę Wiedzy!\r\nOtwieramy pierwszy moduł Sortowni Wiedzy! Jest nią… Baza Wiedzy. Sprawa jest bardzo prosta – zbieramy przydatne linki w jedno miejsce. Tylko tyle i aż tyle. Jeśli chcecie dowiedzieć się od czego zacząć poszukiwania artykułów naukowych, jeśli chcecie poznać podkasty popnaukowe, jeśli chcecie poznać ciekawe kanały na Youtube, albo dotrzeć do przydatnych źródeł z wybranych dziedzin – zajrzyjcie i przetestujcie wyszukiwarkę.\r\n\r\nWe współczesnym świecie bardzo wiele można nauczyć się nie wychodząc z domu (i za darmo). Internet jest pełen zasobów i narzędzi do nauki, weryfikacji informacji i edukującej rozrywki. Na niektóre łatwo trafić. Inne znajduje się po długich poszukiwaniach lub przypadkiem. Menedżery zakładek w naszych przeglądarkach wypchane są przydatnymi linkami, często nieopisanymi lub w złych podfolderach. Chęci na uporządkowanie tego zrodziła pomysł Bazy Wiedzy.\r\n\r\nLinki są zorganizowane według kategorii gatunkowo-funkcjonalnych (np. podkasty, serwisy informacyjne, strony zawierające informacje o grantach). Jest również specjalna kategoria “tematyczne/dziedzinowe”, która rozwinie dalsze okno wyboru wg dziedzin naukowych (np. lingwistyka, neuronauki). Ta z kolei pozwoli na jeszcze dokładniejsze zawężenie poszukiwań, wg tagów (dowolne hasła, lepiej kategoryzujące strony).\r\nBaza Wiedzy jest i będzie w ciągłym rozwoju, a znajdować się na niej będą ręcznie wybrane linki do istotnych stron. Zależy nam na stworzeniu prostego, funkcjonalnego przybornika do świata wiedzy. Kategorie zorganizowaliśmy tak, aby różne osoby mogły znaleźć coś przydatnego. Dla osób zaczynających poszukiwanie wiedzy stworzyliśmy kategorię “Gdzie zacząć”, na której znajdziecie odnośniki do baz z zasobami i wyszukiwarek. Dla osób szukających rozrywki i wiadomości naukowych mamy kategorie podkastów, kanałów wideo i serwisów informacyjnych. Z kolei osoby już eksplorujące wybrane dziedziny naukowe, mamy kategorię “dziedziny”, która pozwala wybrać specyficzne pole badawcze.\r\nWy też możecie dołożyć swoje trzy grosze do rozwoju Bazy Wiedzy! Jeśli znacie przydatne strony, albo jeśli macie wiedzę ekspercką z zakresu jakiejś dziedziny – piszcie do nas! Informacje w zakładce “Kontakt”.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-12-odwied-baz-wiedzy/books-1655783_1280.jpg",
    "last_modified": "2022-03-26T19:41:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-12-witajcie-w-sortowni/",
    "title": "Witajcie w Sortowni",
    "description": {},
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {}
      }
    ],
    "date": "2021-09-11",
    "categories": [
      "Sortownia"
    ],
    "contents": "\r\nOtwieramy stronę! Witajcie na Sortowni Wiedzy (a może powinniśmy powiedzieć - w Sortowni?). Ta strona ma być przede wszystkim swego rodzaju punktem przesiadkowym. Węzłem, z którego dalej można przeskoczyć do innych stron z wiedzą naukową. My w pierwszej kolejności chcemy zgromadzić przydatne odnośniki i nakierować Was na te najbardziej wartościowe. Już teraz możecie zajrzeć do naszej Bazy Wiedzy. Co prawda wciąż jeszcze panuje tam chaos, który próbujemy opanować - ale wraz z mijającym czasem, powinno zacząć się tam przejaśniać. Cel i funkcja Bazy Wiedzy są niezwykle proste. Wręcz obraźliwie proste. To po prostu baza przydatnych linków. Tyle i aż tyle. Odnośniki zebrane w jednym miejscu, opisane i podzielone na kategorie. Znajdziecie tam linki do stron popnaukowych (serwisy, blogi, podkasty), jak i linki do przydatnych repozytoriów i baz naukowych. Linki są naszym subiektywnym wyborem tego, co uważamy za przydatne i istotne. I tu otwarcie mówimy - to nie jest obiektywna selekcja, choć staramy się do obiektywizmu dążyć.\r\nZachęcamy Was do podsyłania nam nowych linków na adres sortownia.wiedzy@gmail.com. Siłą rzeczy, nie znamy się na większości dziedzin naukowych, więc będziemy niezmiernie wdzięczni za pomoc w uzupełnianiu bazy.\r\nBaza Wiedzy to jednak tylko początek. Już teraz mamy w planach kolejne moduły strony, również związane z rozpowszechnianiem nauki. Będziemy publikować teksty opiniotwórcze na blogu i poradniki (m.in. z wiedzą o tym jak korzystać z nauki, jak szukać artykułów, jak rozpoznawać strategie retoryczne itd.). Mamy tysiąc pomysłów na nowe, przydatne narzędzia do poszukiwania wiedzy naukowej – te jednak będziemy powoli i ostrożnie rozwijać, wraz z rozrastaniem się strony.\r\nDla kogo w ogóle to wszystko robimy? Jaki to ma sens? No więc tak – po pierwsze, tak zebrana wiedza jest po prostu przydatna, nawet dla nas samych. Już teraz zdarza się nam szukać linków w Bazie kiedy szukamy (tego co gdzieś tam dawno temu daliśmy do zakładek w wyszukiwarce, ale nikt nie pamięta gdzie i kiedy to było). Baza Wiedzy, nawet w swojej podstawowej formie, pozwala dość szybko znaleźć przydatne strony związane z nauką.\r\nPo drugie, ze strony mogą korzystać studen_ i naukowc_, poszukujący jakiegoś punktu wyjściowego do poszukiwań. Osoby nie zaznajomione z poszukiwaniem wiedzy naukowej znajdą u nas wiele przydatnych odnośników do najważniejszych stron, od których należy zacząć zbieranie bibliografii. Po trzecie, osoby zupełnie niezwiązane ze światem nauki mogą być zainteresowane naszym wyborem stron popnaukowych. W szczególności polecamy sekcję podkastów. Po czwarte, będziemy kierować część materiałów do influencer_ i popularyzator_ nauki. Jednym z naszych celów jest zwiększenie ilości wiedzy naukowej w debacie publicznej. W związku z tym, jeśli masz platformę i własną publiczność, na naszej stronie znajdziesz materiały, które pomogą Ci w dalszym przekazywaniu informacji naukowych.\r\nMamy nadzieję, że to co robimy będzie przydatne. To dla nas najważniejsze. Chcemy zrobić coś pozytywnego w wymiarze społecznym, nawet jeśli będzie to bardzo mała rzecz. Piszcie do nas z uwagami, pomysłami, reakcjami na adres sortownia.wiedzy@gmail.com.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-12-witajcie-w-sortowni/hero-image.jpg",
    "last_modified": "2022-03-26T19:41:28+01:00",
    "input_file": {}
  }
]
