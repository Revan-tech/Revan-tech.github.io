[
  {
    "path": "posts/2023-02-05-baseball/",
    "title": "Dlaczego Å›rednia uderzeÅ„ graczy nie przewiduje sukcesu w pojedyÅ„czym uderzeniu?",
    "description": "O psychologach i maÅ‚ych efektach",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2023-08-02",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nPrzeczytaÅ‚em dwa lata temu artykuÅ‚ Davida Fundera â€œEvaluating Effect Size in Psychological Research: Sense and Nonsenseâ€ w ramach obowiÄ…zkowych zajÄ™Ä‡ z metodologii dla doktorantÃ³w (Funder & Ozer, 2019). ArtykuÅ‚ skupia siÄ™ na problematyce maÅ‚ych wielkoÅ›ci efektÃ³w w psychologii i ich interpreteacji. Jest cytowany w prawie 1500 innych artykuÅ‚ach, co Å›wiadczy o jego duÅ¼ej popularnoÅ›ci. Funder krytykuje standardy dotyczÄ…ce wielkoÅ›ci efektÃ³w opartych na wspÃ³Å‚czynniku korelacji liniowej zaproponowanych przez Jacoba Cohena i proponuje nowe kryteria, jednoczeÅ›nie podajÄ…c moÅ¼liwe przyczyny wystÄ™powania maÅ‚ych wielkoÅ›ci efektÃ³w w psychologii.\r\nChoÄ‡ w artykule sÅ‚usznie argumentuje, Å¼e wielkoÅ›Ä‡ wielkoÅ›ci efektÃ³w powinna byÄ‡ oceniana w zaleÅ¼noÅ›ci od badanego zjawiska, artykuÅ‚ zawieraÅ‚ duÅ¼o twierdzeÅ„, ktÃ³re byÅ‚y dla mnie niepokojÄ…ce metodologicznie.\r\nNajbardziej jednak zadziwiÅ‚ mnie wnioskek Fundera, Å¼e maÅ‚y efekt korelacji liniowej sumuje siÄ™ w jakiÅ› sposÃ³b w czasie. Co to znaczy? PoczÄ…tkowo sÄ…dziÅ‚em, Å¼e byÄ‡ moÅ¼e chodzi o uÅ›rednianie wielkorotnych pomiarÃ³w u pojedyÅ„czej osoby, by uzyskaÄ‡ mniej zaszumionÄ… zmiennÄ…. Na przykÅ‚ad gdy mierzymy czas reakcji badanego podczas wykonywania zadania na komputerze, badany w pojedyÅ„czej prÃ³bie moÅ¼e siÄ™ zamyÅ›liÄ‡ i zaareagowaÄ‡ wolniej. MoÅ¼e teÅ¼ akurat patrzeÄ‡ tam gdzie pojawi siÄ™ bodziec i zareagowaÄ‡ szybciej niÅ¼ gdyby musiaÅ‚ tego bodÅºca szukaÄ‡. UÅ›redniajÄ…c czas reakcji pozbywamy siÄ™ szumu wynikajÄ…cego z takich zdarzeÅ„. Jednak Funderowi nie o to chodziÅ‚o.\r\nFunder oparÅ‚ to twierdzenie na artykule z lat osiemdziesiÄ…tych psychologa Roberta Abelsona (Abelson, 1985). Abelson obliczyÅ‚ korelacjÄ™ miÄ™dzy sukcesem w pojedynczym pojedynczym podejÅ›ciu profesjonalnych bejsbolistÃ³w z Major League, z ich Å›redniÄ… uderzeÅ„ w sezonie. Z jego obliczeÅ„ wyszÅ‚o, Å¼e Å›rednia w sezonie wyjaÅ›niaÅ‚a tylko 0.00317 wariancji sukcesu w pojedyÅ„czym podejÅ›ciu. Abelson zdziwiony stwierdziÅ‚, Å¼e poniewaÅ¼ niewÄ…tpliwie sukces w pojedyÅ„czym podejÅ›ciu jest silnie zwiÄ…zany ze Å›redniÄ… w sezonie, to owa korelacja jest zdecydowanie za maÅ‚a, a on ma doczynienia z jakimÅ› paradoksem.\r\nFunder stwierdziÅ‚, Å¼e skoro najlepsza dostÄ™pna miara zdolonoÅ›ci zawodnika (jego Å›rednia w sezonie) wyjaÅ›nia tak maÅ‚o wariancji sukcesu w pojedyÅ„czym rzucie, a niewÄ…tpliwie jego Å›rednia jest silnie zwiÄ…zana z sukcesem w pojedyÅ„czym rzucie, to zwiÄ…zek ten musi objawiaÄ‡ (kumulowaÄ‡) siÄ™ w czasie. W pojedyÅ„czym rzucie nie jest taki istotny, jak w dwudziestu rzutach. Innymi sÅ‚owy wnioskowanie Fundera wyglÄ…daÅ‚o mniej wiÄ™cej tak: Å›redniÄ… w sezonie moÅ¼na potraktowaÄ‡ jako miarÄ™ jakieÅ› cechy osobowoÅ›ci np. neurotycznoÅ›ci czy ekstrawertyzmu, a sukces w pojedyÅ„czym podejÅ›ciu jako zachowanie powiÄ…zane z tÄ… cechÄ…. Funder twierdzi, Å¼e maÅ‚e korelacje cech osobowoÅ›ci z danymi zachowaniami, wcale nie sÄ… maÅ‚e, poniewaÅ¼ ich prawdziwa siÅ‚a objawia siÄ™ w dÅ‚uÅ¼szej perspektywie czasowej. Czyli, Å¼e w jakiÅ› sposÃ³b siÄ™ kumulujÄ….\r\nTaki wniosek moÅ¼e byÄ‡ podbudowujÄ…cy. MaÅ‚e efekty w psychologii wcale nie sÄ… maÅ‚e. Jednak jest to wniosek zupeÅ‚nie nieporawny. Wynik Abelsona moÅ¼na prosto wyjaÅ›niÄ‡ statystycznie i Å¼aden sposÃ³b nie oznacza to, Å¼e maÅ‚e efekty nie sÄ… maÅ‚e.\r\nÅ»eby zrozumieÄ‡ skÄ…d wziÄ…Å‚ siÄ™ tak maÅ‚y wynik Abelsona, przeÅ›ledzmy jego rozumowanie. Åšrednie profesjonalnych graczy zawieraÅ‚y siÄ™ miÄ™dzy 0.2 a 0.3 wiÄ™c uznaÅ‚ arbitralnie, Å¼e Å›rednia Å›rednich graczy to 0.27, a odchylenie standardowe Å›redniej Å›rednich to 0.25.\r\nPrzypomnijmy sobie, Å¼e wariancjÄ™ zmiennej moÅ¼emy rozÅ‚oÅ¼yÄ‡:\r\n\\[ Var(X) = Var(E(X|Y)) + E(Var(X|Y))\\]\r\nGdzie \\(Var(E(X|Y))\\) to wariancja przewidywania, a \\(E(Var(X|Y))\\) to wariancja bÅ‚Ä™du.\r\nProcent wyjaÅ›nionej wariancji przez zmiennÄ… \\(Y\\) moÅ¼emy obliczyÄ‡:\r\n\\[eta^2 = \\frac{Var(E(X|Y))}{Var(X)}\\]\r\nZ tego wzoru wynika, Å¼e procent wariancji zostanie wyjaÅ›nionej zaleÅ¼y od tego jak bardzo Å›rednie X pod warunkiem rÃ³Å¼ych wartoÅ›ci Y rÃ³Å¼niÄ… siÄ™ od siebie.\r\nZasymujmy dane Abelsona przy uÅ¼yciu jÄ™zyka R. Powiedzmy, Å¼e mamy 1000 profesjonalnych graczy. Dla kaÅ¼dego losujemy jego prawdziwÄ… Å›redniÄ… pojedyÅ„czych sukcesÃ³w ze skrÃ³conego rozkÅ‚adu normalnego (miÄ™dzy 0.2 a 0.3), o Å›redniej 0.27 i odchyleniu standardowym 0.25. NatÄ™pnie kaÅ¼demu graczowi losujemy 520 uderzeÅ„ (tyle jest w sezonie), a prawdopodobieÅ„stwo sukcesu wyznacza jego Å›rednia.\r\nMamy zmienne: \\(X\\) - zmienna binarna (1 - sukces w pojedyÅ„czyn uderzeniu, 0 - brak sukcesu), \\(Y\\) Å›rednia liczba uderzeÅ„ w sezonie, \\(Z\\) - poszczegÃ³lny gracz.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(truncnorm)\r\nlibrary(knitr)\r\n\r\nmeans = rtruncnorm(1000, 0.2,0.3, 0.27, 0.25)\r\ndata = data.frame()\r\n\r\nfor( i in 1:1000) {\r\n  X = sample(c(0,1),520,TRUE, prob = c(1-means[i], means[i]))\r\n  true_mean = rep(means[i],520)\r\n  Y = rep(mean(X),520)\r\n  Subject = rep(i,520)\r\n  data = rbind(data,data.frame(X,Y,true_mean,Subject))\r\n}\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data$X,data$Y),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.078.\r\n\r\ncat( paste(\"Wariancja bÅ‚Ä™du wynosi\",\r\nround(sum(tapply(data$X, data$Y, Var)*tapply(data$X, data$Y, length)/length(data$X)),3)),\".\",sep = \"\")\r\n\r\nWariancja bÅ‚Ä™du wynosi 0.186.\r\n\r\ncat(paste(\"Wariancja przewidywania wynosi\", round(Var(data$X) \r\n- sum(tapply(data$X, data$Y, Var)*tapply(data$X, data$Y, length)/length(data$X)),3),\".\"))\r\n\r\nWariancja przewidywania wynosi 0.001 .\r\n\r\nJak widzimy korelacja jest tutaj bardzo maÅ‚a. Ale co siÄ™ stanie jeÅ›li zasymulujemy dane, w ktÃ³rych Å›rednie graczy bÄ™dÄ… losowane z rozkÅ‚adu z takÄ… samÄ… Å›redniÄ… i odchyleniem standardowym ale z ograniczeniem przedziaÅ‚u od 0 do 1.\r\n\r\n\r\nmeans = rtruncnorm(1000, 0,1, 0.27, 0.25)\r\ndata2 = data.frame()\r\nfor( i in 1:1000) {\r\n  \r\n  X = sample(c(0,1),520,TRUE, prob = c(1-means[i], means[i]))\r\n  true_mean = rep(means[i],520)\r\n  Y = rep(mean(X),520)\r\n  Subject = rep(i,520)\r\n  data2 = rbind(data2,data.frame(X,Y,true_mean,Subject))\r\n\r\n}\r\ncat(paste(\"Korelacja wynosi \", round(cor(data2$X,data2$Y),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.419.\r\n\r\ncat( paste(\"Wariancja bÅ‚Ä™du wynosi\",\r\nround(sum(tapply(data2$X, data2$Y, Var)*tapply(data2$X, data2$Y, length)/length(data2$X)),3)),\".\",sep = \"\")\r\n\r\nWariancja bÅ‚Ä™du wynosi 0.184.\r\n\r\ncat(paste(\"Wariancja przewidywania wynosi\", round(Var(data2$X) \r\n- sum(tapply(data2$X, data2$Y, Var)*tapply(data2$X, data2$Y, length)/length(data2$X)),3),\".\"))\r\n\r\nWariancja przewidywania wynosi 0.039 .\r\n\r\nOtrzymaliÅ›my duÅ¼o wyÅ¼szÄ… korelacjÄ™. Zobaczmy co siÄ™ tu staÅ‚o. Wariancja przewidywania znacznie wzrosÅ‚a, natomiast wariancja bÅ‚Ä™du zmalaÅ‚a. Dlaczego tak siÄ™ staÅ‚o skoro zwiÄ…zek pomiÄ™dzy zmiennymi jest taki sam w obu symulacjach?\r\nOdpowiedÅº na to pytanie zawiera siÄ™ w samym wzorze na wyjaÅ›ninÄ… wariancjÄ™. Przyjrzyjmy siÄ™ najpierw wariancji przewidywania \\(Var(E(X|Y))\\). PoniewaÅ¼ \\(Y = E(X|Z)\\) to wtedy \\(Var(E(X|Y)) = Var(E(X|Z)) = Var(Y)\\).\r\nTo znaczy, Å¼e wysokoÅ›Ä‡ wariancji przewidywania zaleÅ¼y tylko od tego jak bardzo Å›rednie graczy siÄ™ rÃ³Å¼niÄ…. Im bardziej gracze sÄ… do siebie podobni, tym mniejsza wariancja przewidywania w stosunku do wariancji bÅ‚Ä™du. Innymi sÅ‚owy jeÅ›li wszyscy gracze majÄ… bardzo podobnÄ… Å›redniÄ… uderzeÅ„, to nie bÄ™dzie ona dobrze wyjaÅ›niaÄ‡ rÃ³Å¼nic pomiÄ™dzy nimi.\r\nWariancja wyjaÅ›niana (caÅ‚kowita) skÅ‚ada siÄ™ z wariancji bÅ‚Ä™du i wariancji przewidywania. PoniewaÅ¼ X jest zmiennÄ… binarnÄ… to\r\n\\[Var(X) = E(X)(1-E(X))\\]\r\nDla przykÅ‚adu Abelsona wariancja caÅ‚kowita to \\(Var(X) = 0.27(1-0.27) = 0.1971\\).\r\nWariancja przewidywania w przykÅ‚adzie Abelsona zaleÅ¼y od tego jak rÃ³Å¼ne od siebie sÄ… Å›rednie graczy. skoro wiemy, Å¼e Å›rednie graczy znajdujÄ… siÄ™ w przedziale <0.2, 0.3> i majÄ… Å›redniÄ… 0.27, moglibyÅ›my siÄ™ pokusiÄ‡ o pytanie ile maksymalnie ta zmienna mogÅ‚aby wyjaÅ›niÄ‡ wariancji pojedyÅ„czego sukcesu?\r\nJeÅ›li mamy zmiennÄ… w przedziale \\(<0, c>\\) to: \\[\\displaystyle \\sum_i x_i^2 = \\sum_i x_i\\cdot x_i \\leq \\sum_i c\\cdot x_i = cn\\bar{x}\\]\r\nponadto:\r\n\\[\\begin{align*}\r\nn\\cdot \\text{Var}(\\mathbf{x}) &= \\sum_i (x_i - \\bar{x})^2= \\sum_i x_i^2 - 2x_i\\bar{x} + \\bar{x}^2\\\\\r\n&= \\sum_i x_i^2 - 2\\bar{x}\\sum_i x_i + n\\bar{x}^2= \\sum_i x_i^2 - n\\bar{x}^2\\\\\r\n&\\leq cn\\bar{x} - n\\bar{x}^2 = n\\bar{x}(c-\\bar{x})\r\n\\end{align*}\\]\r\nwiÄ™c\r\n\\[\\text{Var}(\\mathbf{x}) \\leq \\bar{x}(c-\\bar{x})\\]\r\nÅ›rednia uderzeÅ„ graczy Abelsona\r\n\\[eta^2 = \\frac{Var(Y)}{Var(X)} \\le \\frac{0.07(0.1-0.07)}{0.1971} = 0.01065\\]\r\nmoÅ¼e w najlepszym wypadku wyjaÅ›niÄ‡ 1% wariancji. Widzimy ze wzoru, Å¼e zachowujÄ…c wartoÅ›Ä‡ Å›redniej sukcesÃ³w jako staÅ‚Ä…, rozszerzanie przedziaÅ‚u, w ktÃ³rym znajdujÄ… siÄ™ moÅ¼liwe Å›rednie graczy, bÄ™dzie zwiÄ™kszaÅ‚o procent moÅ¼liwej do wyjaÅ›nienia wariancji.\r\nWydaje mi siÄ™, Å¼e zdziwienie Abelsona wynikaÅ‚o z faktu, Å¼e spodziewaÅ‚ siÄ™, Å¼e procent wyjaÅ›nionej wariancji w pojedyÅ„czym sukcesie przez Å›redniÄ… gracza powinen byÄ‡ wysoki, poniewaÅ¼ przewidujemy zachowanie na podstawie Å›redniej miary tegoÅ¼ zachowania, czyli jego Å›redniej w dÅ‚uÅ¼szym okresie (analgoicznie do neurotyzmu majÄ…cego przewidywaÄ‡ zachowania neurotyczne).\r\nJednak Abelson liczÄ…c procent wyjasnionej wariancji zadaÅ‚ inne pytanie, mianowicie jak dobrze moÅ¼emy przewidzieÄ‡ sukces gracza w pojedyÅ„czej prÃ³bie na podostawie jego Å›redniej. Jak zauwaÅ¼yliÅ›my, nie bÄ™dzie dobrze przewidywaÄ‡, poniewaÅ¼ zmienna Y nie rÃ³Å¼nicuje wystarczajÄ…co dobrze pomiÄ™dzy graczami wzglÄ™dem wielkoÅ›ci bÅ‚Ä™du. MoÅ¼na to rozumieÄ‡ w katagoriach niskiej rzetelnoÅ›ci.\r\nTo zjawisko generalizuje siÄ™ na inne przykÅ‚ady. Å»eby sobie to zwizualizowaÄ‡ zobaczmy Å›rednie graczy w sezonie z symulacji pierwszej (Å›rednia z 520 prÃ³b pojedyÅ„czych odbiÄ‡) naprzeciw prawdziwych Å›rednich (prawdopodobieÅ„stw, ktÃ³rych uÅ¼yÅ‚em do wygenerowania danych, moÅ¼emy je potraktowaÄ‡ jako ich prawdziwÄ… zdolnoÅ›Ä‡ gry w bejsbol).\r\n\r\nGeneralizuje siÄ™ fakt, Å¼e im bardziej rÃ³Å¼ne od siebie sÄ… Å›rednie warunkowe E(X|Y), tym wiÄ™cej Y wyjaÅ›ni wariancji X.\r\n\r\n\r\ndata_plot <- data %>% group_by(Subject) %>% summarise(skill = min(true_mean), mean = min(Y))\r\n\r\n\r\nplot(data_plot$mean,data_plot$skill)\r\n\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data_plot$skill,data_plot$mean),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.815.\r\n\r\nTeraz to samo z symulacjÄ… 2.\r\n\r\n\r\ndata2_plot <- data2 %>% group_by(Subject) %>% summarise(skill = min(true_mean), mean = min(Y))\r\n\r\nplot(data2_plot$mean,data2_plot$skill)\r\n\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data2_plot$skill,data2_plot$mean),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.996.\r\n\r\nJak widzimy korelacja jest wiÄ™ksza w drugiej symulacji, mimo, Å¼e zwiÄ…zek pomiÄ™dzy zmiennymi jest taki sam. Mamy tu do czynienia ze zjawiskiem Restricted Range Corelation - czyli sytuacji w ktÃ³rej zmienna w prÃ³bie ma mniejszÄ… wariancjÄ™ niÅ¼ w populacji. PoniewaÅ¼ obserwacje sÄ… do siebie bardziej podobne (sÄ… bliÅ¼ej siebie na wykresie), wpÅ‚yw bÅ‚Ä™du jest wiÄ™kszy. PrzybliÅ¼my teraz drugi wykres tak, by widzieÄ‡ wartoÅ›ci prawdziwej Å›redniej z zakresu miÄ™dzy 0.2 a 0.3.\r\n\r\n\r\ndata3_plot = data2_plot %>% filter(skill >0.2 & skill < 0.3)\r\n\r\nplot(data3_plot$mean,data3_plot$skill)\r\n\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data3_plot$skill,data3_plot$mean),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.835.\r\n\r\nWidzimy, Å¼e ten wykres przypomina pierwszy wykres. ZawÄ™ziliÅ›my zakres zmiennej, wiÄ™c bÅ‚Ä…d wydaje siÄ™ relatywnie wiÄ™kszy.\r\nCo natomiast moÅ¼emy powiedzieÄ‡ o maÅ‚ych wielkoÅ›ciach efektu w psychologii? MaÅ‚y efekt, to po prostu maÅ‚y efekt. Nie kumuluje siÄ™ w czasie. Ale to nie znaczy, Å¼e jest nieistony. JeÅ›li badamy zachowanie czÅ‚owieka, byÅ‚oby wrÄ™cz dziwnie, jeÅ›li efekt jednej zmiennej byÅ‚by duÅ¼y, zwÅ‚aszcza w przypadku zÅ‚oÅ¼onych zachowaÅ„.\r\n\r\n\r\n\r\nAbelson, R. P. (1985). A variance explanation paradox: When a little is a lot. Psychological Bulletin, 97(1), 129.\r\n\r\n\r\nFunder, D. C., & Ozer, D. J. (2019). Evaluating effect size in psychological research: Sense and nonsense. Advances in Methods and Practices in Psychological Science, 2(2), 156â€“168.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-05-baseball/Statystyczne_Dygresje.jpg",
    "last_modified": "2023-05-01T14:22:09+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-18-tutorial-bayes-v/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "CzÄ™Å›Ä‡ V: Selekcja modeli, wybÃ³r rozkÅ‚adÃ³w a priori i inne przydatne rzeczy",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2023-06-18",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWstÄ™p\r\nSelekcja modeli\r\nGarÅ›Ä‡ wnioskÃ³w\r\n\r\n\r\nWstÄ™p\r\nDzisiejszy wpis bÄ™dzie nie bÄ™dzie w formie zamkniÄ™tej, bÄ™dÄ™ dodawaÅ‚ do niego treÅ›ci gdy czas pozwoli. Poruszymy problematykÄ™ selekcji modeli, wyboru rozkÅ‚adÃ³w a priori i innych rzeczy.\r\nSelekcja modeli\r\nWyprodukujmy sobie dane powiÄ…zane kwadratowo.\r\n\r\n\r\nset.seed(123)\r\nx = rnorm(100,1,1)\r\ny = 0.5*x^2 + rnorm(100,0,1)\r\nplot(x,y)\r\n\r\n\r\n\r\nNastÄ™pnie zamodelujemy zwiÄ…zek x i y liniowo i kwadratowo.\r\n\r\n\r\nlibrary(rjags)\r\nlibrary(coda)\r\nlibrary(MCMCvis)\r\n\r\nmod_lin = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(y)) {\r\n    y[i] ~ dnorm(a + b * x[i], sigma^-2)\r\n  }}\"\r\n\r\nmod_sq = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(y)) {\r\n    y[i] ~ dnorm(a + b * x[i]^2, sigma^-2)\r\n  }}\"\r\n\r\n\r\nparams = c(\"a\",\"b\",\"sigma\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 3000\r\nnt = 1\r\nnc = 3\r\n\r\nmodel_lin = jags.model(file = textConnection(mod_lin), data = list(x = x, y = y), n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(model_lin, n.iter=ni, by=1)\r\nsamples_lin = coda.samples(model_lin, params, n.iter = nb, thin = nt)\r\n\r\nmodel_sq = jags.model(file = textConnection(mod_sq), data = list(x = x, y = y), n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(model_sq, n.iter=ni, by=1)\r\nsamples_sq = coda.samples(model_sq, params, n.iter = nb, thin = nt)\r\n\r\n\r\nZerknijmy na przedziaÅ‚y wiarygodnoÅ›ci dla wspÃ³Å‚czynnikÃ³w regresji.\r\n\r\n\r\nsummary(samples_lin)\r\n\r\n\r\nIterations = 3101:6100\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean      SD  Naive SE Time-series SE\r\na     -0.2599 0.16924 0.0017839      0.0034188\r\nb      1.0637 0.11957 0.0012604      0.0024116\r\nsigma  1.0846 0.07813 0.0008235      0.0008773\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%     50%     75%   97.5%\r\na     -0.5962 -0.3719 -0.2589 -0.1466 0.07121\r\nb      0.8262  0.9833  1.0639  1.1423 1.29861\r\nsigma  0.9458  1.0309  1.0785  1.1331 1.24936\r\n\r\n\r\n\r\nsummary(samples_sq)\r\n\r\n\r\nIterations = 3101:6100\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean      SD  Naive SE Time-series SE\r\na     -0.02662 0.13115 0.0013825      0.0022044\r\nb      0.46027 0.04316 0.0004550      0.0007105\r\nsigma  0.98054 0.07148 0.0007535      0.0008375\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%      50%     75%  97.5%\r\na     -0.2868 -0.1154 -0.02372 0.06243 0.2257\r\nb      0.3759  0.4309  0.46096 0.48936 0.5443\r\nsigma  0.8522  0.9304  0.97654 1.02500 1.1327\r\n\r\nW obydwu przypadkach przedziaÅ‚y nie zawierajÄ… zera. Musimy rozsÄ…dziÄ‡, ktÃ³ry model jest lepszy. MoglibyÅ›my zrobiÄ‡ to przy pomocy juÅ¼ omÃ³wionego Czynnika Bayesa. Jednak, tak jak juÅ¼ wspominaÅ‚em, obliczenie go w bardziej skomplikowanych modelach jest problematyczne.\r\nW modelach czÄ™stoÅ›ciowych uÅ¼ywa siÄ™ kryteriÃ³w informacyjnych takich jak AIC i BIC do selekcji modeli. W statystyce Bayesowskiej do oceny dopoasowania modelu moÅ¼emy uÅ¼yÄ‡ Deviance Information Cryterion (DIC).\r\n\\[\\text{DIC} = D(\\bar{\\theta}) + 2*p_D\\]\r\ngdzie:\r\n\\[ D(\\theta) =  -  2\\log ({\\rm{P}}(y|\\theta ))\\]\r\n\\[ p_D= D(\\bar{\\theta}) -  \\overline{D(\\theta)}\\]\r\nDIC jest zdefiniowane jako rÃ³Å¼nica miÄ™dzy (ujemnym) logarytmem funkcji wiarygodnoÅ›ci zewaluowanym dla Å›redniej wartoÅ›ci parametrÃ³w \\(\\theta\\) z rozkÅ‚adu post priori \\(D(\\bar{\\theta})\\) (gdzie dla symetrycznych rozkÅ‚adÃ³w otrzymujemy najbardziej wiarygodne wartoÅ›ci) a dewiacjÄ… \\(p_D\\). Dewiacja, z kolei, jest obliczana jako rÃ³Å¼nica miÄ™dzy wartoÅ›ciÄ… funkcji wiarygodnoÅ›ci \\(D(\\bar{\\theta})\\) a Å›redniÄ… wartoÅ›ciÄ… logarytmu funkcji wiarygodnoÅ›ci dla wszystkich parametrÃ³w.\r\nInnymi sÅ‚owy, wartoÅ›Ä‡ \\(D(\\bar{\\theta})\\) dostarcza informacji na temat skupienia gÄ™stoÅ›ci prawdopodobieÅ„stwa wokÃ³Å‚ dominujÄ…cej wartoÅ›ci \\(\\bar{\\theta}\\). Im lepszy jest model, tym mniejsza jest ta wartoÅ›Ä‡. JednakÅ¼e, gdy liczba parametrÃ³w w modelu wzrasta, wartoÅ›Ä‡ \\(D(\\bar{\\theta})\\) moÅ¼e rÃ³wnieÅ¼ maleÄ‡, poniewaÅ¼ bardziej skomplikowany model ma wiÄ™kszÄ… elastycznoÅ›Ä‡ w dopasowaniu siÄ™ do danych. Dlatego drugi skÅ‚adnik, \\(\\overline{D(\\theta)}\\), mierzy stopieÅ„ rozproszenia wiarygodnoÅ›ci wokÃ³Å‚ dominujÄ…cej wartoÅ›ci. Im wiÄ™cej parametrÃ³w, tym wiÄ™ksza wartoÅ›Ä‡ tego skÅ‚adnika.\r\nPodsumowujÄ…c, modele z mniejszÄ… wartoÅ›ciÄ… DIC sÄ… preferowane, poniewaÅ¼ sugerujÄ… wiÄ™ksze skupienie gÄ™stoÅ›ci prawdopodobieÅ„stwa wokÃ³Å‚ dominujÄ…cej wartoÅ›ci, jednoczeÅ›nie uwzglÄ™dniajÄ…c liczbÄ™ parametrÃ³w w modelu.\r\nPoliczmy DIC dla naszych modeli.\r\n\r\n\r\nDic_lin = dic.samples(model_lin,1000)\r\nDic_lin\r\n\r\nMean deviance:  298.6 \r\npenalty 3.053 \r\nPenalized deviance: 301.6 \r\n\r\n\r\n\r\nDic_sq = dic.samples(model_sq,1000)\r\nDic_sq\r\n\r\nMean deviance:  278.3 \r\npenalty 3.061 \r\nPenalized deviance: 281.3 \r\n\r\n\r\n\r\ndiffdic(Dic_sq,Dic_lin)\r\n\r\nDifference: -20.32408\r\nSample standard error: 9.939601\r\n\r\nWidzimy, Å¼e model z kwadratowym zwiÄ…zkiem pomiedzy x i y jest lepiej dopasowany do danych.\r\nPrÃ³cz DIC, w statystyce Bayesowskiej narzÄ™dziami do oceny dopasowania modeli sÄ… Widely Applicable Information Criterion (WAIC) oraz Leave-one-out cross-validation (LOO-CV). SÄ… to metody, ktÃ³re nie sÄ… zaimplementowane w JAGS, jednak warto je poznaÄ‡, poniewaÅ¼ posiadajÄ… lepsze wÅ‚aÅ›ciwoÅ›ci (Gelman et al., 2014).\r\n## WybÃ³r rozkÅ‚adÃ³w a priori\r\nTo chyba najtrudniejszy temat w statystyce Bayesowskiej. Zasadniczo zgodnie z ortodoksyjnÄ… filozofiÄ… BayesowskÄ… rozkÅ‚ady a priori powinny odzwierciedlaÄ‡ naszÄ… wczeÅ›niejszÄ… wiedzÄ™. Filozofia ta moÅ¼e wydawaÄ‡ siÄ™ kuszÄ…ca, poniewaÅ¼ tak wÅ‚aÅ›nie powinna dziaÅ‚aÄ‡ nauka. AkumulowaÄ‡ wiedzÄ™ i nastÄ™pnie po otrzymaniu nowych danych aktualizowaÄ‡ swoje oczekiwania. Jednak gdy przechodzimy do modelowania pojawia siÄ™ problem.\r\nJak przedstawiÄ‡ naszÄ… wiedzÄ™ w postaci rozkÅ‚adÃ³w a priori? Czy jeÅ›li uÅ¼yjemy informatywnego rozkÅ‚adu dla wspÃ³Å‚czynnika regresji, powiedzmy \\(N(100,0.001)\\), i po otrzymaniu rozkÅ‚adu a posteriori przedziaÅ‚ wiarygodnoÅ›ci nie bÄ™dzie zawieraÅ‚ 0, czy wÅ‚aÅ›nie nie oszukaliÅ›my by uzyskaÄ‡ â€œistotnyâ€ wspÃ³Å‚czynnik regresji?\r\nZ kolei jeÅ›li uÅ¼yjemy zbyt informatywnego rozkÅ‚adu skoncentrowanego wokÃ³Å‚ zera, moÅ¼emy ograniczyÄ‡ naszÄ… zdolnoÅ›Ä‡ do wykrycia istniejÄ…cego efektu.\r\nRozwaÅ¼my nastÄ™pujÄ…cy przypadek:\r\n\r\nWidzimy tu przykÅ‚ad zastosowania bardzo restrykcyjnego rozkÅ‚adu a priori. RozkÅ‚ad posteriori nie bardzo przypomina rozkÅ‚ad a priori. Z kolei gdybyÅ›my brali pod uwagÄ™ tylko wiarygodnoÅ›Ä‡ (tzn, zastosowalibyÅ›my pÅ‚aski rozkÅ‚ad a priori), rÃ³Å¼niÅ‚by siÄ™ on znacznie od rozkÅ‚adu a priori.\r\nPrzypomnijmy sobie jakÄ… interpretacje ma rozkÅ‚ad a posteriori i zastosujmy jÄ… do powyÅ¼szego wykresu: Pod warunkiem naszych zaobserwowanych danych, zaobserwowane przez nas dane majÄ… znikome prawdopodobieÅ„stwo zaobserwowania.\r\nHmm. Tak jak wspominaÅ‚em ustalenie wÅ‚aÅ›ciwych rozkÅ‚adÃ³w to rzecz nietrywialna.\r\nZasadniczo, moÅ¼liwe podejÅ›cia do rozkÅ‚adÃ³w a priori:\r\nUÅ¼ywanie nieinformaywnych priorÃ³w (np. rozkÅ‚adÃ³w jednostajnych) - czÄ™ste i najprostsze podejÅ›cie. Zalety: Dobre do celÃ³w eksploracyjnych. Wady: To gÅ‚Ä™bszy temat, jednak w wielu przypadkach uÅ¼ycie sÅ‚abych priorÃ³w moÅ¼e znaczÄ…co zbiasowaÄ‡ wyniki naszej analizy. RozszerzÄ™ ten punkt, a na razie odsyÅ‚am do wpisu Andrewa Gelmana.\r\nEmpiryczny Bayes. MoÅ¼emy uÅ¼yÄ‡ danych by wyestymowaÄ‡ rozkÅ‚ady a prior. PrzykÅ‚ad moÅ¼ecie znaleÅºÄ‡ w pierwszej czÄ™Å›ci tutorialu (przykÅ‚ad z ksiÄ™garniÄ…). Jest to uÅ¼yteczne narzÄ™dzie, jednak niekoniecznie zawsze najlepsze do wnioskowania statystycznego. To znaczy, naleÅ¼y rozwaÅ¼yÄ‡ na ile moÅ¼emy sobie pozwoliÄ‡ na â€œzajrzenieâ€ do danych, zanim uÅ¼yjemy docelowego modelu.\r\nWybieranie informatywnych rozkÅ‚adÃ³w a priori . W jaki sposÃ³b? Zwykle posiadamy jakieÅ› informacje na temat badanego zjawiska. Jakiej wielkoÅ›ci efektu moÅ¼emy siÄ™ spodziewaÄ‡? JeÅ›li wspÃ³Å‚czynnik regresji 0.1 to maksymalna rozsÄ…dna wartoÅ›Ä‡ na jakÄ… moÅ¼emy liczyÄ‡, poniewaÅ¼ wiemy, Å¼e inne zbadane w literaturze efekty znacznie waÅ¼niejszych predykatorÃ³w osiÄ…gajÄ… maksymalnie taki efekt, to \\(N(0,1)\\) nie bÄ™dzie najlepszym priorem, poniewaÅ¼ ponad 90% gÄ™stoÅ›ci prawdopodobieÅ„stwa znajduje siÄ™ poza przedziaÅ‚em [-0.1,0.1]. SÅ‚aby prior mÃ³gÅ‚by na przykÅ‚ad sprawiÄ‡, Å¼e wykrylibyÅ›my efekt â€œistotny, ale trywialnyâ€, jak na przykÅ‚ad korelacjÄ™ miÄ™dzy atrakcyjnoÅ›ciÄ… rodzicÃ³w a prawdopodobieÅ„stwem urodzenia dziewczynki. Ten pozorny efekt nie zostaÅ‚ by uznany za istotny, gdyby uÅ¼yto informatywnego rozkÅ‚adu a priori, ktÃ³ry uwzglÄ™dniaÅ‚ wielkoÅ›ci efektÃ³w dla zmiennych takich jak np. wiek matki (Gelman et al., 2017).\r\nOgÃ³lne wskazÃ³wki dotyczÄ…ce wybierania rozkÅ‚adÃ³w a priori (niezaleÅ¼ne od wyÅ¼ej wymienionych strategii):\r\nRozkÅ‚ad a priori powinien ksztaÅ‚tem przypominaÄ‡ rozkÅ‚ad modelowanego parametru. JeÅ›li parametr jest Å›redniÄ… - to zwykle zamodelujemy go rozkÅ‚adem normalnym. Co w przypadku innych parametrÃ³w? Warto zobaczyÄ‡ jak takie parametry siÄ™ zachowujÄ…. Policzmy sobie wariancjÄ™ z 300^2 prÃ³b, w ktÃ³rych losujemy 25 obserwacji z rozkÅ‚adu normalnego.\r\n\r\n\r\nvariances = replicate(300^2,var(rnorm(25,0,1)))\r\nplot(density(variances))\r\n\r\n\r\n\r\nJak widzimy, rozkÅ‚ad wariancji jest prawoskoÅ›ny. RozkÅ‚adem prawdopodobieÅ„stwa, ktÃ³rym czÄ™sto modeluje siÄ™ wariancjÄ™ jest rozkÅ‚ad Gamma.\r\n\r\n\r\ncurve(dgamma(x,7,7),0,3)\r\n\r\n\r\n\r\nGarÅ›Ä‡ wnioskÃ³w\r\nStatystyka Bayesowska ma swoje wady i zalety. W tutorialu skupiliÅ›my siÄ™ gÅ‚Ã³wnie na metodzie numerycznej, jakÄ… jest MCMC, do modelowania. Ta metoda umoÅ¼liwia takÅ¼e estymacjÄ™ modeli, ktÃ³re nie majÄ… rozwiÄ…zaÅ„ analitycznych (albo ich nie znamy). MoglibyÅ›my estymowaÄ‡ parametry takich modeli za pomocÄ… zwykÅ‚ej optymalizacji. JednakÅ¼e, wtedy pojawia siÄ™ problem, poniewaÅ¼ nie moÅ¼emy obliczyÄ‡ przedziaÅ‚Ã³w ufnoÅ›ci. Mnie interesuje mnie gÅ‚Ã³wnie wnioskowanie statystyczne, a podejÅ›cie Bayesowskie dostarcza nie punktowÄ… wartoÅ›Ä‡ parametru tylko jego rozkÅ‚ad, co pozwala oceniÄ‡ niepewnoÅ›Ä‡ zwiÄ…zana z tymi parametrami!\r\n\r\n\r\n\r\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for bayesian models. Statistics and Computing, 24(6), 997â€“1016.\r\n\r\n\r\nGelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19(10), 555.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-18-tutorial-bayes-v/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-06-24T01:09:21+02:00",
    "input_file": "tutorial-bayes-v.knit.md"
  },
  {
    "path": "posts/2023-04-22-radiosortownia/",
    "title": "Radio Sortownia",
    "description": "Czego sÅ‚uchaÄ‡ podczas czytania tekstÃ³w naukowych?",
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2023-04-25",
    "categories": [],
    "contents": "\r\nPost moÅ¼e byÄ‡ aktualizowany po publikacji â€“ jeÅ›li pojawiÄ… siÄ™ nowe ciekawe ÅºrÃ³dÅ‚a, dodamy je.\r\n\r\nCzego sÅ‚uchaÄ‡ podczas czytania artykuÅ‚Ã³w lub podczas nauki? KaÅ¼dy moÅ¼e sÅ‚uchaÄ‡ czego chce i tego co lubi. Temat zamkniÄ™ty!\r\nAle nie koÅ„czmy jeszcze. Wszyscy mamy jakieÅ› preferencje co do muzyki (lub jej braku) do pracy i nauki. To moÅ¼e teÅ¼ zmieniaÄ‡ siÄ™ w czasie â€“ o jednej porze dnia cisza i zatyczki, kiedy indziej jak najgÅ‚oÅ›niejszy pop, edm i top100 miesiÄ…ca. Jazz-metal, Motown, disco-polo, trap, french touch â€“ kaÅ¼dy gatunek jest dobry, jeÅ›li dziaÅ‚a. RÃ³Å¼ne sÄ… teÅ¼ potrzeby. MoÅ¼emy chcieÄ‡ wprowadziÄ‡ siÄ™ w stan flow, kiedy jesteÅ›my hiper-kreatywni, Å‚atwo Å‚Ä…czymy wiele elementÃ³w i ogarniamy caÅ‚y kontekst naraz. Albo, moÅ¼emy dÄ…Å¼yÄ‡ do uzyskania gÅ‚Ä™bokiego spokoju i wysokiej koncentracji (warto poÅ‚Ä…czyÄ‡ z 5-10 minutami gÅ‚Ä™bokiego oddychania/medytacji).\r\nW kontekÅ›cie skutecznej muzyki do nauki czÄ™sto wymienia siÄ™ ambient, lo-fi, techno, muzykÄ™ klasycznÄ…, Å›cieÅ¼ki dÅºwiÄ™kowe do gier i filmÃ³w. A wiÄ™c muzyka albo spokojna, albo rytmiczna i pozbawiona sÅ‚Ã³w.\r\nRÃ³Å¼ne playlisty znajdziecie na dowolnej platformie streamingowej. W Radiu Sortownia podejdziemy jednak do sprawy inaczej. PokaÅ¼emy mniej i bardziej znane alternatywne projekty soniczne. WspÃ³lnotowo-spÃ³Å‚dzielcze rozgÅ‚oÅ›nie, ambientowe kolaÅ¼e, nieskoÅ„czone rozmowy filozoficzne. DÅºwiÄ™ki rozpiÄ™te w czasie jak Å¼agle, rozgÅ‚oÅ›nie z Madagaskaru i sposoby na sen.\r\nRadio Sortownia zaprasza na nastÄ™pujÄ…ce doÅ›wiadczenia:\r\nSomaFMhttps://somafm.com/\r\nSomaFM to niezaleÅ¼ne radio (bez reklam!), obejmujÄ…ce wiele stacji tematycznych z muzykÄ… niezaleÅ¼nÄ…. Ambient, downtempo, vaporwave, IDM, jazz, doom metal, folk etc. SÄ… rÃ³wnieÅ¼ kanaÅ‚y z muzykÄ… dla szpiegÃ³w i z muzykÄ… dla hakerÃ³w. Space Station Soma sprawdza siÄ™ w codziennej pracy astronautÃ³w, a takÅ¼e o szÃ³stej nad ranem, kiedy trzeba oddaÄ‡ esej z etnografii na ostatni termin.\r\nRadia wspÃ³lnotowehttps://monoskop.org/Community_radio\r\nPod powyÅ¼szym linkiem znajduje siÄ™ lista niezaleÅ¼nych rozgÅ‚oÅ›ni z caÅ‚ego Å›wiata. PomiÄ™dzy nimi jest polskie Radio KapitaÅ‚, ktÃ³re poza graniem muzyki jest teÅ¼ platformÄ… dyskusji nt. kultury, sztuki i spoÅ‚eczeÅ„stwa. DuÅ¼Ä… zaletÄ… takich rozgÅ‚oÅ›ni jest fakt, Å¼e muzykÄ™ dobierajÄ… w nich DJe i DJki z gÅ‚Ä™bokÄ… wiedzÄ… o kontekÅ›cie muzyki i subkulturach, scenach â€“ to coÅ›, czego nie dostarczÄ… algorytmy Spotify.\r\nRadio Gardenhttp://radio.garden/\r\nRadio Garden to aplikacja przeglÄ…darkowa i na telefony, pozwalajÄ…ca lataÄ‡ po kuli ziemskiej, w poszukiwaniu odlegÅ‚ych stacji radiowych. MoÅ¼na sprawdziÄ‡ co grajÄ… w Boliwii, Tajlandii, Nashville, Kirgistanie. MoÅ¼na posÅ‚uchaÄ‡ dziwnych reklam z RPA, gadanego radia z Japonii, romantycznych indonezyjskich ballad. Radio Garden przyda siÄ™ teÅ¼ w nauce jÄ™zykÃ³w, np. moÅ¼emy wybraÄ‡ siÄ™ w dÅ‚ugÄ… podrÃ³Å¼ po Francji i Hiszpanii, przy okazji osÅ‚uchujÄ…c siÄ™ z lokalnÄ… wymowÄ…. JeÅ›li przytÅ‚oczÄ… nas (zdecydowanie najczÄ™stsze) stacje popowe, moÅ¼emy zajrzeÄ‡ na playlisty z wybranymi stacjami radiowymi. Na Independent Sounds znajdziemy m.in. legendarne NTS, na Rare Tongues posÅ‚uchamy rzadkich i zagroÅ¼onych jÄ™zykÃ³w.\r\nRadiooooohttps://radiooooo.com/\r\nTa aplikacja pozwala trenowaÄ‡ nie tylko przesuniÄ™cia geograficzne, ale i temporalne. MoÅ¼emy wybraÄ‡ kraj orazâ€¦ dekadÄ™. MoÅ¼emy wyobraziÄ‡ sobie jakby to byÅ‚o Å¼yÄ‡ w Polsce w latach 70., albo w Iranie lat 60. Czego sÅ‚uchano w Chinach po II Wojnie Åšwiatowej? Czy w Egipcie ludzie taÅ„czyli do disco w latach 80.? Czym Å¼yÅ‚a australijska scena muzyczna w 1913 roku?\r\nYou are listening to, Lofi Air Traffic Controlhttp://youarelistening.to/https://www.lofiatc.com\r\nYou are listening to (_____) jest kolaÅ¼em dÅºwiÄ™kowym, Å‚Ä…czÄ…cym ambient z miastami. Strona nakÅ‚ada na siebie muzykÄ™ z publicznie dostÄ™pnymi transmisjami nadawanymi przez policjÄ™, straÅ¼e poÅ¼arne i lotniska. Lofi Air Traffic Control jest podobnym pomysÅ‚em, tylko za ambient wskakuje lofi.\r\nRadia akademickiehttp://radiocentrum.pl/redakcja/polskie-rozglosnie-akademickie/ https://pl.wikipedia.org/wiki/Kategoria:Studenckie_rozg%C5%82o%C5%9Bnie_radiowe_w_Polsce\r\nWarto przypomnieÄ‡ teÅ¼ o istnieniu rozgÅ‚oÅ›ni akademickich. PojawiajÄ… siÄ™ w nich autorskie audycje z selekcjÄ… muzyki, ale teÅ¼ dyskusje i wywiady. SpoÅ‚ecznoÅ›ciowy i lokalny charakter tych rozgÅ‚oÅ›ni nie kÅ‚Ã³ci siÄ™ z tym, Å¼e majÄ… one profesjonalny poziom produkcji.\r\nWorld Concert Hall https://www.worldconcerthall.com/\r\nNie zapominajÄ…c o muzyce klasycznej, poÅ‚Ä…czmy siÄ™ z salami koncertowymi na caÅ‚ym Å›wiecie, dziÄ™ki World Concert Hall. Tu znajdziemy linki do streamÃ³w audio na Å¼ywo m.in. z Madrytu, Wiednia i Katowic. JeÅ›li to za maÅ‚o, warto zajrzeÄ‡ na https://www.openculture.com/ â€“ w panelu po prawej jest kategoria Free music, gdzie moÅ¼na znaleÅºÄ‡ wiÄ™cej linkÃ³w do projektÃ³w zwiÄ…zanych z muzykÄ… klasycznÄ….\r\nSleep with me Podcasthttp://www.sleepwithmepodcast.com/\r\nJeÅ›li potrzebujemy zasnÄ…Ä‡ lub na naszÄ… koncentracjÄ™ pozytywnie wpÅ‚ywa gadanie w tle, moÅ¼na sprÃ³bowaÄ‡ Sleep with me. SÄ… to wielogodzinne gawÄ™dziarskie przypÅ‚ywy i odpÅ‚ywy, potoki bezsensu i niesamowitych historii. CoÅ› podobnego zrobiÅ‚ teÅ¼ kiedyÅ› Jeff Bridges: http://www.dreamingwithjeff.com/#music-section\r\nInfinite conversationhttps://infiniteconversation.com/\r\nWchodzÄ…c w sferÄ™ dÅºwiÄ™kÃ³w generowanych, moÅ¼emy natrafiÄ‡ na nieskoÅ„czonÄ… dyskusjÄ™ Slavoja Å½iÅ¾ka z Wernerem Herzogiem. Modele neuronowe sÅ‚oweÅ„skiego filozofa i apokaliptycznego reÅ¼ysera gadajÄ… ze sobÄ… o Å¼yciu, Å›mierci i pieszych wÄ™drÃ³wkach. Jest to rÃ³wnie gÅ‚Ä™bokie, co pÅ‚ytkie. Åšmieszne i niepokojÄ…ce. â€œYes, we will simply die. Itâ€™s an empty gesture, but maybe this is the only way to live in peace.â€\r\nMykoLycohttps://www.youtube.com/@MycoLyco\r\nZastanawialiÅ›cie siÄ™ kiedyÅ› â€“ co siÄ™ stanie, jeÅ›li podÅ‚Ä…czy siÄ™ grzyby do syntezatorÃ³w? Na tym kanale znajdziecie odpowiedÅº. RÃ³Å¼ne rodzaje grzybÃ³w!\r\nListen to Wikipediahttp://listen.hatnote.com/\r\nBells are additions, strings are subtractions. Na tej stronie moÅ¼emy posÅ‚uchaÄ‡ na Å¼ywo zmian dokonywanych na Wikipedii. KaÅ¼da zmiana ma tu reprezentacjÄ™ dÅºwiÄ™kowÄ… oraz wizualnÄ…. UspokajajÄ… nie tylko dÅºwiÄ™ki, ale i Å›wiadomoÅ›Ä‡, Å¼e najwiÄ™ksza encyklopedia Å›wiata jest dynamicznym, wiecznie rozwijajÄ…cym siÄ™ organizmem spoÅ‚ecznym.\r\nApollo in Real-time https://apolloinrealtime.org/\r\nTu z kolei moÅ¼na posÅ‚uchaÄ‡ trzech misji ksiÄ™Å¼ycowych â€“ w takim czasie, w jakim one naprawdÄ™ trwaÅ‚y, tzn. w caÅ‚oÅ›ci. SÄ… to transmisje rozmÃ³w miÄ™dzy astronautami a kontrolÄ… lotÃ³w, ale â€“ sÄ… one zsynchronizowane z wideo i wykonywanymi przez misjÄ™ zdjÄ™ciami!\r\nï¹ï¹ï¹ï¹ï¹ï¹ï¹ï¹ï¹ï¹\r\nZnacie inne podobne projekty? Napiszcie! Dodamy do listy.\r\nâ–â–â–\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-22-radiosortownia/boom.png",
    "last_modified": "2023-05-01T14:22:21+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-06-narzeczepub/",
    "title": "Na rzecz formatu epub w polskiej nauce",
    "description": "Dlaczego przytÅ‚aczajÄ…ca wiÄ™kszoÅ›Ä‡ polskich artykuÅ‚Ã³w naukowych jest dostÄ™pna wyÅ‚Ä…cznie w formacie PDF?",
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2023-03-06",
    "categories": [
      "Mikromanifesty"
    ],
    "contents": "\r\nOd PDF do EPUB, ikonki via pngegg.comZacznijmy od rÃ³Å¼nic, a potem dopiero zastanÃ³wmy siÄ™ nad ich znaczeniem.\r\nPDF: - pozwala na stworzenie struktury i formatowania, ktÃ³re zawsze bÄ™dÄ… takie same, - nie pozwala na powiÄ™kszanie i zmianÄ™ wyÅ›wietlanego fontu (czcionki), odstÄ™pÃ³w itd., - obrazki i tabele bÄ™dÄ… zawsze w tym miejscu, w ktÃ³rym zaplanowaÅ‚a to osoba autorska, - plik moÅ¼e byÄ‡ skomplikowany graficznie (np. tekst opÅ‚ywajÄ…cy obraz, pola z kolorowym tÅ‚em), - drukowanie jest proste, dokument po wydrukowaniu bÄ™dzie wyglÄ…daÅ‚ tak jak plik.\r\nEPUB: - wyglÄ…d dokumentu zaleÅ¼y od preferencji uÅ¼ytkownikÃ³w, a takÅ¼e od urzÄ…dzenia wyÅ›wietlajÄ…cego plik, rozmiar strony jest tylko umowny, - dostosowywaÄ‡ moÅ¼na m.in. wielkoÅ›Ä‡ tekstu, interliniÄ™, marginesy, krÃ³j pisma, - obrazki i tabele mogÄ… siÄ™ rozjeÅ¼dÅ¼aÄ‡, jeÅ›li nie zostanÄ… dobrze sformatowane, - plik moÅ¼e zawieraÄ‡ urozmaicenia graficzne, ale te bÄ™dÄ… zaleÅ¼eÄ‡ od zakodowania ich w XML, HTML/CSS, wiÄ™c podlegajÄ… innej logice konstruowania, - drukowanie rÃ³wnieÅ¼ jest proste, ale plik po wydruku nie bÄ™dzie tak schludnie sformatowany, jak to byÅ‚oby w przypadku pliku PDF.\r\nPrzytÅ‚aczajÄ…ca wiÄ™kszoÅ›Ä‡ polskiej nauki zostaÅ‚a zaklÄ™ta w format PDF. Jest to zrozumiaÅ‚e, format PDF pozwala na mimikrÄ™ drukowanych artykuÅ‚Ã³w, czyli bÄ™dÄ… one wyglÄ…daÄ‡ tak samo w drukowanym zbiorze artykuÅ‚Ã³w, jak i na ekranie. PodkreÅ›lam frazÄ™ tak samo, poniewaÅ¼ przeczuwam, Å¼e jest ona rozumiana jako fraza â€“ to jest to samo. Skoro coÅ› wyglÄ…da identycznie, to moÅ¼na uznaÄ‡, Å¼e jest to ta sama rzecz. IdentycznoÅ›Ä‡ nie opiera siÄ™ na identycznoÅ›ci tekstu, tylko na graficznych pozorach podobieÅ„stwa. ArtykuÅ‚y naukowe cechuje wysoki stopieÅ„ oficjalnoÅ›ci, dotyczy to rÃ³wnieÅ¼ formatowania. PoszczegÃ³lne pisma majÄ… swoje wytyczne co do formatowania, moÅ¼e to byÄ‡ nawet ich cecha rozpoznawcza. Publikacja cyfrowa jest wiÄ™c domyÅ›lnie poddawana reÅ¼imowi formatowania tekstu. SpekulujÄ™, Å¼e ten sam tekst w pliku DOCX lub TXT bÄ™dzie uznany za mniej powaÅ¼ny, niÅ¼ tekst opublikowany w formacie PDF. To kwestia legitymizacji, staÅ‚e formatowanie dodaje tekstowi naukowoÅ›ci, powagi. Czy nie jest to jednak jedynie iluzja? JeÅ›li tekst jest dobry i badania sÄ… dobre, ich wartoÅ›Ä‡ siÄ™ nie zwiÄ™kszy przez dodanie oficjalnych nagÅ‚Ã³wkÃ³w, stopek i kolumn. Czy zmiana formatowania byÅ‚aby transgresjÄ…, przeciw instytucjonalnej oficjalnoÅ›ci? Czy artykuÅ‚ straciÅ‚by na wartoÅ›ci akademickiej?\r\nProblem\r\nGÅ‚Ã³wna zaleta formatu PDF (niemodyfikowalnoÅ›Ä‡) jest jednoczeÅ›nie najwiÄ™kszÄ… wadÄ…, w kontekÅ›cie pewnego, doÅ›Ä‡ istotnego procesu â€“ czytania. JeÅ›li narzucone formatowanie jest niewygodne do czytania, musimy to cierpliwie znieÅ›Ä‡. JeÅ›li uÅ¼ywamy urzÄ…dzenia, ktÃ³re niezbyt dobrze wyÅ›wietla pliki PDF â€“ musimy to znieÅ›Ä‡. JeÅ›li ukÅ‚ad dwu-, trzykolumnowy jest dla nas trudny do czytania â€“ musimy to znieÅ›Ä‡. JeÅ›li edytorzy wybrali nieczytelnÄ… czcionkÄ™ i/lub skandalicznie maÅ‚e odstÄ™py pomiÄ™dzy wierszami â€“ musimy to znieÅ›Ä‡.\r\nFormat tekstu nie powinien nie moÅ¼e byÄ‡ dodatkowym utrudnieniem, przy juÅ¼ i tak trudnej do zrozumienia treÅ›ci\r\nTo moÅ¼e byÄ‡ irytujÄ…ce dla osÃ³b sprawnych fizycznie i kognitywnie. Jednak dla osÃ³b niedowidzÄ…cych, czy dla dyslektykÃ³w â€“ to coÅ› wiÄ™cej niÅ¼ irytacja. To juÅ¼ kwestia dostÄ™pnoÅ›ci. Brak moÅ¼liwoÅ›ci zwiÄ™kszenia czcionki (lub zmiany czcionki na specjalnie dostosowanÄ… do potrzeb) moÅ¼e dosÅ‚ownie byÄ‡ barierÄ…, utrudniajÄ…cÄ… lub uniemoÅ¼liwiajÄ…cÄ… dostÄ™p do nauki.\r\nRozwiÄ…zanie\r\nProblemu tak naprawdÄ™ nie ma, jeÅ›li uznamy, Å¼e oficjalne formatowanie nie jest cechÄ… koniecznÄ… dla wartoÅ›ci artykuÅ‚u. Wystarczy opublikowaÄ‡ dwa pliki: PDF i EPUB.\r\nWystarczy opublikowaÄ‡ dwa pliki: PDF i EPUB.\r\nEPUB-y teÅ¼ mogÄ… byÄ‡ Å‚adnie sformatowane. OferujÄ… pod tym wzglÄ™dem nawet ciekawsze rozwiÄ…zania (albo po prostu inne), poniewaÅ¼ dobrze sformatowany plik EPUB powinien mieÄ‡ jasno podzielone sekcje z hiperlinkami, odsyÅ‚ajÄ…cymi do spisu treÅ›ci, oraz dobrze zorganizowane hiperlinki do przypisÃ³w. Format ten pozwala na duÅ¼o lepsze skupienie na treÅ›ci tekstu, a nie na jego wyglÄ…dzie â€“ ktÃ³ry w tym przypadku jest wzglÄ™dny.\r\nPrzykÅ‚ad wzorowej dystrybucji plikÃ³w, czyli wiele formatÃ³w do wyboru na stronie Wolnych Lektur\r\nPliki EPUB pozwalajÄ… na wygodne tworzenie zakÅ‚adek, zaznaczanie tekstu i dodawanie komentarzy. Pliki EPUB moÅ¼na teÅ¼ zabezpieczaÄ‡ znakami wodnymi lub innym rodzajem zabezpieczeÅ„ (DRM). Fakt, umieszczenie skomplikowanych tabel i grafik w epubie moÅ¼e daÄ‡ rozczarowujÄ…cy efekt (choÄ‡ rÃ³wnieÅ¼ zaleÅ¼ny od inteligentnego sformatowania pliku). W tym jednak rzecz, format pdf wciÄ…Å¼ zostaje pod rÄ™kÄ…. Konieczne jest danie ludziom wyboru. Tworzenie EPUB-Ã³w nie wymaga ponoszenia kosztÃ³w â€“ jest darmowe oprogramowanie, sÄ… konwertery. NajwiÄ™cej wysiÅ‚ku bÄ™dzie wymagaÄ‡ stworzenie pierwszego â€œpipelineâ€™uâ€, potoku przetwarzania: od czystego manuskryptu, do sformatowanego EPUB-a. KaÅ¼dy nastÄ™pny plik bÄ™dzie jedynie formalnoÅ›ciÄ…. ZresztÄ…, powracajÄ…c do poruszonej na poczÄ…tku kwestii legitymizacji â€“ zindywidualizowane formatowanie EPUB pod pismo naukowe moÅ¼e staÄ‡ siÄ™ nowym znakiem rozpoznawczym pisma. Warto sprÃ³bowaÄ‡!\r\nRepozytorium Biblioteka Nauki pozwala pobraÄ‡ artykuÅ‚y w wielu formatach\r\nA czemu nie publikacje w formacie HTML, lub PDF-y z dodatkowymi funkcjami dostÄ™pnoÅ›ci? EPUB-y sÄ… pod kaÅ¼dym wzglÄ™dem prostszym, bardziej porÄ™cznym i powszechnym rozwiÄ…zaniem. PojawiÄ… siÄ™ przypadki, w ktÃ³rych kaÅ¼dy z tych formatÃ³w bÄ™dzie najlepszy â€“ ale to tylko przesÅ‚anka do tego, by publikowaÄ‡ treÅ›ci w rÃ³Å¼nych formatach. Dodatkowo warto zapoznaÄ‡ siÄ™ z wyspecjalizowanymi formatami (BRF, DAISY, NIMAS).\r\nPodsumowujÄ…c â€“ dominacja formatu PDF w nauce wynika prawdopodobnie w znacznym stopniu z przyzwyczajenia i instytucjonalnej inercji. Nie jest to zÅ‚y format â€“ sprawdza siÄ™ dobrze w tym, czym jest, czyli w przechowywaniu dokumentÃ³w, bez moÅ¼liwoÅ›ci dostosowywania do czytelnikÃ³w. Nie uwzglÄ™dnia jednak tego, Å¼e czytelnicy sÄ… rÃ³Å¼ni i majÄ… rÃ³Å¼ne potrzeby. I w tym miejscu doskonaÅ‚ym uzupeÅ‚nieniem staje siÄ™ EPUB â€“ uniwersalny format do publikowania tekstu. EPUB pozwala czytelnikom m.in. na dostosowanie wielkoÅ›ci liter, odstÄ™pÃ³w i marginesÃ³w, dziÄ™ki czemu czytanie jest bardziej komfortowe, lub â€“ w ogÃ³le moÅ¼liwe. Pisma, ktÃ³re zdecydujÄ… siÄ™ dystrybuowaÄ‡ swoje artykuÅ‚y w wielu formatach wykaÅ¼Ä… siÄ™, oczywiÅ›cie, myÅ›lÄ… o potrzebach czytelnikÃ³w â€“ ale, moÅ¼e przede wszystkim, szacunkiem dla swoich czytelnikÃ³w.\r\n\r\nâ¬¥ â¬¥ â¬¥\r\n\r\n\r\nI to jest koniec tekstu. To nie jest poradnik tworzenia epubÃ³w, tylko mikromanifest.  WiÄ™cej informacji moÅ¼na znaleÅºÄ‡ poniÅ¼ej, zaÅ‚Ä…czam przydatne linki, zgrupowane w kategorie:\r\nOprogramowanie do tworzenia EPUB-Ã³w: ğŸ’¿ Sigil - polecany i darmowy program:https://sigil-ebook.com/sigil/  ğŸ’¿ Lista kilkunastu rÃ³Å¼nych programÃ³w:https://medevel.com/17-open-source-epub-and-ebook-creators/ \r\nOprogramowanie do czytania EPUB-Ã³w: ğŸ“€ Calibre - darmowy, wielofunkcyjny program:https://calibre-ebook.com/download  ğŸ“€ Sumatra - darmowy, rÃ³wnieÅ¼ wielofunkcyjny program:https://www.sumatrapdfreader.org/free-pdf-reader  ğŸ“€ Thorium - kolejna alternatywa:https://www.edrlab.org/software/thorium-reader/ ğŸ“€ Okular - i jeszcze jedna alternatywa:https://okular.kde.org/pl/ \r\nLaTeX do EPUB:https://minireference.com/blog/generating-epub-from-latex/https://pandoc.org/epub.html https://www.ctan.org/pkg/tex4ebook?lang=en\r\nKsiÄ…Å¼ki:\r\nğŸ“š Coolidge, A., Doner, S., Robertson, T., & Gray, J. (2018). Accessibility Toolkitâ€”2nd Edition. BCcampus. https://opentextbc.ca/accessibilitytoolkit/ â¡ ta publikacja jest darmowa! ğŸ“š Garrish, M. (2012). Accessible EPUB 3. Oâ€™Reilly Media, Inc.Â â¡ ta publikacja jest darmowa! (https://helion.pl/ksiazki/accessible-epub-3-matt-garrish,e_2g0p.htm#format/e)ğŸ“š Garrish, M., & Gylling, M. (2013). EPUB 3 Best Practices. Oâ€™Reilly Media, Inc.ğŸ“š Paszkiewicz, D., & DÄ™bski, J. (2013). DostÄ™pnoÅ›Ä‡ serwisÃ³w internetowych: Dobre praktyki w projektowaniu serwisÃ³w internetowych dostÄ™pnych dla osÃ³b z rÃ³Å¼nymi rodzajami niepeÅ‚nosprawnoÅ›ci. Stowarzyszenie PrzyjaciÃ³Å‚ Integracji. https://depot.ceon.pl/handle/123456789/5609\r\nMateriaÅ‚y w jÄ™zyku polskim:\r\nğŸ’¬ Wytyczne dla dostÄ™pnoÅ›ci informacji technologie informacyjno-komunikacyjne (TIK) w zapewnianiu dostÄ™pnoÅ›ci informacji w procesie uczenia siÄ™ (ICT4IAL):https://www.ict4ial.eu/guidelines-accessible-information \r\nğŸ’¬ Fundacja Instytut Rozwoju Regionalnego - E-podrÄ™cznik dostÄ™pny dla wszystkich. Poradnik dla twÃ³rcÃ³w elektronicznych materiaÅ‚Ã³w edukacyjnych:www.power.gov.pl%2Fmedia%2F13591%2Fe_podrecznik_dostepny_dla_wszystkich.pdf \r\nğŸ’¬ Dominik Paszkiewicz, Jakub DÄ™bski - PodrÄ™cznik DostÄ™pnoÅ›Ä‡ serwisÃ³w internetowych. Dobre praktyki w projektowaniu serwisÃ³w internetowych dostÄ™pnych dla osÃ³b z rÃ³Å¼nymi rodzajami niepeÅ‚nosprawnoÅ›ci:http://www.niepelnosprawni.pl/ledge/x/249472 \r\nğŸ’¬ Polska Akademia DostÄ™pnoÅ›ci:https://pad.widzialni.org/start \r\nğŸ’¬ Kategoria: RÃ³wnoÅ›Ä‡ i dostÄ™pnoÅ›Ä‡ w nauce w Sortowni Wiedzy:https://szpm.shinyapps.io/baza_wiedzy/\r\nğŸ’¬ eTechnologie - Standardy dostÄ™pnoÅ›ci w e-learningu (prezentacja):https://etechnologie.pl/standardy-dostepnosci-w-e-learningu/ \r\nğŸ’¬ DostÄ™pnoÅ›Ä‡ serwisÃ³w internetowych (rÃ³Å¼ne materiaÅ‚y):https://niepelnosprawni.gov.pl/a,60,dostepnosc-serwisow-internetowych\r\nğŸ’¬ ZespÃ³Å‚ Promocji ORE, Standardy przygotowywania i publikowania treÅ›ci oraz projektowania serwisÃ³w internetowych zgodnie z wytycznymi WCAG 2.0 na poziomie AA:https://www.ore.edu.pl/wp-content/uploads/2020/09/standardy-przygotowywania-tresci-zgodnie-z-wytycznymi-wcag-2.1_ore_2020.pdf + inne materiaÅ‚y: https://www.ore.edu.pl/materialy-do-pobrania/ \r\nğŸ’¬ BON UW: EPUB â€“ waÅ¼ny standard ksiÄ…Å¼ek elektronicznych (i inne teksty tematyczne):https://bon.uw.edu.pl/epub-%C2%AC-wazny-standard-ksiazek-elektronicznych/ \r\nğŸ’¬ Beata RÄ™dziak/NiepeÅ‚nosprawni.pl - KsiÄ…Å¼ka (nie) dla wszystkich:http://www.niepelnosprawni.pl/ledge/x/119055\r\nğŸ’¬ Mateusz RÃ³Å¼aÅ„ski/NiepeÅ‚nosprawni.pl - Nie widzÄ™, nie sÅ‚yszÄ™ â€“ oglÄ…dam:http://www.niepelnosprawni.pl/ledge/x/1329924 \r\nMateriaÅ‚y w jÄ™zyku angielskim:\r\nğŸ’¬ Top Tips for Creating Accessible EPUB 3 Files:http://diagramcenter.org/54-9-tips-for-creating-accessible-epub-3-files.htmlhttps://guides.cuny.edu/accessibility/epub\r\nğŸ’¬ Web Accessibility Evaluation Tools List:https://www.w3.org/WAI/ER/tools/  ğŸ’¬ EPUB Accessibility 1.1. Conformance and Discoverability Requirements for EPUB publications:https://www.w3.org/TR/epub-a11y-11/ \r\nğŸ’¬ Accessible Formats, Accessible Instructional Materials Center of Virginia (informacje m.in. o formatach BRF, DAISY, NIMAS):https://www.readingrockets.org/article/accessible-formats\r\nğŸ’¬ Understanding the Accessibility Differences in EPUB and PDF Reading Experiences (prezentacja do pobrania):https://accessinghigherground.org/understanding-the-accessibility-differences-in-epub-and-pdf-reading-experiences/\r\nğŸ’¬ EPUB Adoption in Academic Librariesâ€“Progress and Obstacles:https://inclusivepublishing.org/blog/epub-adoption-in-academic-libraries-progress-and-obstacles/\r\nğŸ’¬ Making digital journals and books more accessible with EPUB:https://insights.taylorandfrancis.com/social-justice/epub\r\nğŸ’¬ Use Calibre to improve ePub accessibility:https://kb.iu.edu/d/bgeh\r\nğŸ’¬ Accessible PDFs:https://www.adobe.com/accessibility/pdf/pdf-accessibility-overview.html https://helpx.adobe.com/pl/indesign/using/creating-accessible-pdfs.html \r\nAlternatywa online dla formatu EPUB w nauce, czyli PubReaderhttps://www.ncbi.nlm.nih.gov/pmc/about/pubreader/  (tutaj zarzut: jeÅ›li strona nie pozwala Å›ciÄ…gaÄ‡ plikÃ³w via PubReader, to bÄ™dziemy musieli zaufaÄ‡, Å¼e serwis bÄ™dzie zawsze dziaÅ‚aÄ‡ â€“ niestety strony naukowe bardzo czÄ™sto wypadajÄ… z obiegu, poniewaÅ¼ dana instytucja przestaÅ‚a utrzymywaÄ‡ serwer lub zmieniÅ‚y siÄ™ adresy odnoÅ›nikÃ³w. Serwisy online-only powinny byÄ‡ traktowane z ograniczonym zaufaniem):\r\nRÃ³Å¼ne deklaracje dostÄ™pnoÅ›ci:\r\nhttps://doc.bibliotekanauki.pl/pl/accessibility/\r\nhttps://www.power.gov.pl/deklaracja-dostepnosci/\r\nhttps://pad.widzialni.org/oswiadczenie-o-dostepnosci \r\nhttps://wcag.uj.edu.pl/lista/-/journal_content/56_INSTANCE_DuU7E3djxFwe/145608681/148186438\r\nhttps://www.uw.edu.pl/deklaracja-dostepnosci/ \r\nhttps://www.osw.waw.pl/pl/deklaracja-dostepnosci\r\nhttps://www.dwutygodnik.com/dostepnosc.html\r\nhttps://www.ebsco.com/pl-pl/technologia/dostepnosc \r\nhttps://support.proquest.com/s/article/Accessibility-Statement-Ebook-Central?language=en_US\r\nhttps://help.taylorfrancis.com/students_researchers/s/article/Accessibility-Statement\r\nArtykuÅ‚y: ğŸ“„ Arellano, J. (2021). Not open for all: Accessibility of open textbooks (Nr 1). 34(1), Art. 1. https://doi.org/10.1629/uksg.557\r\nğŸ“„ Bowes III, F. (2018). An overview of content accessibility issues experienced by educational publishers. Learned Publishing, 31(1), 35â€“38. https://doi.org/10.1002/leap.1145\r\nğŸ“„ Eikebrokk, T., Dahl, T. A., Kessel, S., Oslo, & Hanssen, A. U. C. of A. S. with thanks to: E. (2014). EPUB as Publication Format in Open Access Journals: Tools and Workflow. The Code4Lib Journal, 24. https://journal.code4lib.org/articles/9462\r\nâ€” krÃ³tkie omÃ³wienie artykuÅ‚u po polsku: https://babin.bn.org.pl/?p=2812\r\nğŸ“„ Suzuki, M., & Yamaguchi, K. (2020). On Automatic Conversion from E-born PDF into Accessible EPUB3 and Audio-Embedded HTML5. W K. Miesenberger, R. Manduchi, M. Covarrubias Rodriguez, & P. PeÅˆÃ¡z (Red.), Computers Helping People with Special Needs (s. 410â€“416). Springer International Publishing. https://doi.org/10.1007/978-3-030-58796-3_48 \r\nğŸ“„ Wang, L. L., Cachola, I., Bragg, J., Cheng, E. (Yu-Y., Haupt, C. H., Latzke, M., Kuehl, B., Zuylen, M. van, Wagner, L. M., & Weld, D. S. (2021). Improving the Accessibility of Scientific Documents: Current State, User Needs, and a System Solution to Enhance Scientific PDF Accessibility for Blind and Low Vision Users. ArXiv. https://www.semanticscholar.org/reader/56fbb4152b4535230827eb2bd23c618045855c26\r\nğŸ“„ Zhang, X., Tlili, A., Nascimbeni, F., Burgos, D., Huang, R., Chang, T.-W., Jemni, M., & Khribi, M. K. (2020). Accessibility within open educational resources and practices for disabled learners: A systematic literature review. Smart Learning Environments, 7. https://doi.org/10.1186/s40561-019-0113-2\r\n\r\nâ–â–â–\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-06-narzeczepub/pe.png",
    "last_modified": "2023-03-23T21:23:31+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-12-bayes-factor/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "CzÄ™Å›Ä‡ III: Czynnik Bayesowski",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2023-01-13",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWnioskowanie Statystyczne\r\nPorÃ³wnywanie modeli\r\nCzynnik Bayesowski\r\nCzynnik Bayesowski a zÅ‚oÅ¼onoÅ›Ä‡ modelu\r\nBayesian Point Null Hypothesis Testing\r\nSavage-Dickey density ratio\r\nLikelihood sampling\r\n\r\nPorÃ³wnywanie modeli?\r\nCzynnik Bayesa nie mierzy czy model jest prawdziwy\r\nCzynnik Bayesowski jest wraÅ¼liwy na rozkÅ‚ady a priori parametrÃ³w modelu\r\nRozkÅ‚ady posteriori parametrÃ³w nie muszÄ… siÄ™ zgadzaÄ‡ z czynnikiem Bayesa\r\nCzynnik Bayesa a prawdopodobieÅ„stwo braku efektu\r\nI co z tym wszystkim zrobiÄ‡?\r\n\r\n\r\nZakoÅ„czenie\r\n\r\n\r\n\r\n\r\nOsobiÅ›cie nienawidzÄ™ czynnikÃ³w Bayesaâ€¦\r\n\r\n\r\nâ€” Andrew Gelman, 2017\r\n\r\nWnioskowanie Statystyczne\r\nJak pamiÄ™tamy, wnioskowanie w statystyce czÄ™stoÅ›ciowej opiera siÄ™ na testowaniu hipotezy zerowej. HipotezÄ… zerowÄ… zwykle jest model, ktÃ³ry Å›wiadczy o braku efektu, na przykÅ‚ad, Å¼e rÃ³Å¼nica Å›rednich wynosi lub wspÃ³Å‚czynnik regresji wynosi 0. Im mniejsza p-value, tym bardziej otrzymane dane nie pasujÄ… do modelu zerowego. JeÅ›li jednak p-value jest wiÄ™ksza niÅ¼ 0.05 nie oznacza to, Å¼e model zerowy jest bardziej prawdopodobny niÅ¼ inne. Innymi sÅ‚owy, ten rodzaj testowania moÅ¼e nam jedynie powiedzieÄ‡ o wystÄ™powaniu rÃ³Å¼nicy Å›rednich czy wystÄ™powaniu korelacji, ale nie o jej braku.\r\nPrzypomnijmy sobie, Å¼e wielkoÅ›Ä‡ przedziaÅ‚u ufnoÅ›ci dla testu zaleÅ¼y od wariancji i wielkoÅ›ci prÃ³by. Gdyby NHST pozwalaÅ‚o nam orzekaÄ‡ o rÃ³wnoÅ›ci Å›rednich, udowodnienie takiej hipotezy byÅ‚oby niezwykle Å‚atwe.\r\nNa przykÅ‚ad zaÅ‚Ã³Å¼my, Å¼e chcemy sprawdziÄ‡ czy nowy lek nie rÃ³Å¼ni siÄ™ skutecznoÅ›ciÄ… od leku bÄ™dÄ…cego juÅ¼ na rynku. WystarczyÅ‚oby manipulowaÄ‡ wielkoÅ›ciÄ… prÃ³by, aby otrzymaÄ‡ wniosek o braku istotnej rÃ³Å¼nicy pomiÄ™dzy wpÅ‚ywem tych dwÃ³ch lekÃ³w!\r\nDlatego chcielibyÅ›my mÃ³c orzec, ktÃ³ry z modeli (hipotez) jest bardziej prawdopodobny. PodejÅ›cie Bayesowskie daje nam takÄ… moÅ¼liwoÅ›Ä‡ (choÄ‡, nie do koÅ„ca).\r\nPorÃ³wnywanie modeli\r\nCzym jest model? NajproÅ›ciej mÃ³wiÄ…c model jest zbiorem ograniczeÅ„, ktÃ³ry nakÅ‚adamy na proces generujÄ…cy dane. Na przykÅ‚ad moÅ¼e interesowaÄ‡ nas porÃ³wnanie dwÃ³ch rÃ³Å¼nych modeli regresji z innymi zbiorami predyktorÃ³w. PrzykÅ‚adowy model \\(M_i\\) posiada parametry \\(\\theta\\) i funkcjÄ™ wiarygodnoÅ›ci \\(P(D \\mid \\theta, M)\\). ZaÅ‚Ã³Å¼my, Å¼e chcielibyÅ›my obliczyÄ‡ prawdopodobieÅ„stwo a posteriori modelu \\(P(M \\mid D)\\).\r\n\\[P(M_i \\mid D) = \\frac{ P(D \\mid M_i) \\ P(M_i) }{P(D) = \\int P(D \\mid M) \\ P(M) \\  \\text{d} M}\\,\\]\r\nWidzimy, Å¼e analogicznie jak w poprzednich czÄ™Å›ciach tutorialu za pomocÄ… twierdzenia Bayesa, interesuje nas obliczenie prawdopodobieÅ„stwa modelu \\(M_i\\) pod warunkiem otrzymanych przez nas danych.\r\nPrawdopodobieÅ„stwo \\(P(M_1)\\) jest oczywiÅ›cie prawdopodobieÅ„stwem a priori naszego modelu. Sami je ustalamy, wiÄ™c nie ma z nim problemu.\r\nNasuwa siÄ™ jednak pytanie jak obliczyÄ‡ funkcjÄ™ wiarygodnoÅ›ci? \\(P(D|M_1)\\) oznacza marginalne prawdopodobieÅ„stwo (marginal likelihood) otrzymania danych pod warunkiem modelu dla wszystkich moÅ¼liwych wartoÅ›ci parametrÃ³w:\r\n\\[P(D \\mid M_i) = \\int P(D \\mid \\theta, M_i) \\ P(\\theta \\mid M_i) \\ \\text{d}\\theta\\]\r\nCo to znaczy dla wszystkich moÅ¼liwych parametrÃ³w? ZaÅ‚Ã³Å¼my, Å¼e mamy dwa modele regresji liniowej, rÃ³Å¼niÄ…ce siÄ™ tym, Å¼e jeden ma dodatkowy predyktor. Tak jak pisaÅ‚em wczeÅ›niej w statystyce Bayesowskiej nie otrzymamy punktowych estymat parametrÃ³w \\(\\theta\\), ale ich rozkÅ‚ady \\(P(\\theta|y)\\). PoniewaÅ¼ jednak chcemy otrzymaÄ‡ jednÄ… wartoÅ›Ä‡ - prawdopodobieÅ„stwo otrzymania danych wygenerowanych przez model \\(M_1\\), a nie przez jakieÅ› konkretne wartoÅ›ci parametrÃ³w tego modelu, musimy caÅ‚kowaÄ‡ po parametrach, by pozbyÄ‡ siÄ™ \\(\\theta\\) z rÃ³wnania.\r\nTeraz zerknijmy na mianownik naszego rÃ³wnania zawierajÄ…cy \\(P(D)\\). Tutaj sprawa siÄ™ komplikuje, poniewaÅ¼ by obliczyÄ‡ \\(P(D)\\) musielibyÅ›my caÅ‚kowaÄ‡ po wszystkich (nieskoÅ„czenie wielu) moÅ¼liwych modelach. Jego obliczenie jest w zasadzie niemoÅ¼liwe zarÃ³wno analitycznie (jak wyznaczyÄ‡ rozkÅ‚ad \\(P(M)\\) dla wszystkich moÅ¼liwych modeli), jak i numerycznie (poniewaÅ¼ musielibyÅ›my wymyÅ›liÄ‡ i policzyÄ‡ te wszystkie modele).\r\nA co jeÅ›li porÃ³wnamy dwa modele ze sobÄ…?\r\n\\[ \\underbrace{{\\frac{P(M_1|D)}{P(M_2|D)}}}_{\\text{stosunek a posteriori}} = \\frac{\\frac{P(D|M_1)P(M_1)}{P(D)}}{\\frac{P(D|M_2)P(M_2)}{P(D)}} = \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{{BF_{12}}} * \\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{stosunek a prior}}\\]\r\nRobiÄ…c tak, pozbywamy siÄ™ koniecznoÅ›ci obliczania \\(P(D)\\)!\r\nCzynnik Bayesowski\r\nJednak oceniajÄ…c modele nie uÅ¼ywamy stosunku prawdopodobieÅ„stw posteriori, lecz stosunku marginalnych wiarygodnoÅ›ci nazywanym czynnikiem Bayesowskim:\r\n\\[BF_{12} = \\frac{P(D|M_1)}{P(D|M_2)}\\]\r\nJest tak dlatego, Å¼e stosunek prawdopodobieÅ„stw a posteriori zaleÅ¼y od danych, ale takÅ¼e od prawdopodobieÅ„stw a priori modeli. ManipulujÄ…c nimi, moglibyÅ›my zawsze uzyskaÄ‡ miarÄ™ faworyzujÄ…cÄ… nasz model. UÅ¼ycie stosunku funkcji wiarygodnoÅ›ci, mÃ³wi nam o ile bardziej/mniej prawdopodobne jest, Å¼e \\(M_1\\) wyprodukowaÅ‚ obserwowane dane od \\(M_2\\). Ponadto jeÅ›li przepiszemy wzÃ³r na stosunek a posteriori, uzyskamy\r\n\\[ \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{BF_{12}} = \\underbrace{\\frac{P(M_1|D)}{P(M_2|D)}}_{\\text{stosunek posteriori }}:\\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{stosunek a priori}}\\]\r\nCo daje nam dodatkowÄ… interpretacjÄ™ czynnika Bayesowskiego. Jest on stosunkiem prawdopodobieÅ„stw posteriori podzielony przez stosunek prawdopodobieÅ„stw a priori naszych dwÃ³ch modeli. MÃ³wi nam o ile zmieniÅ‚y siÄ™ nasze przekonania a priori po zobaczeniu danych. Innymi sÅ‚owy, czynnik Bayesowski mÃ³wi nam jak zmieniÅ‚y siÄ™ pod wpÅ‚ywem danych nasze poczÄ…tkowe przekonania co do tego, ktÃ³ry model jest lepszy.\r\nZaÅ‚Ã³Å¼my, Å¼e przeprowadziliÅ›my analizÄ™ dwÃ³ch modeli i otrzymaliÅ›my \\(stosunek \\ posteriori = 4\\), faworyzujÄ…cy pierwszy model. Jednak \\(BF_{12} = 0.5\\), mÃ³wiÄ…c nam, Å¼e drugi model wyprodukowaÅ‚ obserwowane dane z dwa razy wiÄ™kszym prawdopodobieÅ„stwem. SkÄ…d taka rozbieÅ¼noÅ›Ä‡? Wynika ze tego, Å¼e \\(stosunek \\ a \\ priori = 8\\), od poczÄ…tku faworyzowaÅ‚ pierwszy model.\r\nZwrÃ³Ä‡my teÅ¼ uwagÄ™, Å¼e jeÅ›li przypiszemy modelom takie same prawdopodobieÅ„stwa a priori, stosunek a posteriori bÄ™dzie siÄ™ rÃ³wnaÅ‚ czynnikowi Bayesa.\r\nCzynnik Bayesowski a zÅ‚oÅ¼onoÅ›Ä‡ modelu\r\nCzynnik Bayesowski w naturalny sposÃ³b karze za zÅ‚oÅ¼onoÅ›Ä‡ modelu, choÄ‡ ta wÅ‚aÅ›ciwoÅ›Ä‡ nie jest widoczna na pierwszy rzut oka. Å»eby jÄ… zwizualizowaÄ‡, wyobraÅºmy sobie, Å¼e \\(P(D|M)\\) nie jest pojedyÅ„czÄ… wartoÅ›ciÄ… prawdopodobieÅ„stwa otrzymania zaobserwowanych danych, lecz rozkÅ‚adem prawdopodobieÅ„stwa wygenerowania wszystkich moÅ¼liwych zbiorÃ³w danych, przez model. WeÅºmy dwa takie rozkÅ‚ady dla modeli: prostszego modelu - \\(M_1\\) i bardziej zÅ‚oÅ¼onego \\(M_2\\). Bardziej zÅ‚oÅ¼ony model jest w stanie wygenerowaÄ‡ wiÄ™cej moÅ¼liwych zbiorÃ³w danych niÅ¼ prostszy, poniewaÅ¼ jest bardziej elastyczny.\r\n\r\nCaÅ‚ka prawdopodobieÅ„stw (pole pod wykresem) kaÅ¼dego z rozkÅ‚adÃ³w musi wynosiÄ‡ 1 (z definicji rozkÅ‚adÃ³w prawdopodobieÅ„stwa). Oznacza to, Å¼e dla przedziaÅ‚u \\(C_1\\) model \\(M_1\\) bÄ™dzie miaÅ‚ zawsze wiÄ™ksze prawdopodobieÅ„stwo wygenerowania danych niÅ¼ \\(M_2\\), w konsekwencji czynnik Bayesa bÄ™dzie faworyzowaÅ‚ \\(M_1\\). Logika stojÄ…ca za taka wÅ‚asnoÅ›ciÄ… \\(BF\\) jest iÅ›cie Occamowska. Zadajmy sobie pytanie, ktÃ³ry z modeli ma wiÄ™ksze prawdopodobieÅ„stwo wygenerowania danych w przedziale \\(C_1\\) jeÅ›li otrzymane przez nas dane mogÄ… byÄ‡ wyjaÅ›nione przez oba modele. Ten, ktÃ³ry czÄ™Å›ciej bÄ™dzie generowaÅ‚ zbiory danych zawierajÄ…ce siÄ™ w przedziale \\(C_1\\), czyli \\(M_1\\).\r\nBayesian Point Null Hypothesis Testing\r\nPrzypomnijmy sobie NHST (Null Hypothesis Significance Testing), ktÃ³re testuje hipotezy zerowÄ… i alternatywnÄ…. Hipoteza zerowa zwykle oznacza, Å¼e jakiÅ› parametr \\(\\delta = 0\\), a hipoteza alternatywna, Å¼e \\(\\delta \\neq 0\\). czynnik Bayesa w naukach spoÅ‚ecznych jest czÄ™sto wykorzystywany podobnym celu. Nazywa siÄ™ to Bayesian Point Null Hypothesis Testing (BPNHT). Testujemy model, ktÃ³ry zakÅ‚ada, Å¼e \\(\\delta\\) moÅ¼e przyjÄ…Ä‡ dowolnÄ… wartoÅ›Ä‡, wzglÄ™dem takiego, w ktÃ³rym jego wartoÅ›Ä‡ jest zerowa. Takie podobieÅ„stwo pomiÄ™dzy tymi metodami nie jest przypadkowe i obie metody dzielÄ… pewne problemy, o czym wkrÃ³tce.\r\nJako przykÅ‚ad posÅ‚uÅ¼ymy siÄ™ czynnikiem Bayesa do przeprowadzenia Bayesowskiej wersji jednoczynnikowej analizy wariancji. Wygenerujmy sobie dane:\r\n\r\n\r\nset.seed(123)\r\nfactor_a = sample(c(rep(0,50),rep(1,50)))\r\ny = rnorm(100, 3 + 0.80*factor_a, 1)\r\ndata = data.frame(id = 1:50, y = y, factor_a = factor_a)\r\n\r\n\r\nCi z Was, ktÃ³rzy sÄ… zaznajomieni z klasycznÄ… wersjÄ… analizy wariancji wiedzÄ…, Å¼e Anova to specjalny przypadek regresji liniowej. Jest teÅ¼ idealnym przykÅ‚adem do uÅ¼ycia czynnika Bayesowskiego w praktyce, poniewaÅ¼ polega na porÃ³wnaniu modeli. Najprostszym modelem, naszym modelem zerowym, bÄ™dzie model, ktÃ³ry zawiera tylko staÅ‚Ä…. Modelem bardziej skomplikowanym, ktÃ³ry chcemy przetestowaÄ‡ jest model zawierajÄ…cy dodatkowo wspÃ³Å‚czynnik regresji dla czynnika a:\r\n\\[\\begin{eqnarray*}\r\n&  &\r\nM_0: \\ \\ y = \\alpha\\\\\r\n&  &\r\nM_1: \\ \\ y = \\alpha + \\beta*X_a \\\\\r\n&  &\r\n\\end{eqnarray*}\\]\r\nJeÅ›li \\(y\\) nie jest powiÄ…zany z czynnikiem \\(X_a\\), czynnik Bayesa powinien faworyzowaÄ‡ model zerowy.\r\nSavage-Dickey density ratio\r\nDla pewnych modeli, jeÅ›li wybierzemy odpowiednie rozkÅ‚ady a priori, moÅ¼emy obliczyÄ‡ \\(BF\\) analitycznie. W przypadku bardziej skomplikowanych zdani jesteÅ›my na metody numeryczne. Niestety, dla wielu modeli estymacja \\(BF\\) jest trudna i nieÅ‚atwa do zastosowania w praktyce.\r\nIstnieje jednak prosty sposÃ³b na obliczenie czynnika Bayesowskiego dla modeli zagnieÅ¼dÅ¼onych (nested models), czyli takich, w ktÃ³rych jeden model moÅ¼emy traktowaÄ‡ jako rozszerzenie drugiego. Popatrzmy na \\(M_0\\) i \\(M_1\\). MoÅ¼emy przeformuÅ‚owaÄ‡ \\(M_0\\) w nastÄ™pujÄ…cy sposÃ³b:\r\n\\[M_0: \\ \\ y = \\alpha + \\beta*X_a, \\  \\ \\beta = 0\\] Teraz widzimy, Å¼e \\(M_0\\) jest zagnieÅ¼dÅ¼ony w \\(M_1\\), to znaczy jest modelem \\(M_1\\), ktÃ³rego wartoÅ›Ä‡ parametru \\(\\beta\\) zostaÅ‚a ustawiona jako staÅ‚a.\r\nGeneralnie, majÄ…c dwa modele z parametrami \\(\\varphi\\) i \\(\\delta\\), takie, Å¼e \\(M_0:\\delta=\\delta_0,\\varphi\\) i \\(M_1:\\delta,\\varphi\\), ktÃ³re speÅ‚niajÄ… warunek \\(p(\\varphi\\mid M_0) = p(\\varphi\\mid \\delta=\\delta_0,M_1)\\), moÅ¼emy obliczyÄ‡ \\(BF_{01}\\) w nastÄ™pujÄ…cy sposÃ³b:\r\n\\[\\text{BF}_{01} = \\frac{p(\\delta=\\delta_0\\mid y,M_1)}{p(\\delta=\\delta_0\\mid M_1)}\\]\r\nDowÃ³d\r\nZ definicji:\r\n\\[\\begin{equation}\\label{eq:bf}\\text{BF}_{01}=\\frac{p(y \\mid M_0)}{p(y \\mid M_1)}.\\end{equation}\\]\r\nMoÅ¼emy przeksztaÅ‚ciÄ‡ \\(p(y \\mid M_0)\\) w: \\[\\begin{equation}\r\n\\begin{split}\r\n p(y \\mid M_0) &= \\int p(y \\mid \\varphi,M_0) \\, p(\\varphi\\mid M_0) \\, \\mathrm{d} \\varphi \\\\\r\n  &= \\int p(y \\mid \\varphi,\\delta=\\delta_0,M_1) \\, p(\\varphi\\mid \\delta=\\delta_0,M_1) \\, \\mathrm{d} \\varphi \\\\\r\n  &= p(y \\mid \\delta=\\delta_0,M_1).\\\\\r\n\\end{split}\r\n\\label{eq:ml-m0}\r\n\\end{equation}\\]\r\nStosujÄ…c twierdzenie Bayesa do ostatniego wyniku:\r\n\\[\\begin{equation}\\label{eq:ml-bt}\r\np(y \\mid \\delta=\\delta_0,M_1) = \\frac{p(\\delta=\\delta_0\\mid y,M_1) \\, p(y \\mid M_1)}{p(\\delta=\\delta_0\\mid M_1)}\\end{equation}\\]\r\nWiÄ™c:\r\n\\[\\begin{equation}\r\n\\begin{split}\r\n  \\text{BF}_{01} &\\overset{\\eqref{eq:bf}}{=} \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}\\\\\r\n  &= p(y \\mid M_0) \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &\\overset{\\eqref{eq:ml-m0}}{=} p(y \\mid \\delta=\\delta_0,M_1) \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &\\overset{\\eqref{eq:ml-bt}}{=} \\frac{p(\\delta=\\delta_0\\mid y,M_1) \\, p(y \\mid M_1)}{p(\\delta=\\delta_0\\mid M_1)} \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &= \\frac{p(\\delta=\\delta_0 \\mid y,M_1)}{p(\\delta=\\delta_0\\mid M_1)},\r\n\\end{split}\r\n\\end{equation}\\]\r\nObliczmy sobie \\(BF_{10}\\) posÅ‚ugujÄ…c siÄ™ numerycznym przybliÅ¼eniem stosunku Savage-Dickey. By to zrobiÄ‡ musimy najpierw zdefiniowaÄ‡ nasz \\(M_1\\). Przyjmijmy, Å¼e rozkÅ‚ady a priori zarÃ³wno \\(\\alpha\\) jak i \\(\\beta\\) to:\r\n\\[ \\alpha, \\beta \\sim N(0,10)\\] Wobec tego mianownik stosunku Savage-Dickey, bÄ™dzie gÄ™stoÅ›ciÄ… prawdopodobieÅ„stwa wylosowania 0 z powyÅ¼szego rozkÅ‚adu.\r\n\r\n\r\nprior_beta_0 = dnorm(0,0,10)\r\n\r\n\r\nNasz model w JAGS (pisaÅ‚em o nim tutaj) bÄ™dzie wyglÄ…daÅ‚ tak:\r\n\r\n\r\nm1 = '\r\nmodel{\r\nfor(i in 1:length(y)){\r\ny[i] ~ dnorm(mu[i], precision)\r\nmu[i] = alpha + beta*factor_a[i]\r\n}\r\nprecision ~ dunif(0.0000001,100)\r\nalpha ~ dnorm(0,10^-2)\r\nbeta ~ dnorm(0,10^-2)\r\n}'\r\n\r\n\r\nWyprodukujmy rozkÅ‚ady posteriori parametru \\(\\beta\\).\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rjags)\r\nlibrary(ggmcmc)\r\nlibrary(polspline)\r\n\r\n# Parametry, ktÃ³re chcemy Å›ledziÄ‡. \r\nparams = c(\"beta\")\r\n# Inicjacja modelu\r\njmod1 = jags.model(file = textConnection(m1), data = data, n.chains = 4, inits = NULL, n.adapt = 100)\r\n# Wypalanie\r\nupdate(jmod1, n.iter=100^2, by=1)\r\n# Losowanie prÃ³bek z rozkÅ‚adu posteriori\r\npost = coda.samples(jmod1, params, n.iter = 10*100^2, thin = 1)\r\n# tidy format\r\nmodel1 = ggs(post)\r\n\r\n\r\nTeraz wyestymujemy prawdopodobieÅ„stwo wylosowania \\(\\beta = 0\\) z rozkÅ‚adu posteriori.\r\n\r\n\r\npost_samples = filter(model1, Parameter == \"beta\")$value\r\nfit.posterior <- logspline(post_samples)\r\nposterior_beta_0 <- dlogspline(0, fit.posterior)\r\n\r\n\r\nMamy juÅ¼ wszystko by policzyÄ‡ \\(BF_{01}\\). Jednak poniewaÅ¼ \\(M_0\\) jest naszym modelem zerowym, popatrzmy na \\(BF_{10}\\) czyli \\(\\frac{1}{BF_{01}}\\).\r\nNasz \\(BF_{10}\\) wynosi\r\n\r\n\r\n1/(posterior_beta_0/prior_beta_0)\r\n\r\n[1] 3.913636\r\n\r\nOtrzymaliÅ›my wartoÅ›Ä‡ \\(BF_{10}\\) wskazujÄ…cÄ… to, Å¼e porÃ³wnujÄ…c te dwa modele, jest ca. 4 razy bardziej prawdopodobne, Å¼e \\(M_1\\) wyprodukowaÅ‚ obserwowane dane niÅ¼ \\(M_0\\). DuÅ¼o? MaÅ‚o? Jedna z propozycji interpretacji \\(BF\\) jest nastÄ™pujÄ…ca (Kass & Raftery, 1995):\r\n\r\nBF\r\nDowody\r\n< 1\r\nWspierajÄ…ce model zerowy\r\n1-3\r\nAnegdotyczne\r\n3-10\r\nZnaczne\r\n10-100\r\nSilne\r\n> 100\r\nDecyzyjne\r\n\r\nLikelihood sampling\r\nW przypadku gdy nasze modele nie sÄ… zagnieÅ¼dÅ¼one, musimy szukaÄ‡ innych metod. JednÄ… z nich jest naiwne symulowanie metodÄ… Monte Carlo. Opiera siÄ™ na pomyÅ›le, Å¼e \\(P(Y|M_i)\\) moÅ¼emy przybliÅ¼yÄ‡ w nastÄ™pujÄ…cy sposÃ³b (Gronau et al., 2017):\r\n\\[P(D \\mid M_i) = \\int P(D \\mid \\theta, M_i) \\ P(\\theta, M_i) \\ \\text{d}\\theta \\approx \\frac{1}{n} \\sum^{n}_{\\theta_i \\sim P(\\theta \\mid M_i)} P(D \\mid \\theta, M_i)\\] Popatrzmy na naszÄ… caÅ‚kÄ™. JeÅ›li potraktujemy \\(P(D \\mid \\theta, M_i)\\) jako zmiennÄ… \\(x\\), a \\(P(\\theta, M_i)\\) jako zwiÄ…zanÄ… z \\(x\\) funkcjÄ™ prawdopodobieÅ„stwa \\(P(X)\\) (ktÃ³rÄ… w istocie jest), otrzymamy:\r\n\\[P(D \\mid M_i) = \\int x \\ P(x) \\ \\text{dx}\\]\r\nCzyli wzÃ³r na Å›redniÄ…. MoÅ¼emy wiÄ™c przybliÅ¼yÄ‡ wartoÅ›Ä‡ \\(P(D|M_i)\\) losujÄ…c najpierw \\(\\theta\\) z rozkÅ‚adu a priori, nastÄ™pnie obliczajÄ…c \\(P(D \\mid \\theta, M_i)\\) poprzez wstawienie uzyskanej \\(\\theta\\). JeÅ›li powtÃ³rzymy to wielokrotnie, a otrzymane prawdopodobieÅ„stwa uÅ›redniamy, uzyskamy estymatÄ™ \\(P(D \\mid M_i)\\). Takie losowanie rÃ³wnieÅ¼ moÅ¼emy wykonaÄ‡ przy pomocy JAGS, jednak uzyskany \\(BF\\) nie jest zbyt stabilny. Dlatego stosuje siÄ™ udoskonalone metody losowania takie jak np. Bridge Sampling. Nie bÄ™dziemy jednak ich teraz dokÅ‚adnie omawiaÄ‡.\r\nPorÃ³wnywanie modeli?\r\nCzynnik Bayesa wydaje siÄ™ byÄ‡ niezÅ‚Ä… metodÄ…, ktÃ³rÄ… moÅ¼na wykorzystaÄ‡ do wnioskowania statystycznego. ByÄ‡ moÅ¼e nawet pomoÅ¼e nam on rozwiÄ…zaÄ‡ kryzys replikacyjny. Tak teÅ¼ pomyÅ›laÅ‚o wielu, co skutkowaÅ‚o tym, Å¼e programy statystyczne takie jak np. JASP zwracajÄ… nam podsumowania Bayesowskich analiz defaultowo w postaci czynnika Bayesa. RÃ³wnieÅ¼ czasopisma naukowe czÄ™sto oczekujÄ… od autorÃ³w, jeÅ›li Ci posÅ‚ugujÄ… siÄ™ statystykÄ… BayesowskÄ…, wynikÃ³w w postaci czynnikÃ³w Bayesa.\r\nNiektÃ³re czasopisma wrÄ™cz wymagajÄ… czynnikÃ³w Bayesa jako metody przeciwdziaÅ‚ajÄ…cej kryzysowi replikacyjnemu (pisaÅ‚em o kryzysie replikacyjnym trochÄ™ tutaj). Skoro okreÅ›lono algorytm weryfikacji hipotez statystycznych w statystyce Bayesowskiej (rzekomo skuteczniejszy od statystyki czÄ™stosciowej), moÅ¼emy wreszcie wrÃ³ciÄ‡ do pracy, nie martwiÄ…c siÄ™ o problem replikacji.\r\nNiestety, jak to siÄ™ zwykle okazuje, rzeczywistoÅ›Ä‡ nie jest taka rÃ³Å¼owa. Czynnik Bayesa jest kontrowersyjny wÅ›rÃ³d statystykÃ³w. RÃ³Å¼nice w podejÅ›ciach moÅ¼emy zauwaÅ¼yÄ‡ na przykÅ‚ad w zaciÄ™tych dysputach Davida Mackaya z Adrew Gelmanem. Mackay napisaÅ‚ doktorat wykazujÄ…cy wÅ‚aÅ›ciwoÅ›Ä‡ czynnika Bayesa do karania skomplikowanych modeli, ktÃ³rÄ… omÃ³wiliÅ›my wczeÅ›niej. Andrew Gelman i inni wykazali wiele niedociÄ…gniÄ™Ä‡ i problemÃ³w zwiÄ…zanych z czynnikiem Bayesa zwÅ‚aszcza w kontekÅ›cie Bayesian Point Null Hypothesis Testing.\r\nGrafika skradziona ze slajdÃ³w Richard E. TurneraRzuÄ‡my okiem na kilka najwaÅ¼niejszych zastrzeÅ¼eÅ„. To, co waÅ¼ne, to fakt nie wszystkie z nich sÄ… na pierwszy rzut oka oczywiste.\r\nCzynnik Bayesa nie mierzy czy model jest prawdziwy\r\nCzynnik Bayesowski jest stosunkiem marginalnej wiarygodnoÅ›ci dwÃ³ch modeli, czyli o ile bardziej jest prawdopodobne, Å¼e jeden z modeli wyprodukowaÅ‚ obserwowane dane niÅ¼ drugi. Nie mÃ³wi nam on w Å¼aden sposÃ³b o tym, jak dobre te modele w absolutnym sensie (Gelman & Rubin, 1995). MoÅ¼emy byÄ‡ w sytuacji, w ktÃ³rej czynnik Bayesa bardzo faworyzuje jeden z modeli, ale oba modele sÄ… fatalnie dopasowane do danych.\r\nW konsekwencji jeÅ›li stosujemy sposÃ³b wnioskowania oparty na testowaniu hipotezy zerowej i alternatywnej (tak jak zrobiliÅ›my wyÅ¼ej w przypadku Anovy), zasadniczo czynnik Bayesa nie mÃ³wi nam wiÄ™cej niÅ¼ p-value bez spojrzenia na wielkoÅ›Ä‡ efektu. By zbadaÄ‡ dopasowanie modelu potrzebujemy wiÄ™c przeanalizowaÄ‡ rozkÅ‚ady posteriori parametrÃ³w.\r\nCzynnik Bayesowski jest wraÅ¼liwy na rozkÅ‚ady a priori parametrÃ³w modelu\r\nRozkÅ‚ad posteriori parametrÃ³w \\(\\theta\\) jest niezaleÅ¼ny od rozkÅ‚adÃ³w a prior tychÅ¼e, gdy liczba obserwacji dÄ…Å¼y do nieskoÅ„czonoÅ›ci.\r\n\\[\\lim_{n\\to\\infty}P(\\theta|D,M) \\propto P(D|\\theta, M)\\]\r\nTak jak pokazaliÅ›my w pierwszej czÄ™Å›ci tutorialu, gdy liczba obserwacji wzrasta, wpÅ‚yw rozkÅ‚adÃ³w a priori maleje. Jak jest jednak w przypadku czynnika Bayesa? PrzyjmujÄ…c pewne upraszczajÄ…ce zaÅ‚oÅ¼enia (MacKay, 2003), moÅ¼emy powiedzieÄ‡, Å¼e marginalna wiarygodnoÅ›Ä‡ to:\r\n\\[P(D|M) \\approx P(D|\\theta_{map}, M)P(\\theta_{map}|M)\\sigma_{\\theta|D}\\] Gdzie \\(\\theta_{map} = \\arg \\max_{\\theta} P(\\theta|D,M)\\) to maximum posteriori approximation, czyli takie \\(\\theta\\), dla ktÃ³rego prawdopodobieÅ„stwo a posteriori jest najwiÄ™ksze. Z kolei \\(\\sigma_{\\theta|D}\\) to odchylenie standardowe \\(P(\\theta|D,M)\\).\r\nW konsekwencji czynnik Bayesowski zaleÅ¼y od a priori rozkÅ‚adÃ³w parametru modelu \\(P(\\theta_{map}|M)\\). W przypadku gdy uÅ¼yjemy nieinformatywnego rozkÅ‚adu a priori dla \\(\\theta\\), rozkÅ‚adu jednostajnego o szerokoÅ›ci \\(\\sigma_{\\theta}\\), wtedy wzÃ³r upraszcza siÄ™ do\r\n\\[P(D|M) \\approx P(D|\\theta_{map}, M)\\frac{\\sigma_{\\theta|D}}{\\sigma_{\\theta}}\\]\r\nIm bardziej chcemy by dane przemÃ³wiÅ‚y za siebie (im bardziej nieinformatywny rozkÅ‚ad a priori zastosujemy), tym bardziej faworyzowany bÄ™dzie model zerowy w przypadku Bayesian Point Null Hypothesis Testing. WraÅ¼liwoÅ›Ä‡ na rozkÅ‚ady a priori nie jest tak istotnym problemem, gdy porÃ³wnujemy ze sobÄ… niezagnieÅ¼dÅ¼one modele to znaczy w przypadku, gdy testujemy dwa alternatywne wyjaÅ›nienia zjawiska.\r\nRozkÅ‚ady posteriori parametrÃ³w nie muszÄ… siÄ™ zgadzaÄ‡ z czynnikiem Bayesa\r\nW statystyce czÄ™stoÅ›ciowej p-value < 0.05 dla parametru oznacza odrzucenie hipotezy zerowej. Jednak w przypadku analizy Bayesowskiej moÅ¼emy spotkaÄ‡ siÄ™ z sytuacjami, w ktÃ³rych czynnik Bayesa sugeruje przyjÄ™cie modelu zerowego, jednak interesujÄ…cy nas parametr w modelu alternatywnym nie zawiera w 95% przedziale wiarygodnoÅ›ci, i vice versa (Kruschke & Liddell, 2018). Ta wÅ‚asnoÅ›Ä‡ czynnika Bayesa jest czÄ™sto nieintuicyjna, poniewaÅ¼ przenosimy nasze oczekiwania ze statystyki czÄ™stoÅ›ciowej.\r\nCzynnik Bayesa a prawdopodobieÅ„stwo braku efektu\r\nCzÄ™sto siÄ™ mÃ³wi, Å¼e by wnioskowaÄ‡ o braku efektu musimy uÅ¼yÄ‡ statystyki Bayesowskiej, poniewaÅ¼ w statystyce klasycznej moÅ¼emy mÃ³wiÄ‡ co najwyÅ¼ej o braku dowodÃ³w na istnienie efektu. Niestety, przypadku BPNHT, pojawia siÄ™ pewien problem, poniewaÅ¼ czynnik Bayesa jest wraÅ¼liwy na wielkoÅ›Ä‡ prÃ³by (Morey & Rouder, 2011).\r\nBy zobaczyÄ‡ to na wÅ‚asne oczy posÅ‚uÅ¼ymy siÄ™ przykÅ‚adem. ZaÅ‚Ã³Å¼my, Å¼e zebraliÅ›my dane, ktÃ³re majÄ… rozkÅ‚ad normalny \\(y \\sim N(\\mu,\\sigma)\\), ze znanym odchyleniem standardowym \\(\\sigma = 5\\). Chcemy przetestowaÄ‡ dwa modele w myÅ›l BPNHT: \\(M_0: \\mu = 0\\) i \\(M_1: \\mu \\sim U(-\\infty,+\\infty)\\). Czyli model zakÅ‚adajÄ…cy, Å¼e wartoÅ›Ä‡ \\(\\mu = 0\\) i model, ktÃ³ry zakada, Å¼e kaÅ¼da wartoÅ›Ä‡ \\(\\mu\\) jest rÃ³wnie prawdopodobna.\r\nDokÅ‚adny czynnik Bayesa dla tych dwÃ³ch modeli dany jest wzorem1:\r\n\\[BF_{10} = \\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}e^{(\\frac{\\overline{\\text{y}}^2n}{2\\sigma})}\\] Gdzie \\(\\overline{\\text{y}}\\) to Å›rednia w prÃ³bie. JuÅ¼ we wzorze moÅ¼emy zobaczyÄ‡, Å¼e ta wartoÅ›Ä‡ bÄ™dzie coraz wiÄ™ksza, gdy \\(n\\) roÅ›nie. Zobaczmy na wykresie jak wyglÄ…dajÄ… wartoÅ›ci czynnika Bayesa dla ustalonych wartoÅ›ci \\(\\overline{\\text{y}}\\):\r\n\r\n\r\nBF = function(sample_size,sample_mean, sd){\r\n  (sqrt(sample_size)/(sqrt(2*pi)*sd))*exp(((sample_mean^2)*sample_size)/(2*sd^2))\r\n}\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x))+ \r\n  stat_function(fun = BF,args = list(sample_mean = 0.6,sd = 5), aes(color = \"0.6\")) + \r\n  stat_function(fun = BF,args = list(sample_mean = 0.75,sd = 5), aes(color = \"0.75\")) +\r\n  stat_function(fun = BF,args = list(sample_mean = 0.9,sd = 5), aes(color = \"0.9\")) +\r\n  xlim(0,100) +\r\n  xlab(\"N\") + \r\n  ylab(bquote(BF[10])) + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"Sample Mean\", values = c( \"blue\", \"green\", \"red\")) +\r\n  geom_hline(yintercept = 1) +\r\n  theme_light()\r\n\r\n\r\n\r\nOk, im wiÄ™ksza prÃ³ba, tym wiÄ™ksze wsparcie dla hipotezy alternatywnej. ChoÄ‡ spojrzeliÅ›my tylko na dwa partykularne modele, ta wÅ‚asnoÅ›Ä‡ utrzymuje siÄ™ generalnie w caÅ‚ym BPNHT (Johnson & Rossell, 2010). Czy to Åºle? Niekoniecznie, ale to znaczy, Å¼e podobnie jak w przypadku p-value, tym mniejszy efekt jest potrzebny by odrzuciÄ‡ hipotezÄ™ zerowÄ…, im wiÄ™ksza prÃ³ba. Dlatego teÅ¼ ciÄ™Å¼ko jest wnioskowaÄ‡ o braku efektu, poniewaÅ¼ nie wiemy czy czynnik Bayesa faworyzuje model zerowy, poniewaÅ¼ jest lepszy, czy dlatego, Å¼e nie ma wystarczajÄ…cych dowodÃ³w by go odrzuciÄ‡.\r\nNie oznacza jednak, Å¼e statystyka Bayesowska nam na to nie pozwala. JeÅ›li rozkÅ‚ad posteriori parametru jest silnie skoncentrowany blisko zera, moÅ¼emy mÃ³wiÄ‡, Å¼e pod warunkiem danych mamy duÅ¼Ä… pewnoÅ›Ä‡ braku efektu lub marginalnego efektu.\r\nI co z tym wszystkim zrobiÄ‡?\r\nDogÅ‚Ä™bne omÃ³wienie wyÅ¼ej wymienionych problemÃ³w moÅ¼ecie znaleÅºÄ‡ w artykule â€œA Review of Issues About Null Hypothesis Bayesian Testingâ€ (Tendeiro & Kiers, 2019).\r\nAlternatywna rzeczywistoÅ›Ä‡, w ktÃ³rej, w miejsce statystyki czÄ™stoÅ›ciowej, statystyka Bayesowska zdominowaÅ‚a naukÄ™. Grafika skradziona ze slajdÃ³w Richarda Moreya.Z jednej strony czynnik Bayesa pozwala na bezpoÅ›rednie porÃ³wnanie prawdopodobieÅ„stwa wyprodukowania danych przez caÅ‚y model, a nie tylko punktowe wartoÅ›ci parametrÃ³w. JednoczeÅ›nie w naturalny sposÃ³b karze zÅ‚oÅ¼onoÅ›Ä‡ modelu, co w innych miarach dopasowania modelu takich jak na przykÅ‚ad AIC, prÃ³bujemy robiÄ‡ estymujÄ…c zÅ‚oÅ¼onoÅ›Ä‡ poprzez liczbÄ™ wolnych parametrÃ³w.\r\nJednakÅ¼e, jak widzimy, czynnik Bayesa jest miarÄ…, ktÃ³ra rÃ³wnieÅ¼ sÅ‚abe strony, nie jest wiÄ™c cudownym lekiem na statystyczne bolÄ…czki. Z tego miejsca mamy dwie moÅ¼liwoÅ›ci. MoÅ¼emy uÅ¼ywaÄ‡ czynnika Bayesa rozwaÅ¼nie, czyli:\r\nOstroÅ¼nie dobieraÄ‡ rozkÅ‚ady a priori parametrÃ³w modelu.\r\nPrzeprowadzaÄ‡ analizÄ™ wraÅ¼liwoÅ›Ä‡ (sensitivity analysis), tzn. sprawdziÄ‡ jak bardzo wartoÅ›Ä‡ czynnika Bayesa ulega zmianie, gdy uÅ¼yjemy innych rozkÅ‚adÃ³w a priori parametrÃ³w. Niestety, konsekwencjÄ… tego, Å¼e czynnik Bayesa jest trudny do policzenia, jest to Å¼mudny proces.\r\nNie raportowaÄ‡ czynnika Bayesa samotnie, ale takÅ¼e miary rozkÅ‚adu parametrÃ³w a posteriori.\r\nAlbo\r\nunikaÄ‡ porÃ³wnywania modeli. Zamiast tego, na pierwszym miejscu skupiÄ‡ siÄ™ na tworzeniu wiarygodnych modeli wspieranych przez teoriÄ™, przeÅ‚oÅ¼yÄ‡ nacisk na estymacjÄ™ parametrÃ³w, tzn. zamiast porÃ³wnywaÄ‡ interesujÄ…cy nas model do modelu zerowego z wartoÅ›ciÄ… parametru rÃ³wnÄ… 0, skupiÄ‡ siÄ™ na ewaluacji rozkÅ‚adu a posteriori parametrÃ³w. WykorzystaÄ‡ Posterior Predictive Testsing, ktÃ³re moÅ¼e nam wskazaÄ‡ problemy z naszym modelem, bez koniecznoÅ›ci porÃ³wnywania go z innym.\r\nUnikaÄ‡ testowania przy pomocy BPNHT zasadnoÅ›ci pojedynczego modelu. PrÃ³wnywanie modeli jest natomiast jak najbardziej przydatne wtedy, gdy mamy dwa lub wiÄ™cej (np. podyktowane teoriÄ…) konkurujÄ…ce modele inaczej ujmujÄ…ce mechanizm jakiegoÅ› zajawiska np. pamiÄ™ci. Co istotne, do tego moÅ¼emy posÅ‚uÅ¼yÄ‡ siÄ™ statystykami prostszymi do obliczenia, jak np. DIC albo WAIC.\r\nZakoÅ„czenie\r\nDowiedzieliÅ›my siÄ™ czym jest czynnik Bayesowski i przy okazji omÃ³wiliÅ›my kontrowersje wokÃ³Å‚ niego. Mamy juÅ¼ intuicjÄ™ na temat statystyki Bayesowskiej, estymacji modeli i wnioskowania. Fajnie, by byÅ‚o wreszcie coÅ› policzyÄ‡ po Bayesowsku co nie jest prostym modelem liniowym. Dlatego w nastÄ™pnej czÄ™Å›ci przyjrzymy siÄ™ jednej z ciekawszych moÅ¼liwoÅ›ci statystyki Bayesowskiej - modelom hierarchicznym.\r\nNa koniec dodam, Å¼e czynnik Bayesowski, p-value ani Å¼adna inna statystyka nie jest rozwiÄ…zaniem kryzysu replikacyjnego. Nie dlatego, Å¼e te miary obarczone sÄ… wadami, tylko dlatego, Å¼e prawdziwe powody leÅ¼Ä… gdzie indziej. Badacze czasami naiwnie stosujÄ… statystykÄ™, czasami recenzenci wymagajÄ… od badaczy stosowania utartych, choÄ‡ nieadekwatnych procedur. Dodatkowo system ewaluacji pracownikÃ³w naukowych wymaga od nich by publikowali duÅ¼o, a czasopisma naukowe wymagajÄ… by publikowali badania Å›wiadczÄ…ce o istnieniu efektu, raczej niÅ¼ o jego braku. Bez zaadresowania tych problemÃ³w, nie sÄ…dzÄ™ by jakakolwiek statystyczny test rozwiÄ…zaÅ‚ problem replikacji w nauce.\r\n\r\n\r\n\r\nGelman, A., & Rubin, D. B. (1995). Avoiding model selection in bayesian social research. Sociological Methodology, 25, 165â€“173.\r\n\r\n\r\nGronau, Q. F., Sarafoglou, A., Matzke, D., Ly, A., Boehm, U., Marsman, M., Leslie, D. S., Forster, J. J., Wagenmakers, E.-J., & Steingroever, H. (2017). A tutorial on bridge sampling. Journal of Mathematical Psychology, 81, 80â€“97.\r\n\r\n\r\nJohnson, V. E., & Rossell, D. (2010). On the use of non-local prior densities in bayesian hypothesis tests. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(2), 143â€“170.\r\n\r\n\r\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430), 773â€“795.\r\n\r\n\r\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a bayesian perspective. Psychonomic Bulletin & Review, 25(1), 178â€“206.\r\n\r\n\r\nMacKay, D. J. C. (2003). Model comparison and occamâ€™s razor. In Information theory, inference and learning algorithms. Cambridge university press.\r\n\r\n\r\nMorey, R. D., & Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. Psychological Methods, 16(4), 406.\r\n\r\n\r\nTendeiro, J. N., & Kiers, H. A. (2019). A review of issues about null hypothesis bayesian testing. Psychological Methods, 24(6), 774.\r\n\r\n\r\nDowÃ³d moÅ¼ecie znaleÅ›Ä‡ w suplemencie B do tego artykuÅ‚u (Tendeiro & Kiers, 2019)â†©ï¸\r\n",
    "preview": "posts/2023-01-12-bayes-factor/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-02-01T20:03:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-26-poznajcie-jags/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "CzÄ™Å›Ä‡ 2.5: Poznajcie JAGS",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-12-26",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nJAGS\r\nJAGS in JASP\r\nBonus - diagramy Bayesowskie\r\n\r\nDzisiejszy wpis bÄ™dzie krÃ³tki. PokaÅ¼Ä™ wam jak zrobiÄ‡ to co robiliÅ›my w poprzedniej czÄ™Å›ci przy uÅ¼yciu oprogramowania, ktÃ³re pozwoli nam na tworzenie i estymowanie (przy uÅ¼yciu MCMC) nawet bardzo skomplikowanych modeli w Å‚atwy sposÃ³b. MoÅ¼liwoÅ›ci jest kilka, ja przedstawie wam JAGS (Just Another Gibbs Sampler). Jest to samodzielne oprogramowanie, ktÃ³re moÅ¼na uÅ¼ywaÄ‡ bezpoÅ›rednio lub za pomocÄ… jÄ™zykÃ³w programowania takich jak R, a takÅ¼e graficznego programu statystycznego JASP. PobraÄ‡ moÅ¼ecie go tutaj.\r\nJAGS\r\nPowtÃ³rzmy naszÄ… bayesowskÄ… regresjÄ™ z tym razem przy uÅ¼yciu JAGS. StwÃ³rzmy zbior danych:\r\n\r\n\r\nlibrary(rjags)\r\nlibrary(coda)\r\nlibrary(MCMCvis)\r\n\r\nx = rnorm(100,0,10)\r\ny = 5 + x + rnorm(100,0,10)\r\ndata = data.frame(y,x)\r\n\r\n\r\nMusimy zdefiniowaÄ‡ kod naszego modelu. JAGS uÅ¼ywa dosyÄ‡ intuicyjnego kodowania BUGS. Model regresji bÄ™dzie wyglÄ…daÅ‚ tak:\r\n\r\n\r\nmod = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(y)) {\r\n    y[i] ~ dnorm(a + b * x[i], sigma^-2)\r\n  }}\"\r\n\r\n\r\nOperator ~ oznacza, Å¼e parametr po lewej stronie dany jesr rozkÅ‚adem po prawej. PoniewaÅ¼ w naszym kodzie tylko zmienna Y wystÄ™puje w danych, JAGS automatycznie rozpozna, Å¼e zdefiniowany rozkÅ‚ad Y jest funkcjÄ… wiarygodnoÅ›ci. ZauwaÅ¼cie, Å¼e gdy definuje b jako wartoÅ›Ä‡ losowanÄ… z rozkÅ‚adu normalnego \\(N(0,100)\\), jako drugi argument podaÅ‚em \\(100^{-2}\\). JAGS zamiast odchylenia standardowego dla rozkÅ‚adu normalnego przyjmuje precyzjÄ™ (precision), czyli odwrotnoÅ›Ä‡ wariancji. Po wiÄ™cej szczegÃ³Å‚Ã³w polecam zajrzeÄ‡ do dokumentacji.\r\n\r\n\r\n# Parametry, ktÃ³re chcemy Å›ledziÄ‡. \r\nparams = c(\"a\",\"b\",\"sigma\")\r\n\r\n## Hiperparametry\r\nn.adapt = 100\r\n# Liczba iteracji adaptacji\r\nni = 3000\r\n# Liczba iteracji \"wypalania\"\r\nnb = 3000\r\n# Liczba prÃ³bek z rozkÅ‚adu post priori\r\nnt = 1\r\n# Odchudzanie - 1 oznacza, bierzemy kaÅ¼dÄ… prÃ³bkÄ™ z rozkÅ‚adu post priori\r\nnc = 3\r\n# liczba Å‚aÅ„cuchÃ³w\r\n\r\n# Inicjacja modelu\r\njmod = jags.model(file = textConnection(mod), data = data, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\n\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 100\r\n   Unobserved stochastic nodes: 3\r\n   Total graph size: 412\r\n\r\nInitializing model\r\n\r\n# Wypalanie\r\nupdate(jmod, n.iter=ni, by=1)\r\n\r\n# Losowanie prÃ³bek z rozkÅ‚adu post priori\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\nZauwaÅ¼cie, Å¼e Jags miaÅ‚ tylko 3000 iteracji wypalania, podczas gdy mÃ³j kod z poprzedniej czÄ™Å›ci potrzebowaÅ‚ ponad 16 razy wiÄ™cej. JAGS uÅ¼ywa kombinacji rÃ³Å¼nych agorytmÃ³w MCMC, ponado dokonuje za nas tunningu hiperparametrÃ³w (takich jak na przykÅ‚ad step size w Metropolis-Hasting). Dlatego, oprÃ³cz wypalania i losowania, mamy jeszcze adaptacjÄ™. Obejrzmy sobie rozkÅ‚ady post priori:\r\n\r\n\r\nplot(post)\r\n\r\n\r\n\r\nPoliczmy statystyki:\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:6100\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n        Mean      SD Naive SE Time-series SE\r\na     4.5803 0.95620 0.010079       0.010650\r\nb     0.9113 0.08775 0.000925       0.000925\r\nsigma 9.3601 0.68294 0.007199       0.007507\r\n\r\n2. Quantiles for each variable:\r\n\r\n       2.5%    25%    50%    75%  97.5%\r\na     2.702 3.9454 4.5744 5.2252  6.471\r\nb     0.738 0.8524 0.9122 0.9695  1.085\r\nsigma 8.148 8.8782 9.3262 9.7982 10.810\r\n\r\nA takÅ¼e sprawdÅºmy czy Å‚aÅ„cuchy siÄ™ zbiegÅ‚y:\r\n\r\n\r\ngelman.diag(post)\r\n\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\na              1          1\r\nb              1          1\r\nsigma          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\n\r\nStatystyka Gelmana-Rubina wskazuje, Å¼e Å‚aÅ„cuchy siÄ™ zbiegÅ‚y. SprawdÅºmy jeszcze czy aby na pewno stastystyka nie osiÄ…gneÅ‚a takich wartoÅ›ci przez przypadek.\r\n\r\n\r\ngelman.plot(post)\r\n\r\n\r\n\r\nNa wykresie widzimy statystykÄ™ Gelmana Rubina licznÄ… dla kaÅ¼dych 50 iteracji nastÄ™pujÄ…cych po sobie. DziÄ™ki temu moÅ¼emy sprawdziÄ‡ czy nasze prÃ³by wylosowane sÄ… z rozkÅ‚adÃ³w, ktÃ³re naprawdÄ™ siÄ™ zbiegÅ‚y. Widzimy, Å¼e dla poczÄ…tkowych wartoÅ›ci Å‚ancuchÃ³w statystyka ma wyÅ¼sze wartoÅ›Ä‡i, co moÅ¼e sugerowaÄ‡, Å¼e powinniÅ›my zastosowaÄ‡ dÅ‚uÅ¼szy interwaÅ‚ wypalania (choÄ‡ w naszym przypadku statystyka nie przekracza nigdzie wartoÅ›ci 1.15, wiÄ™c bybyÅ‚by to raczej zabieg kosmetyczny).\r\nSprawdÅºmy autokorelacjÄ™:\r\n\r\n\r\nautocorr.diag(post)\r\n\r\n                 a            b        sigma\r\nLag 0  1.000000000  1.000000000  1.000000000\r\nLag 1  0.029878333  0.021306509  0.050087619\r\nLag 5  0.005264560 -0.002882478  0.003968644\r\nLag 10 0.002108300 -0.009317490  0.006910744\r\nLag 50 0.006366223  0.019776605 -0.017047426\r\n\r\nAutokorelacja praktycznie nie wystÄ™puje.\r\nJAGS poradziÅ‚ sobie duÅ¼o lepiej z regresjÄ… liniowÄ…, niÅ¼ mÃ³j zabawkowy kod.Ponadto jego skÅ‚adnia jest relatywnie prosta. Dlatego z niego bÄ™dziemy korzystaÄ‡ w nastÄ™pnych czÄ™Å›ciach tutorialu.\r\nPrzykÅ‚adowe kody JAGS dla wielu modeli moÅ¼ecie znaleÅ›Ä‡ tutaj.\r\nJAGS in JASP\r\nJeÅ›li ktoÅ› nie jest wielkim fanem robienia statystyk za pomocÄ… jÄ™zykÃ³w programowania, JAGS jest kompatybilny z graficznym programem statystycznym JASP.\r\n\r\nBonus - diagramy Bayesowskie\r\nWarto wspomnieÄ‡, Å¼e do wizualizacji modeli bayesowskich czÄ™sto uÅ¼ywa siÄ™ grafÃ³w (ja do ich budowy uÅ¼ywam biblioteki daft w Pythonie). W przypadku naszego modelu:\r\n\r\nimport daft\r\nimport matplotlib.pyplot as plt \r\n\r\npgm = daft.PGM(observed_style=\"inner\")\r\n\r\npgm.add_node(\"alpha\", r\"$\\alpha$\", 0.5, 2)\r\npgm.add_node(\"beta\", r\"$\\beta$\", 1.5, 2)\r\npgm.add_node(\"sigma\", r\"$\\sigma$\", 2.5, 2)\r\npgm.add_node(\"x\", r\"$x_i$\", 2, 1, observed=True)\r\npgm.add_node(\"y\", r\"$y_i$\", 1, 1, observed=True)\r\n\r\npgm.add_edge(\"alpha\", \"y\")\r\npgm.add_edge(\"beta\", \"y\")\r\npgm.add_edge(\"x\", \"y\")\r\npgm.add_edge(\"sigma\", \"y\")\r\n\r\npgm.add_plate([0.5, 0.5, 2, 1], label=r\"$i = 1, \\ldots, N$\", shift=-0.1)\r\n\r\npgm.render()\r\nplt.show() \r\n\r\n\r\n\r\n\r\nDiagram pokazuje, Å¼e rozkÅ‚ad zmiennej \\(y_i\\) definujÄ… 3 nieobserwowalne parametry (pojedyÅ„cze okrÄ™gi) i jedna obserwowalna zmienna (pogrubiony okrÄ…g).\r\nW przypadku modeli bayesowskich zapewno dosyÄ‡ czÄ™sto bÄ™dziecie siÄ™ spotykaÄ‡ z takimi graficznymi opisami modeli. Do grafu doÅ‚Ä…czane sÄ… zwykle definicje parametrÃ³w. W naszym przypadku:\r\n\\[\\alpha \\sim U(-1000, 1000)\\] \\[ \\beta \\sim N(0, 100) \\] \\[ \\sigma \\sim U(0.000001,1000)\\] \\[ y_i \\sim N(a + b * x_i, \\sigma)\\]\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-26-poznajcie-jags/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-05-01T14:22:09+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-30-tutorial-bayes-ii/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "CzÄ™Å›Ä‡ II: Estymacja modeli bayesowskich",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-10-30",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWstÄ™p\r\nMCMC\r\nPrzykÅ‚ad wizualny\r\nPrzykÅ‚ad praktyczny - Bayesowska Regresja Liniowa\r\n\r\nDiagnostyka\r\nGelman-Rubin Convergence Diagnostic\r\nAutokorelacja\r\nPosterior predictive check\r\n\r\nPodsumowanie\r\n\r\n\r\n\r\n\r\nPost bÄ™dzie aktualizowany.\r\n\r\nWstÄ™p\r\nW poprzedniej czÄ™Å›ci dowiedzieliÅ›my siÄ™ na czym polega wnioskowanie bayesowskie. W nastÄ™pnych czÄ™Å›ciach bÄ™dziemy budowaÄ‡ modele bayesowskie i bÄ™dziemy uÅ¼ywaÄ‡ do tego dedykowanych pakietÃ³w i oprogramowania. Dlatego dzisiaj dowiemy siÄ™ jak od wewnÄ…trz wyglÄ…da estymacja rozkÅ‚adÃ³w post priori, gdy nie jest to moÅ¼liwe metodÄ… analitycznÄ….\r\nZaÅ‚Ã³Å¼my, Å¼e zdefiniowaliÅ›my sobie jakÄ…Å› funkcjÄ™ wiarygodnoÅ›ci i rozkÅ‚ady a priori parametrÃ³w \\(\\theta\\). Chcemy numerycznie przybliÅ¼yÄ‡ rozkÅ‚ad \\(P(\\theta|y)\\)\r\n\\[P(\\theta|y) \\propto P(y|\\theta)P(\\theta)\\]\r\nIstniejÄ… dwie najpopularniejsze metody numerycznej estymacji modeli bayesowskich - MCMC i Variational Bayes.\r\nVariational Bayes to metoda, ktÃ³ra zmniejsza wymagania obliczenowe przyjmujÄ…c pewne upraszczajÄ…ce zaÅ‚oÅ¼enia, dziÄ™ki czemu moÅ¼na jÄ… efektywnie stosowaÄ‡ do duÅ¼ych zbiorÃ³w danych.\r\nMCMC (Markov Chain Monte Carlo) jest metodÄ… ktÃ³ra jest dokÅ‚adniejsza, ale bardziej czasochÅ‚onna i to jÄ… dziÅ› wam przedstawiÄ™.\r\nMCMC\r\nZastanÃ³wmy siÄ™ przez chwilÄ™ nad problemem znalezienia rozkÅ‚adÃ³w post priori. GdybyÅ›my szukali najlepszych estymat punktowych parametrÃ³w, jak w czÄ™stoÅ›ciowym wnioskowaniu, moglibyÅ›my ewaluowaÄ‡ naszÄ… funkcjÄ™ \\(P(\\theta|y)\\) i szukaÄ‡ takich \\(\\theta\\) dla ktÃ³rych przyjmuje najwyÅ¼szÄ… wartoÅ›Ä‡.\r\nMoglibyÅ›my to na przykÅ‚ad zrobiÄ‡ w ten sposÃ³b. Dla funkcji \\(f(x)\\) zaczynamy z losowÄ… wartoÅ›ciÄ… \\(x\\). W nastÄ™pnym kroku dodajemy maÅ‚Ä… losowÄ… wartoÅ›Ä‡ \\(\\Delta\\) do \\(x\\) i sprawdzamy czy \\(f(x + \\Delta)\\) jest wiÄ™ksze. JeÅ›li tak, \\(x + \\Delta\\) staje siÄ™ naszym \\(x\\). JeÅ›li nie, powtarzamy porzednie kroki. Zatrzymujemy siÄ™ wtedy, gdy \\(x\\) przestanie siÄ™ zmieniaÄ‡.\r\nTen algorytm ma pewnÄ… wadÄ™, poniewaÅ¼ odszuka on najbliÅ¼sze lokalne maksimum, ktÃ³re nie musi byÄ‡ globalnym. Do tego jeszcze wrÃ³cimy.\r\nW podejsciu bayesowskim szukamy rozkÅ‚adu \\(P(\\theta|y)\\). MoglibyÅ›my go przybliÅ¼yÄ‡ losujÄ…c prÃ³bÄ™ z niego. Ale jak wylosowaÄ‡ prÃ³bÄ™ z rozkÅ‚adu, ktÃ³rego nie znamy? MoÅ¼emy najpier poszukaÄ‡ Å›rodka rozkÅ‚adu za pomocÄ… wyÅ¼ej opisanej metody. Gdy juÅ¼ go znajdziemy, moÅ¼emy zasymulowaÄ‡ losowanie w prosty sposÃ³b. JeÅ›li \\(P(\\theta|y)\\) ma wartoÅ›Ä‡ 0.2 w pewnym miejscu rozkÅ‚adu, a w 0.1 w innym, to wiemy Å¼e pierwsza wartoÅ›Ä‡ \\(\\theta\\) musi wystÄ™powaÄ‡ Å›rednio dwa razy czÄ™Å›ciej.\r\nMoÅ¼emy wiÄ™c przyjÄ…Ä‡ nastÄ™pujÄ…cÄ… strategiÄ™:\r\nZainicjowaÄ‡ losowo pierwszÄ… wartoÅ›Ä‡ \\(\\theta\\) i obliczyÄ‡ \\(P(\\theta|y)\\).\r\nPrzesunÄ…Ä‡ siÄ™ o losowÄ… wartoÅ›Ä‡ \\(\\Delta\\) (losowanÄ… z rozkÅ‚adu normalnego, ktÃ³rego SD jest wielkoÅ›ciÄ… przesuniÄ™cia - step size) i obliczyÄ‡ \\(P(\\theta + \\Delta|y)\\).\r\nJeÅ›li \\(P(\\theta + \\Delta|y)\\) jest wiÄ™ksze od \\(P(\\theta|y)\\) przyjmujemy \\(\\theta + \\Delta\\) jako losowÄ… prÃ³bkÄ™ z rozkÅ‚adu.\r\nJeÅ›li \\(P(\\theta + \\Delta|y)\\) jest mniejsze od \\(P(\\theta|y)\\) losujemy wartoÅ›Ä‡ miÄ™dzy z przedziaÅ‚u [0,1].\r\nPrzyjmujemy \\(\\theta + \\Delta\\) jako losowÄ… prÃ³bkÄ™ z rozkÅ‚adu jeÅ›li wylosowana wartoÅ›Ä‡ jest mniejsza od \\(\\frac{P(\\theta + \\Delta|y)}{P(\\theta|y)}\\).\r\nPowtarzamy procedurÄ™ od punktu 2, jeÅ›li wylosowana wartoÅ›Ä‡ jest wiÄ™ksza od \\(\\frac{P(\\theta + \\Delta|y)}{P(\\theta|y)}\\).\r\nPrzyjÄ…Ä‡ \\(\\theta+\\Delta\\) jako nowÄ… wartoÅ›c \\(\\theta\\) i powtÃ³rzyÄ‡ procedurÄ™ od punktu 2.\r\nTa wersja MCMC to algorytm Metropolisa-Hastingsa. Å»eby byÄ‡ rzetelnym, prawdopodobnie musiaÅ‚bym zaczÄ…Ä‡ od przedstawienia wam czym jest proces stochatyczny, Å‚aÅ„cuchy Markova, rozkÅ‚ad stacjonarny, et cetera. Ale poniewaÅ¼ chcÄ™ siÄ™ skupiÄ‡ na pokazaniu jak wyglÄ…da estymacja MCMC w praktyce, pominÄ™ to. DowÃ³d i opis matematyczny dziaÅ‚ania algorytmu Metropolisa-Hastingsa moÅ¼ecie znaleÅºÄ‡ tutaj.\r\nSkÄ…d wiemy, Å¼e losujemy z wÅ‚aÅ›ciwej przestrzeni parametrÃ³w rozkÅ‚adu? Å»eby wyjaÅ›niÄ‡ ten termin pÃ³jrzmy na wykres poniÅ¼ej.\r\n\r\n\r\n\r\nJak widzimy, ten rozkÅ‚ad prawdopodbieÅ„stwa ma dwa maksima. ChcielibyÅ›my, by rozkÅ‚ad post priori byÅ‚ odwzorowany jak najdokÅ‚adniej. Jednak nasz algorytm moÅ¼e w utknÄ…Ä‡ makismum po lewej, dajÄ…c nam bÅ‚Ä™dne przybliÅ¼enie rozkÅ‚adu.\r\nCzy nasz algorytm jest w stanie poradziÄ‡ sobie z tym? OdowiedÅº brzmi: WykonujÄ…c nieskoÅ„czonÄ… liczbÄ™ iteracji, wartoÅ›ci zbiegnÄ… do wÅ‚aÅ›ciwej przestrzeni rozkÅ‚adu. Dzieje siÄ™ tak dlatego, Å¼e nasz algorytm moÅ¼e poruszaÄ‡ siÄ™ takÅ¼e po mniejszych wartoÅ›ciach \\(P(\\theta|y)\\), jest wiÄ™c w stanie opuÅ›ciÄ‡ lokalne maksima.\r\nMy jednak nie dysponujemy nieskoÅ„czonÄ… liczbÄ… iteracji. Dlatego algorytm powtarzamy \\(n\\) razy. KaÅ¼dÄ… instancjÄ™ nazywamy Å‚aÅ„cuchem, a poniewaÅ¼ kaÅ¼dy Å‚aÅ„cuch rozpoczyna w innym losowym miejscu przestrzeni parametrÃ³w, jeÅ›li Å‚aÅ„cuchy siÄ™ zbiegnÄ…, moÅ¼emy domniemywaÄ‡, Å¼e odnaleÅºliÅ›my wÅ‚aÅ›ciwÄ… przestrzeÅ„ i zaczÄ…Ä‡ losowaÄ‡ prÃ³bki z naszego rozkÅ‚adu.\r\nPrzykÅ‚ad wizualny\r\nZobaczmy to na przykÅ‚adzie, ktÃ³ry kiedyÅ› podpatrzyÅ‚em i wydaje mi siÄ™ idealny do zaprezentowania dziaÅ‚ania MCMC. WeÅºmy sobie pewnÄ… funkcjÄ™, ktÃ³ra jako argumenty bierze \\(x\\) i \\(y\\).\r\n\r\n\r\nf = function (x,y) {\r\n  return(20*exp(-0.2*sqrt((x^2+y^2)/2))+exp(0.5*(cos(2*pi*x)+cos(2*pi*y))))\r\n}\r\n\r\nlibrary(lattice)\r\npoints = matrix(nrow = 61, ncol = 61, seq(-3,3,0.1)) \r\nfilled.contour(x = points[,1], y = points[,1], z = f(points, t(points)), nlevels=20)\r\n\r\n\r\n\r\nJak widzimy, nasza funkcja ma wiele lokalnych maksimÃ³w, ale tylko jedno globalne na Å›rodku. SprawdÅºmy jak poradzi sobie nasz algorytm. Uwaga, funkcja, ktÃ³rej uÅ¼ywamy nie jest rozkÅ‚adem prawdopodbieÅ„stwa, dlatego algorytm wyglÄ…da trochÄ™ inaczej niÅ¼ w podpunktach przedstawionych wczeÅ›niej.\r\n\r\n\r\njmp = 0.2 # step size\r\niter = 10000 #liczba iteracji algorytmu\r\nchain = 3 # liczba Å‚aÅ„cuchÃ³w\r\n\r\nsamples =  array(NA, c(10000,3,3)) # Pusty tensor na nasze prÃ³bki\r\n\r\n# WartoÅ›ci poczÄ…tkowe Å‚aÅ„cuchÃ³w\r\nsamples[1,1,1] = 2.3 \r\nsamples[1,2,1] = 1.98\r\nsamples[1,1,2] = -2.3  \r\nsamples[1,2,2] = -1.98\r\nsamples[1,1,3] = 1 \r\nsamples[1,2,3] = -2.5\r\n\r\nfor (j in 1:chain){\r\nsamples[1,3,j] = f(samples[1,1,j], samples[1,2,j]) # ewaluujemy naszÄ… funkcjÄ™ dla wartoÅ›ci\r\n# poczÄ…tkowych Å‚aÅ„cuchÃ³w\r\nn = 1\r\n\r\nwhile(n <= (iter-1)) { #Iterujemy dopÃ³ki nie zbierzemy 'iter' prÃ³bek\r\n  new_x = rnorm(1, samples[n,1,j], jmp) # przemieszczamy siÄ™ o losowÄ… wartoÅ›Ä‡ dla wartoÅ›ci x\r\n  new_y = rnorm(1, samples[n,2,j], jmp) # przemieszczamy siÄ™ o losowÄ… wartoÅ›Ä‡ dla wartoÅ›ci y\r\n  \r\n  if (exp(-12*(f(samples[n,1,j], samples[n,2,j])-f(new_x, new_y))) > runif(1,0,1)) {\r\n  # Sprawdzamy czy nowe wartoÅ›ci zwracajÄ… wyzszÄ… wartoÅ›Ä‡ naszej funkcji. PoniwewaÅ¼ funkcja nie\r\n  # jest rozkÅ‚adem prawdopodobieÅ„stwa nie moÅ¼emy policzyÄ‡ stosunku prawdopodobieÅ„st. Zamiast\r\n  # tego uÅ¼ywamy funkcji eksponencjalnej rÃ³Å¼nicy pomnoÅ¼onej przez ujemnÄ… staÅ‚Ä….  \r\n    \r\n    n = n + 1\r\n    samples[n,1,j] = new_x\r\n    samples[n,2,j] = new_y\r\n    samples[n,3,j] = f(new_x, new_y)\r\n\r\n  }   \r\n}\r\n}\r\n\r\n\r\nZobaczmy jak poradziÅ‚y sobie nasze Å‚aÅ„cuchy.\r\n\r\n\r\nfilled.contour(x=points[,1], y=points[,1], z =f(points,t(points)), nlevels=20, plot.axes = {\r\n  axis(1); axis(2); lines(samples[1:10000,1:2,1], col =\r\n  \"black\");lines(samples[1:10000,1:2,2], col = \"blue\"); lines(samples[1:10000,1:2,3], col = \r\n  \"green\") })\r\n\r\n\r\n\r\nÅaÅ„cuchy znajdowaÅ‚y na swojej drodze lokalne maksima, w ktÃ³rych pozostawaÅ‚y na jakiÅ› czas, po czym zmierzaÅ‚y do nastÄ™pnych. Ostatecznie wszystkie zbiegÅ‚y siÄ™ do lokalnego maksimum.\r\nDobrze. Wiemy, Å¼e nasz algorytm znalazÅ‚ globalne maksimum, poniewaÅ¼ znamy funkcjÄ™, ktÃ³rej ekstremum szukamy. Jednak w przeciwieÅ„stwie do powyÅ¼szego przykÅ‚adu, zwykle nie znamy funkcji generujÄ…cej rozkÅ‚ad post priori - dopiero staramy siÄ™ go estymowaÄ‡. Jak wiÄ™c sprawdziÄ‡ czy znaleÅºliÅ›my wÅ‚asciwÄ… przestrzeÅ„ losowania? SpÃ³jrzmy na pierwsze 200 interacji dla argumentu \\(x\\).\r\n\r\n\r\nx_samples = as.data.frame(samples[1:iter,1,1:3])\r\nx_samples$iteration = 1:dim(samples)[1]\r\ncolnames(x_samples) = c(1,2,3,\"Iteration\")\r\nx_samples_burn_in = x_samples[1:200,]\r\nx_samples_in_space = x_samples[2000:2500,]\r\nx_samples_burn_in = pivot_longer(x_samples_burn_in,cols =1:3, names_to = \"chain\", values_to = \"x\")\r\nx_samples_in_space = pivot_longer(x_samples_in_space,cols =1:3, names_to = \"chain\", values_to = \"x\")\r\n\r\nggplot(x_samples_burn_in, mapping = aes(x = Iteration, y = x, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWidzimy, Å¼e kaÅ¼dy Å‚aÅ„cuch oscyluje wokÃ³Å‚ innej wartoÅ›ci. Teraz spÃ³jrzmy na iteracje miÄ™dzy 2000 a 2500.\r\n\r\n\r\nggplot(x_samples_in_space, mapping = aes(x = Iteration, y = x, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWidzimy, Å¼e wszytkie Å‚aÅ„cuchy siÄ™ zbiegÅ‚y. Co prawda, nie daje nam to caÅ‚kowitej pewnoÅ›ci, Å¼e Å‚aÅ„cuchy odnalazÅ‚y wÅ‚aÅ›ciwÄ… przestrzeÅ„.\r\nDziaÅ‚anie algorytmu MCMC dzieli siÄ™ na dwie czÄ™Å›ci: wypalanie (burn-in) i losowanie z rozkÅ‚adu post priori. Wypalamy Å‚aÅ„cuchy dopÃ³ki siÄ™ nie zbiegnÄ…, wtedy moÅ¼emy rozpoczÄ…Ä‡ estymacje rozkÅ‚adu post priori z prÃ³bek z Å‚aÅ„cuchÃ³w. Im wiÄ™kszej liczby Å‚aÅ„cuchÃ³w uÅ¼yjemy, tym silniejszÄ… mamy przesÅ‚ankÄ™ (gdy wszystkie siÄ™ zbiegnÄ…), Å¼e odnaleÅºliÅ›my wÅ‚aÅ›ciwÄ… przestrzeÅ„ losowania.\r\nPrzykÅ‚ad praktyczny - Bayesowska Regresja Liniowa\r\nNabyliÅ›my juÅ¼ intuicjÄ™, jak dziaÅ‚a MCMC. Zabierzmy siÄ™ wiÄ™c za prawdziwy statystyczny problem.\r\nPowiedzmy, Å¼e chcemy policzyÄ‡ bayesowskÄ… regresjÄ™ liniowÄ… zmiennej \\(y\\) ze wzglÄ™du na \\(x\\):\r\n\\[y_i = \\alpha + \\beta x_i + \\epsilon{_i}\\] gdzie \\(\\alpha\\) to staÅ‚a, \\(\\beta\\) to wspÃ³Å‚Ä‡zynnik regresji a \\(\\epsilon{_i}\\) to bÅ‚Ä…d.\r\nWytwÃ³rzmy sobie przykÅ‚adowe dane:\r\n\r\n\r\nx = rnorm(100,0,10)\r\ny = 5 + x + rnorm(100,0,10)\r\n\r\n\r\nW ujÄ™ciu Bayesowskim bÄ™dziemy szukali rozkÅ‚adÃ³w post priori interesujÄ…cych nas parametrÃ³w. NaszÄ… zmiennÄ… zaleÅ¼nÄ… zamodelujemy w nastÄ™pujÄ…cy sposÃ³b:\r\n\\[ y_i \\sim N(\\alpha + \\beta x_i, \\sigma)\\]\r\nCo oznacza, Å¼e wartoÅ›Ä‡ zmiennej zaleÅ¼nej u kaÅ¼dej obserwacji jest wylosowana z rozkÅ‚adu normalnego o Å›redniej \\(\\alpha + \\beta x_i\\) i odchyleniu standardowym \\(\\sigma\\). JednoczeÅ›nie, jak widzimy, jest to nasza funkcja wiarygodnoÅ›ci \\(P(y|\\theta)\\) - prawdopodbieÅ„stwo otrzymania \\(y\\) pod warunkiem parametrÃ³w \\(\\alpha\\), \\(\\beta\\) i \\(\\sigma\\) zapisanych w skrÃ³cie jako wektor \\(\\theta\\). Musimy zdefiniowaÄ‡ jeszcze rozkÅ‚ady a priori dla parametrÃ³w \\(\\theta\\) i moÅ¼emy przystÄ…piÄ‡ do obliczania rozkÅ‚adÃ³w post priori interesujÄ…cyh nas parametrÃ³w. W tym przykÅ‚adzie uÅ¼yjemy nieinformatywnych rozkÅ‚adÃ³w a priori. Zamodelujemy \\(\\alpha\\) i \\(\\sigma\\) rozkÅ‚adami jednostajnymi, a \\(\\beta\\) rozkÅ‚adem normalnym o Å›redniej 0.\r\nZaimplementujmy algorytm Metropolisa-Hastingsa by wyestymowaÄ‡ rozkÅ‚ady parametrÃ³w regresji.\r\n\r\n\r\n# Definujemy naszÄ… funkcjÄ™ wiarygodnoÅ›ci, prior i posterior. ZwrÃ³cie uwagÄ™, Å¼e funkcjÄ™\r\n# zwracajÄ… nam logarytm naturalny gÄ™stoÅ›ci prawdopodobieÅ„stwa. Jest to uÅ¼yteczna \r\n# transformacja, poniewaÅ¼ komputery nie radzÄ… sobie dobrze z wartoÅ›ciami bardzo bliskimi 0. \r\n\r\nlikelihood <- function(parameters){\r\n  a=parameters[1]; b=parameters[2]; sigma=parameters[3]\r\n  sum(dnorm(y,a +b*x, sigma, log = TRUE))\r\n}\r\n\r\nprior <- function(parameters){\r\n  a=parameters[1]; b=parameters[2]; sigma=parameters[3]\r\n  sum(dunif(a,-1000,1000,log = TRUE), dnorm(b,0,100,log = TRUE),   \r\n  dunif(sigma,0.000001,1000,log = TRUE))\r\n}\r\n\r\nposterior <- function(parameters) {likelihood(parameters) + prior(parameters)}\r\n\r\nchain = 3 # Liczba Å‚aÅ„cuchÃ³w\r\nn.iter <- 300000 # Liczba iteracji\r\nresults <- array(NA, c(n.iter,3,3)) # Pusty tensor na nasze prÃ³bki\r\n\r\nn.burn = 50000 # Liczba iteracji, ktÃ³rÄ… odrzucimy w ramach wypalania\r\n\r\nfor (j in 1:chain){\r\n\r\nparameters <- c(runif(1,-50,50), rnorm(1,0,10),runif(1,0.000001,100)) # Losujemy wartoÅ›ci poczÄ…tkowe dla Å‚aÅ„cucha\r\nresults[1,,j] <- parameters\r\nnaccepted = 2\r\n\r\n  while(naccepted <= n.iter){\r\n    \r\n    candidate <-  rnorm(3, mean = parameters, sd = 0.1) # przemieszczamy siÄ™ o losowÄ… wartoÅ›Ä‡\r\n    # parametrÃ³w od wartoÅ›ci obecnych parametrÃ³w \r\n    \r\n    ratio <- exp(posterior(candidate) - posterior(parameters)) # Odejmujemy poniewaÅ¼ log(a/b) \r\n  # = log(a) - log(b). RÃ³Å¼nicÄ™ wkladamy do funckcji eksponencjalnej by z logarytmÃ³w otrzyamÄ‡\r\n  # stosunek prawdopodobieÅ„stw. \r\n    \r\n    if (runif(1) < ratio) {parameters <- candidate \r\n    results[naccepted, ,j] <- parameters\r\n    naccepted = naccepted + 1}\r\n  }}\r\n\r\nresults <- results[(n.burn+1):n.iter,,] # Usuwamy 'n.burn' pierwszych iteracji\r\n\r\nall_chains = data.frame()\r\n\r\nfor (i in 1:chain){\r\n  \r\n  all_chains = rbind(all_chains,data.frame(results[,,i], chain = as.character(i), Iteration = (n.burn+1):n.iter))\r\n  \r\n  }\r\n\r\ncolnames(all_chains) = c(\"a\", \"b\", \"sigma\",\"chain\", \"Iteration\")\r\n\r\n\r\nMamy to! SprawdÅºmy czy nasze Å‚aÅ„cuchy siÄ™ zbiegÅ‚y.\r\n\r\n\r\nlibrary(gridExtra)\r\np1 <- ggplot(all_chains, mapping = aes(x = Iteration, y = a, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\np2 <- ggplot(all_chains, mapping = aes(x = Iteration, y = b, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\np3 <- ggplot(all_chains, mapping = aes(x = Iteration, y = sigma, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\ngrid.arrange(p1, p2,p3, nrow = 3)\r\n\r\n\r\n\r\nWizualna inspekcja wskazuje, Å¼e Å‚aÅ„cuchy zbiegÅ‚y sie wystarczajÄ…co (pÃ³Å¼niej poznamy iloÅ›ciowe wskaÅºniki). Zobaczmy rozkÅ‚ady post piori.\r\n\r\n\r\np1 <- ggplot(all_chains, mapping = aes(x = a))+\r\n  geom_histogram(fill = \"white\",color=\"black\",) +\r\n  theme_minimal()\r\np2 <- ggplot(all_chains, mapping = aes(x = b))+\r\n  geom_histogram(fill = \"white\",color=\"black\",) +\r\n  theme_minimal()\r\np3 <- ggplot(all_chains, mapping = aes(x = sigma))+\r\n  geom_histogram(fill = \"white\",color=\"black\") +\r\n  theme_minimal()\r\ngrid.arrange(p1, p2, p3, nrow = 1)\r\n\r\n\r\n\r\nPoliczmy statystyki rozkÅ‚adÃ³w.\r\n\r\n\r\nsummary(all_chains[,1:3])\r\n\r\n       a                b              sigma       \r\n Min.   :0.6338   Min.   :0.4456   Min.   : 7.213  \r\n 1st Qu.:4.2037   1st Qu.:0.8558   1st Qu.: 9.315  \r\n Median :4.8798   Median :0.9194   Median : 9.779  \r\n Mean   :4.8822   Mean   :0.9195   Mean   : 9.819  \r\n 3rd Qu.:5.5547   3rd Qu.:0.9830   3rd Qu.:10.274  \r\n Max.   :9.3682   Max.   :1.3589   Max.   :13.426  \r\n\r\nPorÃ³wnajmy z klasycznÄ… regresjÄ….\r\n\r\n\r\nm = lm(y~x)\r\ndata.frame(a = m$coefficients[1], b = m$coefficients[2], sigma = sd(m$residuals), row.names = \"Parameters\")\r\n\r\n                  a         b    sigma\r\nParameters 4.919473 0.9192618 9.606116\r\n\r\nDiagnostyka\r\nGdy juÅ¼ mamy nasz model, musimy sprawdziÄ‡ czy zostaÅ‚ wyestymowany poprawnie oraz w jakim zakresie jest w stanie odtworzyÄ‡ zaobserwowane dane.\r\nGelman-Rubin Convergence Diagnostic\r\nStatystyka Gelmana-Rubina sprawdza iloÅ›ciowo czy Å‚aÅ„cuchy siÄ™ zbiegÅ‚y. Opiera siÄ™ na stosunku wariancji miÄ™dzy Å‚aÅ„cuchami do wariancji wewnÄ…trz Å‚aÅ„cuchÃ³w (Gelman & Rubin, 1992).\r\n\\[\\begin{eqnarray*}\r\n\\bar{x}_j\r\n& = &\r\n\\frac{1}{N}\\sum_{t=1}^N x_t^{(j)}\\hspace{2em}\\text{(Åšrednia Å‚aÅ„cucha)}\\\\\r\n\\bar{x}_\\cdot\r\n& = &\r\n\\frac{1}{J}\\sum_{j=1}^J \\bar{x}_j\\hspace{2em}\\text{(Åšrednia Å‚aÅ„cuchÃ³w)}\\\\\r\nB\r\n& = &\r\n\\frac{N}{J-1}\r\n\\sum_{j=1}^J (\\bar{x}_j-\\bar{x}_\\cdot)^2\\hspace{2em}\\text{(Wariancja pomiÄ™dzy Å‚aÅ„cuchami)}\\\\\r\ns^2_j\r\n& = &\r\n\\frac{1}{N-1}\r\n\\sum_{t=1}^N (x_t^{(j)}-\\bar{x}_j)^2\\hspace{2em}\\text{(Wariancja wewnÄ…trz Å‚aÅ„cucha)}\\\\\r\nW\r\n& = &\r\n\\frac{1}{J}\\sum_{j=1}^J s_j^2\\hspace{2em}\\text{(Åšrednia wariancja wewnÄ…trz Å‚aÅ„cuchÃ³w)}\r\n\\end{eqnarray*}\\]\r\nGdzie \\(N\\) to liczba iteracji, a \\(J\\) to liczba Å‚aÅ„cuchÃ³w. Statystyka Gelmana Rubina jest dana wzorem:\r\n\\[R = \\frac{\r\n\\frac{N-1}{N}W + \\frac{1}{N}B\r\n}{W}\\]\r\nZwykle uznaje siÄ™, Å¼e Å‚aÅ„cuchy siÄ™ zbiegÅ‚y, jeÅ›li R < 1.15. StatystykÄ™ liczy siÄ™ dla kaÅ¼dego parametru. W przypadku naszej regresji:\r\n\r\n\r\nBW = pivot_longer(all_chains,cols = 1:3, names_to = \"parameter\", values_to = \"Value\")\r\nBW = BW %>% group_by(parameter,chain) %>% summarise(Mean = mean(Value), Var = var(Value), n =n())\r\nW = BW %>% group_by(parameter) %>% summarise(W = mean(Var)/3) \r\nB = BW %>% group_by(parameter) %>% summarise(B = var(Mean)) \r\n\r\nR = (W$W*((n.iter - n.burn - 1)/(n.iter - n.burn)) + B$B)/W$W\r\nnames(R) = c(\"a\",\"b\",\"sigma\")\r\nkable(R,col.names = \"R\")\r\n\r\n\r\nR\r\na\r\n1.001088\r\nb\r\n1.000062\r\nsigma\r\n1.003041\r\n\r\nKaÅ¼dy parametr speÅ‚nia kryterium Gelmana-Rubina.\r\nAutokorelacja\r\nJak wspominaÅ‚em juÅ¼ wielokrotnie wczeÅ›niej, MCMC ma symulowaÄ‡ losowanie z rozkÅ‚adu a posteriori. WyobraÅºmy sobie takÄ… sytuacjÄ™ - losujemy 1000 osÃ³b, by estymowaÄ‡ rozkÅ‚ad wzrostu w populacji. Obserwacje sÄ… od siebie niezaleÅ¼ne, wylosowanie osoby A nie ma wpÅ‚ywu na prawdopodobieÅ„stwo wylosowania osoby B. W przypadku Å‚aÅ„cuchÃ³w nie jest to prawdÄ…. ZauwaÅ¼my, Å¼e wylosowanie obserwacji B, zaleÅ¼y od tego jakÄ… wartoÅ›c ma obserwacja A.\r\nW MCMC poruszamy siÄ™ od obserwacji do obserwacji rÃ³Å¼niÄ…cych siÄ™ o jakÄ…Å› maÅ‚Ä… losowÄ… wartoÅ›Ä‡. W nastÄ™pstwie w Å‚aÅ„cuchach wystÄ™puje autokorelacja. JeÅ›li jest duÅ¼a, moÅ¼e zbiasowaÄ‡ nasz rozkÅ‚ad a posteriori, poniewaÅ¼ pewne wartoÅ›ci bÄ™dÄ… nadreprezentowane, a efektywna liczba prÃ³b z rozkÅ‚adu post priori bÄ™dzie mniejsza niÅ¼ N.\r\nAutokowariancja dla opÃ³Åºnienia (lag) \\(t\\) jest zdefiniowana tak:\r\n\\[aCov(t) = \\frac{1}{N - t} \\sum_{n=1}^{N-t} (\\theta_n - \\mu_f)\\,(\\theta_{n+t}-\\mu_\\theta)\\]\r\ngdzie\r\n\\[\\mu_\\theta = \\frac{1}{N}\\sum_{n=1}^N \\theta_n\\]\r\na \\(\\theta_n\\) to wartoÅ›Ä‡ wylosowana przez Å‚aÅ„cuch w iteracji \\(n\\).\r\nAutokorelacja dana jest wiÄ™c:\r\n\\[aCor(t) = aCov(t)/aCov(0)\\]\r\nSprawdÅºmy autokorelacjÄ™ dla pierwszego Å‚aÅ„cucha i parametru \\(\\beta\\).\r\n\r\n\r\nacf(results[,2,1],lag.max = 1000)\r\n\r\n\r\n\r\nWyglada to dobrze. Slina autokorleacja wystÄ™puje tylko dla bardzo maÅ‚ego opÃ³Åºnienia. SprawdÅºmy teraz autokorelacje parametru \\(\\sigma\\).\r\n\r\n\r\nacf(results[,3,1],lag.max = 1000)\r\n\r\n\r\n\r\nW przypadku \\(\\sigma\\) wyglÄ…da to gorzej. MoÅ¼ecie siÄ™ spotkaÄ‡ z tym, Å¼e niektÃ³rzy badacze by zmniejszyÄ‡ autokorelacje, stosujÄ… odchudzanie (thinning), polegajÄ…ce na tym, do naszej prÃ³by rozkÅ‚adu post priori bierzyemy co \\(n\\)-tÄ… wartoÅ›Ä‡ z Å‚aÅ„cucha.\r\n\r\n\r\nthinned_results = results[seq(1, n.iter - n.burn, by = 10),,]\r\nacf(thinned_results[,3,1],lag.max = 2000)\r\n\r\n\r\n\r\nJednakÅ¼e, odchudzanie nie jest najlepszÄ… strategiÄ…. Historycznie odchudzanie byÅ‚o stosowane z chÄ™ci zaoszczÄ™dzenia pamiÄ™ci komputerÃ³w. ChoÄ‡ zmniejsza autokorelacjÄ™, odchudzanie zmniejsza precyzjÄ™ naszych wynikÃ³w (Link & Eaton, 2012).\r\nZastanÃ³wmy siÄ™ dlaczego. ChcielibyÅ›my by Å›rednia wartoÅ›ci naszego Å‚aÅ„cucha byÅ‚a jak najbliÅ¼sza prawdziwej Å›redniej rozkÅ‚adu.\r\nJak pamiÄ™tamy, wariancja dla rozkÅ‚adu Å›rednich niezaleÅ¼nych prÃ³b losowych wynosi (zauwaÅ¼cie, Å¼e to inna \\(\\sigma\\) niÅ¼ wczeÅ›niej):\r\n\\[\\sigma^2 = \\frac{1}{N}\\mathrm{Var}(\\theta)\\]\r\nJednak, jeÅ¼eli obserwacje sÄ… skorelowane, wariancja wynosi:\r\n\\[\\sigma^2 = \\frac{\\tau}{N}\\mathrm{Var}(\\theta)\\]\r\nDlaczego tak jest, to na razie pominiemy (wymagaÅ‚o by to dÅ‚uÅ¼szego zatrzymania siÄ™ nad tym problemem, niemniej byÄ‡ moÅ¼e do tego wrÃ³cimy). \\(\\tau\\) to zintegrowana funkcja autokorelacji (integrated autocorrelation function) dana wzorem:\r\n\\[ \\tau =1 + 2\\sum^N_{t= 1} \\mathrm{aCor(t)}\\] Wobec tego \\(\\frac{N}{\\tau}\\) to efektywna liczba prÃ³bek (ESS), a \\(\\tau\\) to liczba iteracji jakie musi minÄ…Ä‡ Å‚aÅ„cuch zanim â€œzapomniâ€ gdzie sie zaczÄ…Å‚.\r\nPopatrzmy na wykres autokorelacji dla \\(\\beta\\). Do pewnego \\(t\\) autokorelacja jest malejÄ…cÄ… funkcjÄ…, po przekroczeniu \\(t\\) zaczyna oscylowaÄ‡ wokÃ³Å‚ 0. PoniewaÅ¼ autokorelacje mogÄ… byÄ‡ tylko pozytywne gdy \\(N\\) dÄ…Å¼y do nieskonczonoÅ›ci, obserwowane empirycznie negatywne korelacje na pewno wynikajÄ… z szumu (Geyer, 1992). Sumowanie ich by otrzymaÄ‡ zmniejszy nam estymatÄ™ \\(\\tau\\). RownieÅ¼ te maÅ‚e dodatnie korelacje sÄ… efektem szumu.\r\nZwrÃ³Ä‡my uwagÄ™ na wzÃ³r na autokowariancjÄ™. Im wiÄ™kszy lag \\(t\\) tym mniejsza liczebnoÅ›Ä‡ prÃ³bki (wynosi \\(N-t\\)), z ktÃ³rej liczymy autokowariancÄ™. W zwiÄ…zku z tym im wiÄ™ksze \\(t\\) tym bardziej zaszumione estymaty autokowariancji otrzymujemy. Jednym ze sposÃ³bw na obejÅ›cie tego problemu jest estymowanie \\(\\tau\\) w nastÄ™pujÄ…cy sposÃ³b:\r\n\\[ \\tau =1 + 2\\sum^M_{t= 1} \\mathrm{aCor(t)}\\] Gdzie \\(M\\) jest ostatniÄ… najwiÄ™kszÄ… wartoÅ›ciÄ… \\(t\\) dla ktÃ³rej wszystkie \\(t < M\\) majÄ… dodatniÄ… autokowariancjÄ™.\r\nPoliczmy efektywnÄ… liczbÄ™ prÃ³bek dla parametru \\(\\sigma\\) i Å‚aÅ„cucha 1.\r\n\r\n\r\nESS = function(data,parameter,chain){\r\n  \r\n  tmp = data[,parameter,chain]\r\n  ACF = acf(tmp,plot = F,lag.max = 10000, type = \"covariance\")$acf/var(tmp)\r\n  tau =  2*sum(ACF[1:(which(ACF<0)[1]-1)]) - 1\r\n  length(tmp)/tau\r\n}\r\n\r\nESS(results,3,1)\r\n\r\n[1] 1174.496\r\n\r\nTeraz policzmy ESS dla odchudzonego Å‚aÅ„cucha.\r\n\r\n\r\nESS(thinned_results,3,1)\r\n\r\n[1] 1166.745\r\n\r\nEES zmniejszyÅ‚o siÄ™ wzglÄ™dem nieodchudzonego Å‚aÅ„cucha. Generalnie, znacznie lepszÄ… metodÄ… redukowania wpÅ‚ywu autokorelacji, jest uÅ¼ycie wiÄ™kszej liczby dÅ‚uÅ¼szych Å‚aÅ„cuchÃ³w. Odchudzanie naleÅ¼y stosowaÄ‡ tylko jeÅ›li mierzymy siÄ™ z bardzo duÅ¼Ä… autokorelacjÄ…. ReguÅ‚a kciuka zostaÅ‚a zaproponowana w tej ksiÄ…Å¼ce (Christensen et al., 2010) - odchudzaÄ‡ jeÅ›li duÅ¼a autokorelacja wystÄ™puje powyÅ¼ej lag > 30. W naszym przypadku byÅ‚o to wiÄ™c niejako uzasadnione. Nie naleÅ¼y jednak stosowaÄ‡ odchudzania jako rutyny w MCMC.\r\nPosterior predictive check\r\nWreszcie, gdy upewnimy siÄ™, Å¼e losoujemy z wÅ‚aÅ›ciwej przestrzeni parametrÃ³w, i Å¼e nasze prÃ³bki przynajmnie udajÄ…, Å¼e sÄ… od siebie niezaleÅ¼ne, czas na sprawdzenie czy nasz model wÅ‚aÅ›ciwie modeluje to, co powinien modelowaÄ‡ (model fit). Posterior predictive check polega na losowaniu z posterior predictive distribution, ktÃ³ry jest rozkÅ‚adem prawdopodbieÅ„stwa uzyskania nowych danych, pod warunkiem zebranych przez nas danych:\r\n\\[P(y^*|y)=\\int P(y^*|\\theta)P(\\theta|y)d\\theta\\] gdzie \\(y^*\\), to nowe dane. W przypadku MCMC estymacja rozkÅ‚adu \\(P(y^*|y)\\) skÅ‚ada siÄ™ z dwÃ³ch krokÃ³w: WylosowaÄ‡ wartoÅ›Ä‡ \\(\\theta\\) z rozkÅ‚adu post priori. WstawiÄ‡ wylosowane \\(\\theta\\) do \\(P(y^*|\\theta)\\) i wylosowaÄ‡ \\(y^*\\).\r\nJak wyglÄ…da \\(P(y^*|\\theta)\\)? Podobnie jak nasza funkcja wiarygodnoÅ›ci, z tÄ… rÃ³Å¼nicÄ…, Å¼e tym razem nie obliczamy wiarygodnoÅ›ci uzyskania \\(n\\) prÃ³bek z zadanego rozkadu, lecz po po prostu losujemy z owego rozkÅ‚adu.\r\nMy sprawdzimy nasze dopasowanie modelu poprzez analizÄ™ wizualnÄ…. Wyplotujmy sobie histogram z naszych danych obok rozkÅ‚adu generowanego przez model.\r\n\r\n\r\npredicted <- NULL\r\n\r\nfor (j in 1:length(x)){\r\nfor (i in 1:chain){\r\npredicted <- c(predicted,rnorm(dim(results)[1], results[dim(results)[1],1,i] +\r\nresults[dim(results)[1],2,i]*x[j], results[dim(results)[1],3,i]))\r\n}}\r\n\r\npredicted = as.data.frame(predicted)\r\ny = as.data.frame(y)\r\nggplot() + \r\n  geom_histogram(data = y,mapping = aes(x = y,y = ..density..),\r\n                 colour = 1, fill = \"white\") +\r\n  geom_density(data = predicted, aes(x =predicted, y = ..density..)) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWszystko wydaje siÄ™ byÄ‡ w porzÄ…dku. Problemy z posterior predictive check moÅ¼e Å›wiadczyÄ‡ o tym, Å¼e wybraliÅ›my zÅ‚Ä… funkcjÄ… wiarygodnoÅ›ci lub/i rozkÅ‚ady a priori.\r\nPodsumowanie\r\nUff, przebrneliÅ›my przez estymacjÄ™ bayesowskÄ…. Na szczÄ™Å›cie od teraz bÄ™dziemy stosowaÄ‡ pakiety, ktÃ³re zrobiÄ… to za nas. Co waÅ¼ne, zaprezentowany przeze mnie dziÅ› kod raczej nie nadaje siÄ™ do prawdziwej analizy. Dedykowane oprogramowanie przeprowadza losowanie MCMC w sposÃ³b bardziej optymalny (i skomplikowany). Warto jednak poznaÄ‡ na czym polega metoda estymacji, ktÃ³rej czÄ™sto bÄ™dziemy uÅ¼ywaÄ‡.\r\n\r\n\r\n\r\nChristensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). Bayesian ideas and data analysis: An introduction for scientists and statisticians (p. 146). CRC press.\r\n\r\n\r\nGelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science, 457â€“472.\r\n\r\n\r\nGeyer, C. J. (1992). Practical markov chain monte carlo. Statistical Science, 473â€“483.\r\n\r\n\r\nLink, W. A., & Eaton, M. J. (2012). On thinning of chains in MCMC. Methods in Ecology and Evolution, 3(1), 112â€“115.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-30-tutorial-bayes-ii/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-12-26T21:37:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-23-tutorial-bayes-iv/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "CzÄ™Å›Ä‡ IV: Wprowadzenie do modeli wielopoziomowych",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-10-30",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nMotywacja\r\nNiewykrycie prawdziego efektu\r\nWykrycie nieprawdziwego efektu\r\nZ czym siÄ™ mierzymy?\r\nCzym jest wiÄ™c model mieszany?\r\n\r\nModele wielopoziomowe\r\nModel z efektem klastra\r\nImplementacja\r\nNiezaleÅ¼noÅ›Ä‡ warunkowa\r\n\r\nBayesowskie dzielenie informacji\r\nModelujmy dalej\r\nModelowanie interceptu i wpspÃ³Å‚czynnika regresji dla klastra\r\nPredyktory na wyÅ¼szych poziomach\r\n\r\nPodsumowanie\r\n\r\n\r\n\r\n\r\nPost bÄ™dzie aktualizowany.\r\n\r\nDzisiaj zajmiemy siÄ™ bayesowskimi wielopopoziomowymi (mieszanymi/hierarchicznymi) modelami liniowymi. Na ten temat poÅ›wiÄ™ca siÄ™ caÅ‚e ksiÄ…Å¼ki, (choÄ‡ z drugiej strony czasami mam wraÅ¼enie, Å¼e traktuje siÄ™ ten temat jak czarnÄ… magiÄ™). Zobaczmy o co caÅ‚e zmieszanie.\r\nMotywacja\r\nZamiast zaczynaÄ‡ od wprowadzenia definicji mieszanego modelu liniowego, przyjrzyjmy siÄ™ poniÅ¼szym sytuacjom.\r\nNiewykrycie prawdziego efektu\r\nZaÅ‚Ã³Å¼my, Å¼e wykonujemy przez pewien czas pomiary zadowolenia z obsÅ‚ugi i wysokoÅ›ci napiwkÃ³w u staÅ‚ych klientÃ³w pewnej restauracji.\r\n\r\n\r\n\r\nWidzimy tu brak zaleÅ¼noÅ›ci liniowej. Teraz zrÃ³bmy tak by na wykresie kolor kropki odpowiadaÅ‚ konkretnemu klientowi.\r\n\r\n\r\n\r\nWidzimy, Å¼e u kaÅ¼dego z klientÃ³w wystÄ™puje zwiÄ…zek liniowy pomiÄ™dzy napiwkiem i zadowoleniem. Jednak poniewaÅ¼, nasze pomiary zagnieÅ¼done sÄ… w klientach, co objawia siÄ™ tym, Å¼e kaÅ¼dy klient Å›rednio daje innÄ… wartoÅ›Ä‡ napiwku, przeoczylibyÅ›my ten zwiÄ…zek, nie uwzglÄ™dniajÄ…c tego w modelu.\r\nWykrycie nieprawdziwego efektu\r\nCzy moÅ¼e nam siÄ™ zdarzyÄ‡ sytuacja odwrotna? UÅ¼ywajÄ…c prostej analizy liniowej wykryjemy efekt, ktÃ³rego nie ma? ZaÅ‚Ã³Å¼my, Å¼e chcemy oceniÄ‡ wpÅ‚yw picia napojÃ³w kofeinowych na wyniki ucznia w nauce. ZebraliÅ›my dane z trzech klas liceum pytajÄ…c o Å›rednie spoÅ¼ycie napojÃ³w kofeinowych dziennie i Å›redniÄ… ocen.\r\n\r\n\r\n\r\nZaznaczmy na wykresie kolorami uczniÃ³w przynaleÅ¼Ä…cych do danej klasy.\r\n\r\n\r\n\r\nJak widzimy, w poszegÃ³lnych klasach spoÅ¼ycie kofeiny nie ma wpÅ‚ywu na wyniki ucznia w nauce. Kiedy moÅ¼emy zaobserwowaÄ‡ takÄ… zaleÅ¼noÅ›Ä‡? ZaÅ‚Ã³Å¼my na przykÅ‚ad, Å¼e kaÅ¼da z klas ma innego wychowawcÄ™ i ich wpÅ‚yw przedkÅ‚ada siÄ™ na Å›redniÄ… ocen. ZaÅ‚Ã³Å¼my teÅ¼, Å¼e zupeÅ‚nie przypadkowo w klasach z lepszym wychowawcÄ…, Å›rednie spoÅ¼ycie napojÃ³w kofeinowych jest wiÄ™ksze. Otrzymamy wtedy sytuacjÄ™ jak na powyÅ¼szych wykresach.\r\nOczywiÅ›cie, moÅ¼e istnieÄ‡ jakaÅ› zaleÅ¼noÅ›Ä‡ pomiÄ™dzy Å›rednim spoÅ¼yciem kofeiny na klasÄ™ a jej Å›redniÄ… ocen lecz to nie odpowiada na pytanie, ktÃ³re zadaliÅ›my tzn. o wpÅ‚yw kofeiny na osiÄ…gniÄ™cia ucznia. Tutaj warto wprowadziÄ‡ sobie rozrÃ³Å¼nienie miÄ™dzy efektem wÄ™wnÄ…trzobiektowym i miÄ™dzyobiektowym. ZrobiÅ‚em to w jednym z poprzednich wpisÃ³w, moÅ¼na przeczytaÄ‡ tutaj.\r\nZ czym siÄ™ mierzymy?\r\nProblem moÅ¼emy zdefiniowaÄ‡ na kika sposobÃ³w. W pierwszej sytuacji chcemy zbadaÄ‡ zwiÄ…zek pomiÄ™dzy napwkami a zadowoleniem z obsÅ‚ugi. Na przeszkodzie jednak staje nam efekt kienta - kaÅ¼dy klient ma inny bazowy Å›redni napiwek. Klient jest tu zmiennÄ… zakÅ‚Ã³cajÄ…cÄ…. By poprawnie wyestymowaÄ‡ wpÅ‚yw zadowolenia na napiwki, moglibyÅ›my kontrolowaÄ‡ wpÅ‚yw poszczegÃ³lnego klienta. W istocie, na tym z grubsza polega mechanizm dziaÅ‚ania mieszanych modeli liniowych.\r\nPrzyjrzyjmy siÄ™ teraz problemowi z innej perspektywy. Gdy chcemy zbadaÄ‡ wpÅ‚yw zmiennej \\(X\\) na Y, na przykÅ‚ad liczÄ…c wspÃ³Å‚czynnik korelacji Pearsona, musimy wykonaÄ‡ nastÄ™pujÄ…ce kroki (ksiÄ…Å¼kowo): Losujemy obserwacje do naszej prÃ³by. NastÄ™pnie, poniewaÅ¼ kaÅ¼da obserwacja miaÅ‚a takie samo prawdopobieÅ„stwo dostania siÄ™ do prÃ³by, traktujemy te obserwacje jako niezaleÅ¼ne zmienne losowe o identycznym rozkÅ‚adzie (iid, independent and identically distributed random variables). Na chÅ‚opski rozum, oznacza to, Å¼e jeÅ›li stworzymy sobie histogram zmiennej X, otrzymamy z grubsza estymatÄ™ rozkÅ‚adu zmiennej \\(X\\) w populacji. W przypadku powyÅ¼szych danych, nie jest to prawda. Przyjrzyjmy sie dokÅ‚adniej tej definicji.\r\nDefinicja skÅ‚ada siÄ™ z dwÃ³ch zasadniczych elementÃ³w. NiezaleÅ¼noÅ›Ä‡ oznacza, Å¼e obserwacje sÄ… niepowiÄ…zane (wylosowanie \\(x_1\\) nie mÃ³wi nam nic o prawdopobieÅ„stwie wylosowania \\(x_2\\)), co czÄ™sto nie jest speÅ‚nione w danych pogrupowanych w klastry, poniewaÅ¼ obserwacje wewnÄ…trz klastra sÄ… do siebie bardziej podobne. Innymi sÅ‚owy obserwacje w klastrze sÄ… ze sobÄ… skorelowane.\r\nDrugÄ… czÄ™Å›ciÄ… definicji jest fakt, Å¼e obserwacje zostaÅ‚y wylosowane z tego samego rozkÅ‚adu. PrzykÅ‚adowo, jeÅ›li wykonujemy pojedynczy pomiar zadowolenia u rÃ³Å¼nych osÃ³b odwiedzajÄ…cych restauracjÄ™, to moÅ¼emy powiedzieÄ‡, Å¼e wszystkie te obserwacje zostaÅ‚y wylosowane z tego samego rozkÅ‚adu zadowolenia (by byÄ‡ dokÅ‚adnym, z tego samego rozkÅ‚adu w danym klastrze, czyli w tej samej restauracji). JednakÅ¼e, w praktyce wielokrotnie wykonujemy pomiary zadowolenia u tych samych osÃ³b, co oznacza, Å¼e zmienna â€œzadowolenieâ€ ma inny rozkÅ‚ad u klienta A, a inny u klienta B.\r\nBy wiÄ™c poprawnie przeanalizowaÄ‡ takie dane, musimy zastosowaÄ‡ model mieszany.\r\nCzym jest wiÄ™c model mieszany?\r\nWikipedia opisuje go tak: Mieszany model liniowy to model statystyczny, ktÃ³ry uwzglÄ™dnia zarÃ³wno efekty staÅ‚e (fixed effects), jak i losowe (random effects) w celu analizy zgrupowanych (clustered) danych.\r\nJest to definicja czÄ™stoÅ›ciowa, niemniej jednak pewnie kaÅ¼dy, kto miaÅ‚ stycznoÅ›Ä‡ z modelami mieszanymi, sÅ‚yszaÅ‚ o efektach staÅ‚ych i losowych. Bez wchodzenia zbytnio w tÄ™ terminologiÄ™, efekty staÅ‚e sÄ… traktowane jako efekty, ktÃ³re majÄ… okreÅ›lonÄ… wartoÅ›Ä‡, ktÃ³rÄ… staramy siÄ™ oszacowaÄ‡, na przykÅ‚ad wpÅ‚yw zadowolenia na wysokoÅ›Ä‡ napiwku. Efekt losowy to efekt, ktÃ³ry jest opisany jakimÅ› rozkÅ‚adem statystycznym, a my napotykamy na jakÄ…Å› jego realizacjÄ™, na przykÅ‚ad hojnoÅ›Ä‡ danego klienta. Jednak gdy przeprowadzimy kolejne badanie na innych klientach, wciÄ…Å¼ estymujemy ten sam zwiÄ…zek miÄ™dzy zadowoleniem a wysokoÅ›ciÄ… napiwku, ale nasi klienci bÄ™dÄ… mieli innÄ… bazowÄ… wartoÅ›Ä‡ hojnoÅ›ci.\r\nMy nie bÄ™dziemy korzystaÄ‡ z tej terminologii. WedÅ‚ug mnie czasami wporowadza ona zamieszanie w rozumieniu tej klasy modeli. Na szczÄ™Å›cie mamy na to bardzo dobre uzasadnienie. Przypomnijcie sobie, Å¼e w statystyce Bayesowskiej wszystkie parametry traktujemy jako rozkÅ‚ady statytyczne. WiÄ™c rozrÃ³Å¼nienie na efekty staÅ‚e i losowe nie ma wiÄ™kszego sensu, poniewaÅ¼ w modelach Bayesowskich wszystkie efekty sÄ… losowe.\r\nModele wielopoziomowe\r\nModel z efektem klastra\r\nJak wiÄ™c bÄ™dzie wyglÄ…daÅ‚ bayesowski model, ktÃ³rym poprawnie moÅ¼emy przeanalizowaÄ‡ dane z powyÅ¼szych przykÅ‚adÃ³w?\r\nW poprzednich czÄ™Å›ciach estymowaliÅ›my regresjÄ™ liniowÄ…. MiaÅ‚a ona miaÅ‚a postaÄ‡:\r\n\\[y_{i} \\sim N(\\mu = \\alpha + \\beta * x_i, \\sigma)\\]\r\nA wszystkie paramtery w tym rÃ³wnaniu miaÅ‚y rozkÅ‚ady a priori ze zdefiniowanymymi przez nas hiperparametrami (czyli takimi parametrami, ktÃ³re sÄ… zdefiniowane arbitralnie, a nie estymowane z danych).\r\nTeraz musimy uwzglÄ™dniÄ‡ w modelu rÃ³Å¼nice pomiÄ™dzy obserwacjami wynikajace z przynaleÅ¼noÅ›ci do klastra:\r\n\\[y_{i} \\sim N(\\mu = \\beta * x_i + k_j , \\sigma)\\]\r\n\\[k_{j} \\sim N(\\alpha, \\tau)\\]\r\n\\[\\alpha \\sim U(-10^{3}, 10^{3})\\]\r\n\\[ \\tau \\sim N(10^{-6}, 10^{3}) \\]\r\n\\[ \\beta \\sim N(0, 10^{2}) \\]\r\n\\[ \\sigma \\sim N(10^{-6}, 10^{3}) \\]\r\nÅ»eby dokÅ‚adnie zrozumieÄ‡ co siÄ™ dzieje, spÃ³jrzmy na diagram naszego nowego modelu.\r\n\r\nÅ»eby zrozumieÄ‡ mechanizm tego modelu, warto zwizualizowaÄ‡ sobie jak jest estymowany (mi to osobiscie pomagaÅ‚o zrozumieÄ‡ jak dziaÅ‚ajÄ… bayesowskie modele). Przy uÅ¼yciu MCMC, najpierw losujemy \\(\\alpha\\) i \\(\\tau\\) z ich rozkÅ‚adÃ³w a priori, a nastÄ™pnie te wartoÅ›ci sÄ… uÅ¼ywane do wylosowania wartoÅ›ci \\(k_j\\) dla kaÅ¼dego klastra, korzystajÄ…c z rozkÅ‚adu normalnego. Na koniec, majÄ…c wylosowane takÅ¼e wartoÅ›ci \\(\\beta\\) i \\(\\sigma\\), sprawdzamy, jak prawdopodobne jest, Å¼e model o takich parametrach wyprodukowaÅ‚ zaobserwowane dane \\(y\\).\r\nParametr \\(k_j\\), ktÃ³ry wystÄ™puje w funckcji wiarygodnoÅ›ci ma rozkÅ‚ad a priori opisany rozkÅ‚adem normalnym o Å›redniej \\(\\alpha\\) i wariancji \\(\\tau\\), ktÃ³ry takÅ¼e jest estymowany z danych, a jego parametry majÄ… wÅ‚asne rozkÅ‚ady a priori. Reszta modelu pozostaje taka sama jak w przypadku standardowej regresji. Zamiast pojedynczego interceptu, model ma teraz \\(j\\) interceptÃ³w, gdzie \\(j\\) to liczba klastrÃ³w.\r\nRozkÅ‚ad a priori \\(P(k_j|\\alpha,\\tau)\\) okreÅ›la prawdopodobieÅ„stwo wystÄ…pienia intercepta o danej wartoÅ›ci, gdzie \\(\\alpha\\) oznacza Å›redni intercept dla caÅ‚ego zbioru danych, a \\(\\tau\\) to wariancja interceptÃ³w.\r\nImplementacja\r\nPosÅ‚uÅ¼ymy siÄ™ danymi (do pobrania tutaj) z drugiego przykÅ‚adu i zaimplementujemy model uÅ¼yciu JAGS.\r\n\r\n\r\nlibrary(rjags)\r\nlibrary(coda)\r\n\r\ncolnames(d) = c(\"Zadowolenie\", \"Klient\", \"Napiwek\")\r\n\r\nmod = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  tau ~ dunif(0.000001,1000)\r\n  \r\n  for (i in 1:5){\r\n  k[i] ~ dnorm(a, tau^-2)\r\n  }\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(Napiwek)) {\r\n    mu[i] <- b * Zadowolenie[i] + k[Klient[i]]\r\n    Napiwek[i] ~ dnorm(mu[i], sigma^-2)\r\n  }}\"\r\n\r\nparams = c(\"a\",\"b\",\"k\",\"tau\",\"sigma\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 6000\r\nnt = 1\r\nnc = 6\r\njmod = jags.model(file = textConnection(mod), data = d, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(jmod, n.iter=ni, by=1)\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:9100\r\nThinning interval = 1 \r\nNumber of chains = 6 \r\nSample size per chain = 6000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean       SD  Naive SE Time-series SE\r\na     16.5100  7.87697 0.0415153      0.0765400\r\nb      0.2777  0.02614 0.0001378      0.0003779\r\nk[1]   6.2376  0.25082 0.0013220      0.0028611\r\nk[2]  11.4953  0.23383 0.0012324      0.0024677\r\nk[3]  16.2927  0.24670 0.0013002      0.0026946\r\nk[4]  21.6225  0.22811 0.0012023      0.0023383\r\nk[5]  26.5587  0.23112 0.0012181      0.0024175\r\nsigma  0.8541  0.06148 0.0003240      0.0003498\r\ntau   13.1714 11.67397 0.0615272      0.2492754\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%     50%     75%   97.5%\r\na      2.3227 13.2364 16.4631 19.6847 30.4752\r\nb      0.2266  0.2602  0.2776  0.2951  0.3298\r\nk[1]   5.7422  6.0706  6.2380  6.4056  6.7278\r\nk[2]  11.0347 11.3398 11.4964 11.6500 11.9617\r\nk[3]  15.8088 16.1289 16.2932 16.4570 16.7785\r\nk[4]  21.1711 21.4704 21.6255 21.7746 22.0685\r\nk[5]  26.1064 26.4034 26.5576 26.7139 27.0142\r\nsigma  0.7443  0.8110  0.8502  0.8936  0.9839\r\ntau    5.2418  8.0037 10.5506 14.7879 35.9838\r\n\r\nJeÅ›li spojrzymy na kwantyle parametru \\(\\beta\\) zauwaÅ¼ymy, Å¼e 0 znajduje siÄ™ poniÅ¼ej 2.5% percentyla. Innymi sÅ‚owy, wykryliÅ›my efekt pomiÄ™dzy napiwkami a zadowoleniem z obsÅ‚ugi.\r\nZauwaÅ¼my teÅ¼, Å¼e model moglibysmy zaimplementowaÄ‡ w nastÄ™pujÄ…cej formie:\r\n\\[y_{i} \\sim N(\\mu =\\alpha +  \\beta * x_i + k_j , \\sigma)\\]\r\n\\[k_{j} \\sim N(0, \\tau)\\]\r\nOtrzymalibyÅ›my dokÅ‚okÅ‚adnie to samo z tÄ… rÃ³Å¼nicÄ…, Å¼e teraz nie \\(k_j\\) nie byÅ‚oby interceptem dla klastra, a wartoÅ›ciÄ… o jakÄ… rÃ³Å¼ni siÄ™ intercept dla klastra od \\(\\alpha\\) (czyli, \\(\\alpha\\) + \\(k_j\\) byÅ‚by intercepem dla klastra \\(j\\).)\r\nWspomniaÅ‚em wyÅ¼ej, Å¼e modele mieszane zasadniczo moÅ¼na interpretowaÄ‡ w kategorii kontrolowania zmiennych zakÅ‚Ã³cajÄ…cych. Policzmy zwykÅ‚Ä… regresjÄ™ (czÄ™stoÅ›ciowÄ…, ale dotyczy to teÅ¼ bayesowskiego odpowiednika), w ktÃ³rej uwzglÄ™dnimy predyktory kategorialne wskazujÄ…ce przynaleÅ¼noÅ›Ä‡ do klastra.\r\n\r\n\r\nlibrary(broom)\r\ntidy(lm(Napiwek ~ Zadowolenie + Klient, d))\r\n\r\n# A tibble: 6 Ã— 5\r\n  term        estimate std.error statistic  p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)    6.23     0.246       25.3 4.42e-45\r\n2 Zadowolenie    0.278    0.0256      10.8 1.67e-18\r\n3 Klient2        5.26     0.261       20.1 9.36e-37\r\n4 Klient3       10.1      0.260       38.6 1.61e-61\r\n5 Klient4       15.4      0.263       58.6 1.23e-78\r\n6 Klient5       20.3      0.262       77.6 1.84e-90\r\n\r\nOtrzymaliÅ›my prawie taki sam wspÃ³Å‚czynnik regresji! RÃ³Å¼ny sposÃ³b ujÄ™cia problemu prowadzi do tego samego rozwiÄ…zania.\r\nOt, caÅ‚a tajemnica modeli mieszanych. W identyczny sposÃ³b moÅ¼emy rozszerzyÄ‡ model, by uwzglÄ™dniaÅ‚ rÃ³Å¼nice we wspÃ³Å‚czynniku regresji pomiÄ™dzy klastrami. BÄ…dÅº zmodyfikowaÄ‡ go tak by uwzglÄ™dniaÅ‚ wiÄ™cej poziomÃ³w i.e.Â klastry wenÄ…trz klastrÃ³w. I taki przykÅ‚ad sobie zrobimy gdy bÄ™dziemy omawiaÄ‡ case studies, ale najpierw opowiedzmy sobie o wÅ‚aÅ›ciwoÅ›ciach modeli wielopoziomowych.\r\nNiezaleÅ¼noÅ›Ä‡ warunkowa\r\nÅÄ…czny rozkÅ‚ad prawdopodobieÅ„stwa post priori naszych parametrÃ³w to:\r\n\\[P(\\beta,k_j,\\sigma,\\alpha,\\tau|y) \\propto P(y|\\beta,k_j,\\sigma) P(k_j|\\alpha,\\tau) P(\\beta)P(\\alpha)P(\\tau)P(\\sigma)\\]\r\nTo na co warto zwrÃ³ciÄ‡ uwagÄ™, to relacje niezaleÅ¼noÅ›ci.\r\n\r\n\r\nWarunkowa niezaleÅ¼noÅ›Ä‡\r\n\r\nNiech \\(A\\), \\(B\\) i \\(C\\) bÄ™dÄ… zdarzeniami. \\(A\\) i \\(B\\) sÄ… warunkowo niezaleÅ¼ne jeÅ›li:\r\n\\[P(A|B,C)= P(A|C) \\]\r\nEkwiwalentnie\r\n\\[P(A,B|C) = P(A|C)P(B|C)\\]\r\n\r\n\r\nCzyli, jak widzimy na diagramie, bezpoÅ›rednio zaleÅ¼ne sÄ… ze sobÄ… tylko elementy poÅ‚Ä…czone strzaÅ‚kami. Na przykÅ‚ad \\(P(y|\\beta,k_j,\\sigma)\\) nie zaleÅ¼y bezpoÅ›rednio od \\(P(\\alpha)\\) poniewaÅ¼:\r\n\\[P(y|\\beta,k_j,\\sigma,\\alpha) = P(y|\\beta,k_j,\\sigma)\\]\r\nInnymi sÅ‚owy, caÅ‚a informacja jakÄ… \\(\\alpha\\) ma o prawdopodobieÅ„stwie \\(y\\), jest juÅ¼ zawarta w \\(k_j\\).\r\nMoÅ¼emy zadaÄ‡ sobie pytanie, skoro ta klasa modeli nazywana jest (teÅ¼) modelami wielopoziomowymi, to gdzie te poziomy. JeÅ›li mamy funkcjÄ™ wiargodnoÅ›ci i rozkÅ‚ady a priori dla parametrÃ³w, ktÃ³re zarzÄ…dzane sa przez wybrane przez badacza wartoÅ›ci hiperparametrÃ³w, to modelujemy dane na 1 poziomie. JeÅ›li jednak parametry rozkÅ‚adÃ³w a priori same majÄ… rozkÅ‚ady a priori wtedy modelujemy dane na 2 (lub wiÄ™cej) poziomach.\r\nBayesowskie dzielenie informacji\r\nRozwaÅ¼my teraz nastÄ™pujÄ…cy problem: zmierzyliÅ›my zmiennÄ… \\(X\\) w rÃ³Å¼nych podgrupach, a jedyne, co nas interesuje, to Å›rednia wartoÅ›Ä‡ zmiennej \\(X\\) w danej podgrupie. Jednak podgrupy rÃ³Å¼niÄ… siÄ™ liczebnoÅ›ciÄ…. Mamy trzy moÅ¼liwoÅ›ci postÄ™powania:\r\nPoliczyÄ‡ Å›redniÄ… dla kaÅ¼dej grupy, ignorujÄ…c pozostaÅ‚e grupy w tym procesie oraz fakt, Å¼e dokÅ‚adnoÅ›Ä‡ Å›rednich bÄ™dzie mniejsza dla mniej licznych grup (no pooling).\r\nPoliczyÄ‡ Å›redniÄ… zmiennej \\(X\\) , uznajÄ…c, Å¼e lepiej jest skorzystaÄ‡ z najpewniejszej estymaty i zignorowaÄ‡ podziaÅ‚ na grupy (full pooling).\r\nUÅ¼yÄ‡ modelu mieszanego, ktÃ³ry pozwoli nam wykorzystaÄ‡ wszystkie dostÄ™pne dane, poprzez wykorzystanie informacji z caÅ‚ej naszej prÃ³by (partial pooling, borrowing information).\r\nW jaki sposÃ³b model mieszany pozwoli nam na to? Dla uproszczenia zdefiniujmy model, w ktÃ³rym interesuje nas prawdopodobieÅ„stwo otrzymania Å›redniej \\(y_{ij}\\) w prÃ³bie dla danej podgrupy \\(j\\). Jako funkcjÄ™ wiarygodnoÅ›ci wybierzmy rozkÅ‚ad normalny:\r\n\\[P(y|\\theta_j, \\sigma_j) \\sim N(\\theta_j , \\sigma_j)\\]\r\nParametry tego rozkÅ‚adu to prawdziwa Å›rednia w grupie \\(\\theta_j\\) oraz odchylenie standardowe w grupie \\(\\sigma_j\\).\r\nJednoczeÅ›nie, rozkÅ‚ad prawdziwych Å›rednich w grupach zamodelujmy nastÄ™pujÄ…co:\r\n\\[\\theta_{j} \\sim N(\\mu, \\tau)\\]\r\ngdzie \\(\\mu\\) to ogÃ³lna Å›rednia zmiennej \\(X\\) , a \\(\\tau\\) to odchylenie standardowe rozkÅ‚adu ogÃ³lnej Å›redniej.\r\nDobrze, to co nas naprawdÄ™ interesuje, to policzenie najlepszej estymaty prawdziwej Å›redniej w grupie \\(\\theta_j\\). Do tego potrzebujemy rozkÅ‚adu \\(P(\\theta_j|\\bar{x_{i}})\\). Aby obliczyÄ‡ szukany przez nas rozkÅ‚ad, moÅ¼emy zastosowaÄ‡ twierdzenie Bayesa:\r\n\\[P(\\theta_j|y) \\propto P(y|\\theta_j)P(\\theta_j)\\]\r\n\r\nRozkÅ‚ad Normalny\r\n\r\nRozkÅ‚ad normalny dany jest wzorem:\r\n\\[x \\sim N(\\mu,\\sigma); \\; \\; P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\r\n  \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\,\\right)\\]\r\n\r\nGdzie \\(\\mu\\) i \\(\\sigma\\) to kolejno Å›rednia i wariancja \\(x\\). PoniewaÅ¼ w statystyce bayesowskiej czÄ™sto wystarczÄ… nam relacje proporcjonalnoÅ›ci moÅ¼emy usnÄ…Ä‡ wszystko, co tylko skaluje nasz rozkÅ‚ad:\r\n\\[P(x) \\propto\r\n  \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\,\\right)\\]\r\n\r\n\r\nFunkcja wiarygodnoÅ›ci naszych danych to iloczyn prawdopdobieÅ„stwa otrzymania kaÅ¼dej obserwacji:\r\n\\[P(y|\\theta_j) = \\prod_{i=1}^{n} \\exp\\left( -\\frac{(y_{ij}-\\theta_j)^2}{\\sigma^2}\\,\\right) = \\exp\\left( -\\frac{\\sum_{i=1}^{n}(y_{ij}-\\theta_j)^2}{2\\sigma^2}\\,\\right)\\]\r\nA wiÄ™c nasz rozkÅ‚ad post priori bÄ™dzie proporcjonalny do:\r\n\\[P(\\theta_i|y) \\propto  \\exp\\left(-\\frac{\\sum_{i=1}^{n}(y_{ij} - \\theta_j)^2}{2\\sigma_j^2}\\right)\\exp\\left(-\\frac{(\\theta_j-\\mu)^2}{2\\tau^2}\\right)\\]\r\nGdy po wykonaniu kilku przeksztaÅ‚ceÅ„ otrzymamy:\r\n\\[P(\\theta_i|y) \\propto \\exp\\left(-\\frac{(\\theta_j - \\hat{\\theta_j})^2}{2S_i^2}\\right)\\]\r\nRozkÅ‚ad normalny jest rozkÅ‚adem zgodym (conjugate prior) dla samego siebie. A wiÄ™c rozkÅ‚ad a posteriori jest rozkÅ‚adem normalnym o Å›redniej \\(\\hat{\\theta_i}\\) i wariancji \\(S^2\\).\r\nGdzie\r\n\\[\\hat{\\theta_j} = \\frac{\\tau^2 \\bar{x_j} + \\frac{\\sigma_j^2}{n}\\mu}{\\tau^2 + \\frac{\\sigma_j^2}{n_j}} = \\left(\\frac{\\tau^2}{\\tau^2 + \\frac{\\sigma_j^2}{n_j}}\\right)\\bar{x_j} + \\left(\\frac{\\frac{\\sigma_j^2}{n_j}}{\\tau^2 + \\frac{\\sigma_j^2}{n}}\\right)\\mu\\]\r\n\\[\\bar{x_j} = \\sum_{i=1}^{n_j}\\frac{y_{ij}}{n_j}\\]\r\n\\[\\frac{1}{S_j^2} = \\frac{1}{\\tau^2} + \\frac{1}{\\frac{\\sigma_j^2}{n}}\\]\r\nCzyli nasz rozkÅ‚ad posteriori \\(\\theta_j\\) ma rozkÅ‚ad o Å›redniej \\(\\hat{\\theta}_{j}\\) i wariancji \\(S^2\\). Zwykle, jeÅ›li chcemy otrzymaÄ‡ punktowÄ… estymatÄ™ parametru z rozkÅ‚adu posteriori, bierzemy jego Å›redniÄ… (w przypadku rozkÅ‚adu normalnego jego Å›rednia ma najwyÅ¼szÄ… wartoÅ›Ä‡ gÄ™stoÅ›ci prawdopodobieÅ„stwa). W tym przypadku bÄ™dzie to \\(\\hat{\\theta_i}\\). MoÅ¼emy zauwaÅ¼yÄ‡, Å¼e wzÃ³r na \\(\\hat{\\theta_i}\\), ktÃ³ry otrzymaliÅ›my, to Å›rednia waÅ¼ona Å›redniej empirycznej w prÃ³bie w danej grupie \\(\\bar{x_j}\\) i ogÃ³lnej Å›redniej w prÃ³bie \\(\\mu\\). MoÅ¼emy zapisaÄ‡ to nastÄ™pujÄ…co:\r\n\\[\\hat{\\theta}_j = \\lambda_j \\bar{x_j} + (1-\\lambda_j)\\mu\\]\r\ngdzie:\r\n\\[\\lambda_j = \\frac{\\tau^2}{(\\tau^2 + \\frac{\\sigma_j^2}{n})}\\]\r\nWidzimy, Å¼e najlepsza estymata Å›redniej w grupie bÄ™dzie siÄ™ rÃ³Å¼niÅ‚a od Å›redniej w prÃ³bie dla grupy \\(j\\) w zaleÅ¼noÅ›ci od tego, jak bardzo \\(\\lambda_i\\) bÄ™dzie mniejsze od 1. BÄ™dzie siÄ™ tak dziaÅ‚o, gdy \\(\\frac{\\sigma_j^2}{n_j}\\) bÄ™dzie duÅ¼y. A kiedy bÄ™dzie duÅ¼y?\r\nIm mniejsza wariancja w danej grupie i im wiÄ™ksza liczebnoÅ›Ä‡ grupy, tym bardziej estymata Å›redniej bÄ™dzie siÄ™ zbliÅ¼aÅ‚a do Å›redniej empirycznej w grupie. Natomiast, jeÅ›li wariancja w grupie jest duÅ¼a i/lub grupa ma maÅ‚Ä… liczebnoÅ›Ä‡, estymata \\(\\hat{\\theta}_i\\) bÄ™dzie bardziej przesuniÄ™ta w stronÄ™ \\(\\mu\\), poniewaÅ¼ prawdopodobieÅ„stwo, Å¼e Å›rednia w grupie \\(j\\) znacznie odbiega od Å›redniej ogÃ³lnej, jest mniejsze (przypomnijmy sobie przykÅ‚ad z ksiÄ™garniÄ… z pierwszej czÄ™Å›ci tutorialu).\r\n\\(\\lambda_i\\) nazywamy wspÃ³Å‚cznynnikiem Å›ciÄ…gania (shrinkage coefficient). ChoÄ‡ jego formuÅ‚a nie bÄ™dzie zawsze wyglÄ…daÅ‚a tak elegancko, jak w tym przypdaku (gdzie rozkÅ‚ad a priori i funkcja wiarygodnoÅ›ci dane sÄ… rozkÅ‚adami normalnymi), widzimy, Å¼e model mieszany pozwala nam na estymacjÄ™ Å›rednich w grupach z wiÄ™kszÄ… pewnoÅ›ciÄ…, poniewaÅ¼ wykorzystujemy do tego obserwacje ze wszystkich podgrup!\r\nTa wÅ‚asnoÅ›Ä‡ pozwala nam nie tylko na uzyskanie wiÄ™kszej pewnoÅ›ci naszych estymat, ale takÅ¼e na nie martwienie siÄ™ problemem wielokrotnych porÃ³wnaÅ„. Model w naturalny sposÃ³b przybliÅ¼a Å›rednie w grupach do Å›redniej ogÃ³lnej gdy wariancja grup \\(\\tau^2\\) zbliÅ¼a sie do 0. Nie bÄ™dziemy omawiaÄ‡ tego dokÅ‚adniej, ale Andrew Gelman opisaÅ‚ to w tym artykule (Gelman et al., 2012), ktÃ³ry wam polecam.\r\nModelujmy dalej\r\nZobaczmy teraz nieco bardziej skomplikowane przykÅ‚ady modelu wielopoziomowego.\r\nRadon jest radioaktywnym gazem szlachetnym, drugÄ… po paleniu tytoniu, przyczynÄ… raka pÅ‚uc. Zanieczyszcza on wnÄ™trza budynkÃ³w przedostajÄ…c siÄ™ do Å›rodka przez maÅ‚e szczeliny i otwory, stwarzajÄ…c zagroÅ¼enie dla ludzi.\r\nMamy dane z 919 domÃ³w z 85 hrabstw, pomiar radonu w mieszkaniach oraz infromacje czy dom ma piwnicÄ™ (0) czy nie (1). Posiadamy takÅ¼e pomiar Å›redniego stÄ™Å¼enia uranu w glebie hrabstwa (Dane moÅ¼ecie je pobraÄ‡ tutaj). Zobaczmy je.\r\n\r\n\r\ndata = read.csv(\"Radon_Data.csv\")\r\n\r\nhead(data)\r\n\r\n  county floor     radon    uranium\r\n1      1     1 0.7884574 -0.6890476\r\n2      1     0 0.7884574 -0.6890476\r\n3      1     0 1.0647107 -0.6890476\r\n4      1     0 0.0000000 -0.6890476\r\n5      2     0 1.1314021 -0.8473129\r\n6      2     0 0.9162907 -0.8473129\r\n\r\nModelowanie interceptu i wpspÃ³Å‚czynnika regresji dla klastra\r\nRadon uwalnia siÄ™ z gleby, wiÄ™c chcemy dowiedzieÄ‡ siÄ™, jak posiadanie piwnicy wpÅ‚ywa na stÄ™Å¼enie radonu. Nasz model bÄ™dzie miaÅ‚ postaÄ‡:\r\n\\[y_i = N(a_j + b_jx_i,\\sigma)\\]\r\nIndeks \\(i\\) oznacza numer obserwacji, a indeks \\(j\\) - numer hrabstwa. W naszym modelu uwzglÄ™dniamy zarÃ³wno wyraz wolny (intercept), jak i wspÃ³Å‚czynnik regresji dla danego hrabstwa. W ten sposÃ³b modelujemy korelacje pomiÄ™dzy obserwacjami w hrabstwach, ktÃ³re wynikajÄ… nie tylko z bazowego poziomu stÄ™Å¼enia radonu w hrabstwie, ale takÅ¼e z rÃ³Å¼nic w poziomie stÄ™Å¼enia radonu wynikajÄ…cych z posiadania piwnicy.\r\nWarto zwrÃ³ciÄ‡ uwagÄ™ na fakt, Å¼e teraz kaÅ¼de hrabstwo ma swÃ³j wÅ‚asny intercept \\(a_j\\) i wspÃ³Å‚czynnik regresji \\(b_j\\). Prawdopodobnie istnieje pewna zaleÅ¼noÅ›Ä‡ miÄ™dzy tymi parametrami. Na przykÅ‚ad w hrabstwach o wysokim bazowym poziomie radonu (\\(a_j\\)) posiadanie piwnicy moÅ¼e mieÄ‡ niewielki wpÅ‚yw na poziom stÄ™Å¼enia (\\(b_j\\)), a w hrabstwach o niskim bazowym poziomie radonu posiadanie piwnicy moÅ¼e znaczÄ…co wpÅ‚ynÄ…Ä‡ na poziom stÄ™Å¼enia. Taka zaleÅ¼noÅ›Ä‡ rÃ³wnieÅ¼ wpÅ‚ywa na korelacje obserwacji w grupach (hrabstwach), dlatego naleÅ¼y jÄ… uwzglÄ™dniÄ‡ w modelu.\r\nJak to zrobiÄ‡? Aby to zrozumieÄ‡, przyjrzyjmy siÄ™ wielowymiarowemu rozkÅ‚adowi normalnemu. Podobnie jak opisujemy rozkÅ‚ad zmiennej \\(x\\) za pomocÄ… rozkÅ‚adu normalnego, tak moÅ¼emy opisaÄ‡ Å‚Ä…czny rozkÅ‚ad zmiennych \\(x\\) i \\(y\\) za pomocÄ… wielowymiarowego rozkÅ‚adu normalnego. Wielowymiarowy rozkÅ‚ad normalny jest opisany dwoma parametrami: wektorem Å›rednich zmiennych \\(\\mu\\) oraz macierzÄ… kowariancji \\(\\boldsymbol{\\Sigma}\\), ktÃ³ra opisuje zwiÄ…zki miÄ™dzy zmiennymi. JeÅ›li dwie zmienne sÄ… niezaleÅ¼ne, to odpowiednie elementy macierzy kowariancji sÄ… rÃ³wne zero.â€\r\n\r\nWielowymiarowy RozkÅ‚ad Normalny\r\n\r\nWielowymiarowy rozkÅ‚ad normalny opisuje sposÃ³b, w jaki zachowuje siÄ™ \\(m\\) zmiennych losowych, podobnie jak jednowymiarowy rozkÅ‚ad normalny opisuje sposÃ³b, w jaki zachowuje siÄ™ jedna zmienna losowa. Jest on opisany przez wektor Å›rednich \\(\\boldsymbol{\\mu}\\) oraz macierz kowariancji \\(\\boldsymbol{\\Sigma}\\). Wielowymiarowy rozkÅ‚ad normalny oznaczamy symbolem \\(\\mathcal{N}_m(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), gdzie \\(m\\) okreÅ›la wymiarowoÅ›Ä‡ przestrzeni, w ktÃ³rej zmienne losowe majÄ… swoje wartoÅ›ci.\r\n\r\nFunkcja gÄ™stoÅ›ci prawdopodobieÅ„stwa dla \\(m\\)-wymiarowego rozkÅ‚adu normalnego ma postaÄ‡:\r\n\\[f(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{\\frac{m}{2}}\\sqrt{|\\boldsymbol{\\Sigma}|}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right)\\]\r\n\r\ngdzie \\(\\boldsymbol{x}\\) jest \\(m\\)-elementowym wektorem zmiennych losowych. Symbol \\(|\\boldsymbol{\\Sigma}|\\) oznacza wartoÅ›Ä‡ wyznacznika macierzy \\(\\boldsymbol{\\Sigma}\\).\r\n\r\n\r\nA wiÄ™c rokÅ‚ad a priori dla \\(a_j\\), \\(b_j\\) bÄ™dzie wyglÄ…daÅ‚ tak:\r\n\\[ \\begin{aligned}\r\n\\begin{pmatrix}a_j \\\\ b_j \\end{pmatrix} &\\sim \\mathcal{N_m}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\\\\r\n\\end{aligned}\\]\r\nGdzie\r\n\\[\\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\alpha} \\\\ \\boldsymbol{\\beta} \\end{pmatrix}, \\qquad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathrm{Var}(a_j) & \\mathrm{Cov}(a_j,b_j) \\\\ \\mathrm{Cov}(a_j,b_j) & \\mathrm{Var}(b_j) \\end{pmatrix}\\]\r\nOczywiÅ›cie wszystkie parametry z powyÅ¼szego wzoru majÄ… swoje rozkÅ‚ady a priori (moÅ¼ecie zobaczyÄ‡ je w kodzie poniÅ¼ej).\r\nNasz model w JAGS bÄ™dzie wyglÄ…daÅ‚ nastÄ™pujÄ…co:\r\n\r\n\r\nmodel =  \"model {\r\n#Likehood\r\nfor (i in 1:length(radon)){\r\nradon[i] ~ dnorm(radon.hat[i], tau.y)\r\nradon.hat[i] <- a[county[i]]+ b[county[i]]*floor[i]\r\n}\r\n\r\n#Prior dla sigma\r\nsigma.y ~ dunif (0, 1000)\r\ntau.y <- pow(sigma.y, -2)\r\n\r\n#Prior dla alpha\r\nalpha ~ dnorm (0, .0001)\r\n\r\n#Prior dla beta\r\nbeta ~ dnorm (0, .0001)\r\n\r\n#Priory dla a_j i b_j\r\nfor (j in 1:max(county)){\r\nmu[j,1] <- alpha \r\nmu[j,2] <- beta \r\ng[j,1:2] ~ dmnorm(mu[j,1:2],tau.g[1:2,1:2])\r\na[j] <- g[j,1]\r\nb[j] <- g[j,2]\r\n}\r\n\r\n#Prior dla wariancji a_j\r\nsigma.a ~ dunif (0, 100)\r\nsigma.g[1,1] <- pow(sigma.a, 2)\r\n\r\n#Prior dla wariancji b_j\r\nsigma.g[2,2] <- pow(sigma.b, 2)\r\nsigma.b ~ dunif (0, 100)\r\n\r\n#Prior dla wspÃ³czynnika korelacji a_j i b_j\r\nrho ~ dunif (-1, 1)\r\n\r\n#Kowariancja a_j i b_j\r\nsigma.g[1,2] <- rho*sigma.a*sigma.b\r\nsigma.g[2,1] <- sigma.g[1,2]\r\n\r\n#Zamiana na macierz precyzji\r\ntau.g[1:2,1:2] <- inverse(sigma.g[,])\r\n}\"\r\n\r\n\r\nO ile nie interesuje nas relacja miÄ™dzy piÄ™trem a radonem w konkretnym hrabstwie, moÅ¼emy Å›ledziÄ‡ jedynie wektor \\(\\boldsymbol{\\mu}\\) zawierajÄ…cy ogÃ³lny intercept (\\(\\alpha\\)) i wspÃ³Å‚czynnik regresji \\(\\beta\\). Wyestymujmy takÅ¼e \\(\\rho\\), czyli nasz wspÃ³Å‚czynnik korelacji pomiÄ™dzy \\(a_j\\) i \\(b_j\\).\r\n\r\n\r\nparams = c(\"alpha\",\"beta\", \"rho\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 6000\r\nnt = 1\r\nnc = 4\r\njmod = jags.model(file = textConnection(model), data = data, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(jmod, n.iter=ni, by=1)\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:9100\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 6000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean      SD  Naive SE Time-series SE\r\nalpha  1.4630 0.05470 0.0003531      0.0008216\r\nbeta  -0.6866 0.08453 0.0005457      0.0024429\r\nrho   -0.2220 0.40279 0.0026000      0.0213931\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%     50%       75%   97.5%\r\nalpha  1.3563  1.4260  1.4625  1.499288  1.5720\r\nbeta  -0.8456 -0.7444 -0.6896 -0.630995 -0.5148\r\nrho   -0.8533 -0.5100 -0.2832 -0.009378  0.8514\r\n\r\nTak jak siÄ™ spodziewaliÅ›my, im wiÄ™kszy indywidualny intercept, tym mniejszy wspÃ³Å‚czynnik regresji dla posiadania piwnicy. Taki model, ktÃ³ry wÅ‚aÅ›nie stworzyliÅ›my, jest konceptualnym odpowiednikiem modelu z random interceptem i random wspÃ³Å‚czynnikiem w statystyce czÄ™stoÅ›ciowej.\r\nPredyktory na wyÅ¼szych poziomach\r\nZauwaÅ¼my, Å¼e w danych mamy jeszcze informacje o Å›rednim stÄ™Å¼eniu uranu w glebie w danym hrabstwie. Radon powstaje w wyniku rozpadu uranu, wiÄ™c jest to istotny predyktor stÄ™Å¼enia radonu w mieszkaniu. JednakÅ¼e, posiadamy tylko dane dotyczÄ…ce hrabstwa, a nie mieszkania, co wymaga od nas zamodelowania zwiÄ…zku na poziomie klastra. Jak bÄ™dzie wyglÄ…daÅ‚ nasz model? Funkcja wiarygodnoÅ›ci pozostaje taka sama:\r\n\\[y_i = N(a_j + b_jx_i,\\sigma)\\]\r\nale nasz rozkÅ‚ad a priori \\(a_j\\) i \\(b_j\\) bÄ™dzie wyglÄ…daÅ‚ tak:\r\n\\[ \\begin{aligned}\r\n\\begin{pmatrix}a_j \\\\ b_j \\end{pmatrix} &\\sim \\mathcal{N_m}(\\begin{pmatrix} \\boldsymbol{\\alpha + \\lambda u_j} \\\\ \\boldsymbol{\\beta} \\end{pmatrix} \\begin{pmatrix} \\mathrm{Var}(a_j) & \\mathrm{Cov}(a_j,b_j) \\\\ \\mathrm{Cov}(a_j,b_j) & \\mathrm{Var}(b_j) \\end{pmatrix}) \\\\\r\n\\end{aligned}\\]\r\nJak moÅ¼emy zauwaÅ¼yÄ‡, intercept dla danego hrabstwa jest teraz modelowany jako zmienna losowa z rozkÅ‚adu o Å›redniej \\(\\alpha + \\lambda u_j\\). Widzimy teÅ¼ dosyÄ‡ wyraÅºnie, Å¼e modelujemy relacjÄ™ pomiÄ™dzy stÄ™Å¼eniem radonu a Å›rednim stÄ™Å¼eniem uranu na innym poziomie niÅ¼ relacjÄ™ zmiennÄ… oznaczajÄ…cÄ… posiadanie piwnicy. JeÅ›li nazwiemy predyktorem dla pierwszego poziomu zmiennÄ… floor, to znaczy zmiennÄ… na poziomie kaÅ¼dej obserwacji (mieszkania), to Å›rednie stÄ™Å¼enie uranu wpÅ‚ywa tak samo na kaÅ¼de mieszkanie w hrabstwie, a wiÄ™c jest predyktorem drugiego poziomu, to znaczy predyktorem interceptu dla klastra.\r\nMoÅ¼emy dopeÅ‚niÄ‡ skomplikowanie modelu w nastÄ™pujÄ…cy sposÃ³b.\r\n\\[ \\begin{aligned}\r\n\\begin{pmatrix}a_j \\\\ b_j \\end{pmatrix} &\\sim \\mathcal{N_m}(\\begin{pmatrix} \\boldsymbol{\\alpha + \\lambda u_j} \\\\ \\boldsymbol{\\beta + \\gamma u_j} \\end{pmatrix} \\begin{pmatrix} \\mathrm{Var}(a_j) & \\mathrm{Cov}(a_j,b_j) \\\\ \\mathrm{Cov}(a_j,b_j) & \\mathrm{Var}(b_j) \\end{pmatrix}) \\\\\r\n\\end{aligned}\\]\r\nCzym w takim wypadku bÄ™dzie \\(\\gamma\\)? BÄ™dzie nam mÃ³wiÅ‚ jak zmienia siÄ™ wspÃ³Å‚czynnik dla piÄ™tra w zaleÅ¼noÅ›ci od Å›redniego poziomu uranu w hrabstwie. Innymi sÅ‚owy bÄ™dzie wspÃ³Å‚czynnikiem dla interakcji dla predyktora pierwszego poziomu - piÄ™tra i predyktora drugiego poziomu - uranu.\r\nZaimplementujmy taki model.\r\n\r\n\r\nmodel =  \"model {\r\n#Likehood\r\nfor (i in 1:length(radon)){\r\nradon[i] ~ dnorm(radon.hat[i], tau.y)\r\nradon.hat[i] <- a[county[i]]+ b[county[i]]*floor[i]\r\n}\r\n\r\n#Prior dla sigma\r\nsigma.y ~ dunif (0, 1000)\r\ntau.y <- pow(sigma.y, -2)\r\n\r\n#Prior dla alpha\r\nalpha ~ dnorm (0, .0001)\r\n\r\n#Prior dla beta\r\nbeta ~ dnorm (0, .0001)\r\n\r\n#Prior dla lambda\r\nlambda ~ dnorm (0, .0001)\r\n\r\n#Prior dla gamma\r\ngamma ~ dnorm (0, .0001)\r\n\r\n#Priory dla a_j i b_j\r\nfor (j in 1:max(county)){\r\nmu[j,1] <- alpha + lambda*uranium[j]\r\nmu[j,2] <- beta + gamma*uranium[j]\r\ng[j,1:2] ~ dmnorm(mu[j,1:2],tau.g[1:2,1:2])\r\na[j] <- g[j,1]\r\nb[j] <- g[j,2]\r\n}\r\n\r\n#Prior dla wariancji a_j\r\nsigma.a ~ dunif (0, 100)\r\nsigma.g[1,1] <- pow(sigma.a, 2)\r\n\r\n#Prior dla wariancji b_j\r\nsigma.g[2,2] <- pow(sigma.b, 2)\r\nsigma.b ~ dunif (0, 100)\r\n\r\n#Prior dla wspÃ³czynnika korelacji a_j i b_j\r\nrho ~ dunif (-1, 1)\r\n\r\n#Kowariancja a_j i b_j\r\nsigma.g[1,2] <- rho*sigma.a*sigma.b\r\nsigma.g[2,1] <- sigma.g[1,2]\r\n\r\n#Zamiana na macierz precyzji\r\ntau.g[1:2,1:2] <- inverse(sigma.g[,])\r\n}\"\r\n\r\n\r\nBy uÅ¼yÄ‡ tego modelu musimy wyciÄ…gnÄ…Ä‡ uranium z naszych danych i stworzyÄ‡ z niego wektor tylu wartoÅ›ci, ile jest hrabstw.\r\n\r\n\r\nuranium = unique(data %>% select(county,uranium)) %>% arrange(county) %>% select(uranium)\r\ndata = as.list(data)\r\ndata[[4]] = uranium$uranium\r\n\r\n\r\nZaimplementujmy model\r\n\r\n\r\nparams = c(\"alpha\",\"beta\", \"rho\", \"lambda\",\"gamma\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 6000\r\nnt = 1\r\nnc = 4\r\njmod = jags.model(file = textConnection(model), data = data, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(jmod, n.iter=ni, by=1)\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:9100\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 6000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean      SD  Naive SE Time-series SE\r\nalpha   1.4662 0.03733 0.0002409       0.001365\r\nbeta   -0.6720 0.08772 0.0005662       0.003427\r\ngamma  -0.4356 0.22992 0.0014841       0.008282\r\nlambda  0.8169 0.09538 0.0006157       0.003355\r\nrho     0.3093 0.43181 0.0027873       0.030863\r\n\r\n2. Quantiles for each variable:\r\n\r\n          2.5%      25%     50%     75%    97.5%\r\nalpha   1.3962  1.44042  1.4654  1.4915  1.54106\r\nbeta   -0.8353 -0.73408 -0.6755 -0.6171 -0.48892\r\ngamma  -0.8884 -0.58505 -0.4392 -0.2794  0.02015\r\nlambda  0.6301  0.75404  0.8161  0.8799  1.00167\r\nrho    -0.5815  0.00579  0.3332  0.6672  0.96624\r\n\r\nPodsumujmy, co dowiedzieliÅ›my siÄ™ z tego modelu. Åšrednia beta jest ujemna, co oznacza, Å¼e jeÅ›li dom nie posiada piwnicy, to Å›rednie stÄ™Å¼enie radonu jest mniejsze. Åšrednia lambda jest dodatnia, co sugeruje, Å¼e im wyÅ¼szy Å›redni poziom radonu w danym hrabstwie, tym wyÅ¼sze stÄ™Å¼enie radonu w mieszkaniu w tym hrabstwie.\r\nJednak Å›rednia gamma jest ujemna, co oznacza, Å¼e im wyÅ¼sze stÄ™Å¼enie uranu w danym hrabstwie, tym mniejsze stÄ™Å¼enie radonu w domach bez piwnicy. To wydaje siÄ™ byÄ‡ nieco dziwne, ale warto zwrÃ³ciÄ‡ uwagÄ™ na przedziaÅ‚ wiarygodnoÅ›ci. Dla gamma 95% przedziaÅ‚ wiarygodnoÅ›ci zawiera 0, co sugeruje, Å¼e ta estymata moÅ¼e nie byÄ‡ zbyt wiarygodna.\r\nPodsumowanie\r\nZobaczyliÅ›my, jak tworzy siÄ™ modele hierarchiczne w podejÅ›ciu bayesowskim. Niemniej jednak, poniewaÅ¼ ten post skupiaÅ‚ siÄ™ tylko na wybranych aspektach, moÅ¼liwe, Å¼e pozostajecie z niedosytem. W takim przypadku polecam ksiÄ…Å¼kÄ™ Gelmana i Hill o modelach hierarchicznych, w ktÃ³rej w sposÃ³b systematyczny pokazujÄ…, jak modelowaÄ‡ dane wielopoziomowe (Gelman & Hill, 2006).\r\n\r\n\r\n\r\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.\r\n\r\n\r\nGelman, A., Hill, J., & Yajima, M. (2012). Why we (usually) donâ€™t have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5(2), 189â€“211.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-23-tutorial-bayes-iv/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-05-31T10:04:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-31-tutorial-bayes/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "CzÄ™Å›c I: Nabywanie intuicji",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-08-14",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nMaÅ‚y wstÄ™p\r\nPrawdopodobieÅ„stwo\r\nTwierdzenie Bayesa\r\nDyskretny przykÅ‚ad\r\n\r\nPewnoÅ›Ä‡ pomiaru\r\nCzÄ™stoÅ›ciowcy\r\nBayesowcy\r\n\r\nPrzykÅ‚ad praktyczny\r\nPodsumowanie\r\n\r\n\r\n\r\nMaÅ‚y wstÄ™p\r\nPomyÅ›laÅ‚em sobie, Å¼e napiszÄ™ taki maÅ‚y tutorial statystyki Bayesowskiej.\r\nPrÃ³ba wytÅ‚umaczenie czegoÅ› komuÅ› jest jednÄ… z najlepszych metod nauki i weryfikacji swojej wÅ‚asnej wiedzy. W przeciwieÅ„stwie do rozwaÅ¼aÅ„ we wÅ‚asnym umyÅ›le, gdy tÅ‚umaczy siÄ™ innym jakieÅ› zagadnienie, nie moÅ¼na nie odpowiedzieÄ‡ na pewne pytania, czy niektÃ³re rzeczy zgrabnie pominÄ…Ä‡ :)\r\nTutorial nie bÄ™dzie na pewno wyczerpujÄ…cym podejÅ›ciem do tematu. JeÅ›li ktoÅ› potrzebuje metodycznego wprowadzenia do metod bayesowskich, moÅ¼ne znaleÅºÄ‡ je Å›wietnej ksiÄ…Å¼ce â€œBayesian Data Analysisâ€ (Gelman et al., 2021). Tutorial jest raczej pewnego rodzaju przyÅ›pieszonym kursem pozwalajÄ…cy na pÃ³Åºniejsze samodzielne eksplorowanie i zabawÄ™ statystykÄ… BayesowskÄ…. Forma tutorialu wynika z podejÅ›cia w akademii, ktÃ³re dla rozpoczynajÄ…cych karierÄ™ naukowcÃ³w moÅ¼na sprowadziÄ‡ do zdania: najpierw badaj, potem zastanawiaj siÄ™ jak zbadaÄ‡.\r\nOsobiÅ›cie preferujÄ™ uÅ¼ywanie jÄ™zykÃ³w programowania i obliczenia w tym tekÅ›cie bÄ™dÄ… wykonywane za pomocÄ… R, ale wiele z pokazanych tu rzeczy moÅ¼na takÅ¼e wykonaÄ‡ w graficznym programie statystycznym JASP. W zrozumieniu zawartych tu treÅ›ci pomoÅ¼e, jeÅ›li uczestniczyÅ‚eÅ›/Å‚aÅ› wczeÅ›niej w jakimÅ› kursie statystyki.\r\nI jeszcze kilka sÅ‚Ã³w wstÄ™pu o samej statystyce bayesowskiej. Istnieje obiegowa opinia, Å¼e statystyka Bayesowska jest trudniejsza od klasycznej. OsobiÅ›cie wydaje mi siÄ™, Å¼e â€œprostotaâ€ klasycznej statystyki wynika z jej dogmatyzmu. IstniejÄ… instrukcje na zastosowanie odpowiednich testÃ³w do danych, utarÅ‚y siÄ™ schematy, do ktÃ³rych naleÅ¼y siÄ™ stosowaÄ‡, mimo, Å¼e czasami sÄ… arbitralne. Statystyka Bayesowska wymaga wiÄ™cej namysÅ‚u od badacza, poniewaÅ¼ nie istniejÄ… tak ostre â€reguÅ‚y kciukaâ€ jak w statystyce klasycznej.\r\nO statystyce bayesowskiej mÃ³wiÅ‚o siÄ™ ostatnio, jako rozwiÄ…zaniu kryzysu replikacyjnego. Mam pewnÄ… obawÄ™, Å¼e statystyka bayesowska moÅ¼e tak samo popaÅ›Ä‡ w dogmatyzm, jeÅ›li potraktuje siÄ™ jÄ… mechanicznie jak klasycznÄ… statystykÄ™. Nie jestem twardym Bayesowcem, to znaczy nie uwaÅ¼am, Å¼e naleÅ¼y porzuciÄ‡ statystykÄ™ czÄ™stoÅ›ciowÄ… (tÄ… zwykÅ‚Ä…) na rzecz Bayesowskiej. Obie majÄ… pewne zalety i wady. Podzielam zdanie Kevina Rosa, Å¼e dobry wspÃ³Å‚czesny statystyk korzysta z kaÅ¼dego z podejÅ›Ä‡ w zaleÅ¼noÅ›ci od potrzeb.\r\nPrawdopodobieÅ„stwo\r\nWszyscy intuicyjnie czujemy czym jest prawdopodobieÅ„stwo. JeÅ›li chcemy sprawdziÄ‡ czy moneta nie jest obciÄ…Å¼ona i czÄ™Å›ciej wypada reszka niÅ¼ orzeÅ‚, najprostszym sposobem jest dokonanie â€œeksperymentu losowegoâ€ i sprawdzenie ile razy wypadnie reszka spoÅ›rÃ³d wszystkich prÃ³b. Im wiÄ™cej razy rzucimy monetÄ…, tym bardziej bÄ™dziemy pewni otrzymanego wyniku. W statystyce klasycznej wychodzimy z podobnego zaÅ‚oÅ¼enia, formalnie zdefiniowanego tak:\r\n\\[ \\lim_{n\\to\\infty} P(A) = \\frac{k_n(A)}{n} \\]\r\ngdzie \\(k_n(A)\\) to liczba zdarzeÅ„ sprzyjajÄ…cych zdaÅ¼eniu \\(A\\) a \\(n\\) to liczba prÃ³b. SÅ‚ownie moÅ¼emy przedstawiÄ‡ to zaÅ‚oÅ¼enie tak: Gdy liczba prÃ³b dÄ…Å¼y do nieskoÅ„czonoÅ›ci, czÄ™stoÅ›Ä‡ wystÄ…pieÅ„ zdarzeÅ„ \\(A\\) poÅ›rÃ³d wszystkich prÃ³b bÄ™dzie siÄ™ rÃ³wnaÅ‚o prawdopodobieÅ„stwu wystÄ…pienia zdarzenia \\(A\\). Innymi sÅ‚owy im wiÄ™cej razy rzucimy monetÄ…, tym wiÄ™ksze mamy szanse na otrzymanie takiego stosunku reszek do orÅ‚Ã³w, ktÃ³ry jest bliski prawdziwemu prawdopodobieÅ„stwu otrzymania reszki.\r\nW statystyce Bayesowkiej na prawdopodobieÅ„stwo patrzy siÄ™ trochÄ™ inaczej. ZakÅ‚ada siÄ™, Å¼e prawdopodobieÅ„stwo jest miarÄ… pewnoÅ›ci, ktÃ³rÄ… moÅ¼emy przypisaÄ‡ pewnym modelom (hipotezom). Wnioskowanie Bayesowskie moÅ¼na porÃ³wnaÄ‡ do tego, jak wnioskujÄ… ludzie. Mamy jakieÅ› oczekiwania, nastÄ™pnie dostajemy dane i rewidujemy nasze oczekiwania.\r\nWnioskowanie Bayesowskie opiera siÄ™ na twierdzeniu Bayesa:\r\n\\[  P(Hipoteza|Dane) = \\frac{P(Dane|Hipoteza)P(Hipoteza)}{P(Dane)}\\]\r\n\\(P(Hipoteza)\\) nazywane prawdopodobieÅ„stwem a priori, oznacza uprzedniÄ… wiedzÄ™, przed zebraniem danych, ktÃ³re hipotezy sÄ… bardziej prawdopodobne. Innymi sÅ‚owy sÄ… to nasze oczekiwania, na przykÅ‚ad, Å¼e gdy odkrÄ™cimy kran, najprawdopodobniej poleci z niego woda. RÃ³wnieÅ¼ brak wiedzy/oczekiwaÅ„ jest prawdopodobieÅ„stwem a priori. Wtedy wszystkie hipotezy moÅ¼na zamodelowaÄ‡ jako rÃ³wnie prawdopodobne.\r\nCzÅ‚on \\(P(Dane|Hipoteza)\\) (wiarygodnoÅ›Ä‡, likehood) oznacza prawdopodobieÅ„stwo otrzymania danych, Å¼e pod warunkiem wygenerowania ich w zgodzie z danÄ… hipotezÄ… (wygenerowania ich przez okreÅ›lony model). MoÅ¼na powiedzieÄ‡, Å¼e jest to sposÃ³b w jaki aktualizujemy nasze przekonania - tzn. nasz model rzeczywistoÅ›ci (po odkrÄ™ceniu kranu coÅ› z niego leci).\r\nWreszcie czÅ‚on \\(P(Hipoteza|Dane)\\) jest prawdopodobieÅ„stwem a posteriori, czyli prawdopodobieÅ„stwem hipotezy w zaleÅ¼noÅ›ci od posiadanych danych. Innymi sÅ‚owy prawdopodobieÅ„stwo a posteriori mÃ³wi nam jak bardzo zmieniÅ‚y siÄ™ nasze przekonania a priori pod wpÅ‚ywem danych. Czyli jeÅ›li z kranu popÅ‚ynÄ™Å‚o wino, to zrewidowaliÅ›my nasze oczekiwania wzglÄ™dem tego kranu.\r\n\\(P(Dane)\\) jest parametrem skalujÄ…cym (staÅ‚Ä…), bez specjalnej interpretacji. O jego roli przekonamy siÄ™ nieco dalej w tekÅ›cie.\r\nWszystko to wyglÄ…da fajnie, ale juÅ¼ na tym etapie moÅ¼e budziÄ‡ zamieszanie. Czym sÄ… owe prawdopodobieÅ„stwa? Å»eby to dobrze zrozumieÄ‡, przypomnimy sobie czym jest prawdopodobieÅ„stwo warunkowe, a nastÄ™pnie wyprowadzimy wzÃ³r Bayesa.\r\nTwierdzenie Bayesa\r\nPrawdopodobieÅ„stwo zdarzenia \\(A\\) w zaleÅ¼noÅ›ci od zdarzenia B rÃ³wna siÄ™ prawdopodobieÅ„stwu wystÄ…pienia zdarzeÅ„ \\(A\\) i \\(B\\) podzielonym na prawdopodobieÅ„stwo zdarzenia \\(B\\):\r\n\\[P(A|B) = \\frac{P(A \\land B)}{P(B)}\\] By obliczyÄ‡ prawdopodobieÅ„stwo palenia papierosÃ³w pod warunkiem bycia mÄ™Å¼czyznÄ… musimy po prostu podzieliÄ‡ liczbÄ™ mÄ™Å¼czyzn palÄ…cych papierosy przez liczbÄ™ wszystkich mÄ™Å¼czyzn.\r\nZauwaÅ¼my, Å¼e powyÅ¼sze rÃ³wnanie moÅ¼emy przeksztaÅ‚ciÄ‡. Najpierw pomnÃ³Å¼my obie strony rÃ³wnania przez \\(P(B)\\).\r\n\\[  P(A|B)P(B) = {P(A \\land B)}\\] Teraz podzielmy obie strony przez \\(P(A)\\).\r\n\\[  \\frac{P(A|B)P(B)}{P(A)} = \\frac{P(A \\land B)}{P(A)} = P(B|A)\\] To co otrzymaliÅ›my, jest twierdzeniem Bayesa:\r\n\\[  P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\]\r\nDyskretny przykÅ‚ad\r\nBy uÅ›wiadomiÄ‡ sobie przydatnoÅ›Ä‡ twierdzenia Bayesa wykonajmy maÅ‚e Ä‡wiczenie. ZaÅ‚Ã³Å¼my, Å¼e opracowaliÅ›my test pozwalajÄ…cy na wykrycie pewnej choroby z 99% prawdopodobieÅ„stwem u osÃ³b faktycznie chorych. MoÅ¼emy zapisaÄ‡ to w nastÄ™pujÄ…cy sposÃ³b \\(P(Pozytywny|Choroba) = 0.99\\). Test jednak daje teÅ¼ wynik faÅ‚szywie pozytywny u 5% osÃ³b zdrowych \\(P(Pozytywny|\\neg Choroba) = 0.05\\). Wiemy takÅ¼e, Å¼e nosicielem choroby jest 1 procent populacji \\(P(Choroba) = 0.01\\). ChcielibyÅ›my siÄ™ dowiedzieÄ‡ jakie jest prawdopodobieÅ„stwo choroby, jeÅ›li wynik jest pozytywny \\(P(Choroba|Pozytywny)\\). By je obliczyÄ‡ potrzebujemy jeszcze prawdopodobieÅ„stwa otrzymania wyniku pozytywnego. UzyskaÄ‡ je moÅ¼emy za pomocÄ… prawa sumy.\r\n\r\n\r\nPrawo caÅ‚kowitego prawdopodobieÅ„stwa  JeÅ›li \\(B_1\\), \\(B_2\\) â€¦ \\(B_n\\) sÄ… parami rozÅ‚Ä…czne, a ich prawdopodobieÅ„stwo sumy zdarzeÅ„ wynosi 1, to dla dowolnego zdarzenia \\(A\\) zachodzi wzÃ³r:  Dla wartoÅ›ci dyskretnych. \\[P(A)=\\sum_{B}P(A \\land B) \\] Dla wartosci ciÄ…gÅ‚ych. \\[ P(A) = \\int_{}P(A \\land B)dB\\]\r\n\r\n\r\nCzyli, by obliczyÄ‡ intersujÄ…ce nas prawdopodobieÅ„stwo musimy podstawiÄ‡ odpowiednie wartoÅ›ci do wzoru:\r\n\\[P(Choroba|Pozytywny) =\\frac{P(Pozytywny|Choroba)P(Choroba)}{P(Pozytywny) = P(Pozytywny|Choroba)P(Choroba) + P(Pozytywny|\\neg Choroba)P( \\neg Choroba)}\\]\r\nPo podstawieniu do wzoru\r\n\\[\\frac{0.99*0.01}{0.99*0.01 + 0.05*0.99} = 0.166 \\]\r\nTylko 16% procent osÃ³b z pozytywnym testem faktycznie bÄ™dzie miaÅ‚o chorobÄ™. PowyÅ¼szy przykÅ‚ad obrazuje czemu czasami profilaktyczne badania mogÄ… powodowaÄ‡ wiÄ™cej problemÃ³w niÅ¼ ich brak. JeÅ›li osoby byÅ‚by wysyÅ‚ane na badanie bez innych wskazaÅ„ Å›wiadczÄ…cych o chorobie, ponad 85% osÃ³b zdrowych, otrzymaÅ‚oby faÅ‚szywy wynik Å›wiadczÄ…cy o chorobie.\r\nPewnoÅ›Ä‡ pomiaru\r\nRozwaÅ¼my teraz nastÄ™pujÄ…cy problem. Mierzymy IQ 15 osÃ³b i chcemy siÄ™ dowiedzieÄ‡ z jakÄ… pewnoÅ›ciÄ… moÅ¼emy mÃ³wiÄ‡ o Å›redniej IQ z prÃ³by jako estymacie Å›redniej IQ w populacji.\r\nCzÄ™stoÅ›ciowcy\r\nW statystyce klasycznej policzylibyÅ›my bÅ‚Ä…d standardowy i przedziaÅ‚ ufnoÅ›ci zgodnie z paradygmatem Null Hypothesis Significance Testing (NHST). Dla uwidocznienia rÃ³Å¼nic pomiÄ™dzy podejÅ›ciem Bayesowskim a czÄ™stoÅ›ciowym, przypominajmy sobie na jakich zaÅ‚oÅ¼eniach opiera siÄ™ NHST.\r\nZgodnie z Centralnym Twierdzeniem Granicznym Å›rednia niezaleÅ¼nych zmiennych losowych o takim samym rozkÅ‚adzie dÄ…Å¼y o rozkÅ‚adu normalnego gdy liczba zmiennych dÄ…Å¼y do nieskoÅ„czonoÅ›ci. Åšrednia takiego rozkÅ‚adu jest rÃ³wna Å›redniej w populacji, a jej wariancja wynosi \\(\\frac{D^2}{n}\\), gdzie \\(D^2\\) to wariancja w populacji. Zwizualizujmy taki rozkÅ‚ad dla Å›rednich z wielkoÅ›ci prÃ³by rÃ³wnej 15. Wiemy, Å¼e Å›rednia IQ wynosi 100, a odchylenie standardowe 15.\r\n\r\n\r\nlibrary(tidyverse)\r\nbreaks <- qnorm(c(0, .025, .2, .5, .8, .975, 1),100, 15/sqrt(15))\r\nggplot(data.frame(x = c(85, 115)), aes(x)) +\r\nscale_fill_brewer(\"x\") +\r\nstat_function(fun = dnorm,n = 1000, args = list(mean = 100, sd = 15/sqrt(15)),geom = \"area\",\r\n  colour = \"gray30\", alpha = 0.7, aes(fill = after_stat(x) |> cut(!!breaks),\r\n  group = after_scale(fill))) +\r\n  geom_vline(xintercept = c(100 - 15/sqrt(15)*qnorm(.975), 100 + 15/sqrt(15)*qnorm(.975))) +\r\n  labs(x = \"Mean IQ\", y = \"Density\") +\r\n  scale_fill_discrete(name = \"P(X < x)\", labels = c('0.025', '0.2', '0.5', '0.8', '0.975','1'))\r\n\r\n\r\n\r\nStworzyliÅ›my w ten sposÃ³b estymowany rozkÅ‚ad wszystkich moÅ¼liwych Å›rednich IQ z piÄ™tnastoosobowych prÃ³b. Ten rozkÅ‚ad mÃ³wi nam, Å¼e losujÄ…c 15 osobowÄ… prÃ³bÄ™ mamy 95% prawdopodobieÅ„stwo na wylosowanie takiej prÃ³by, w ktÃ³rej Å›rednie IQ znajdzie siÄ™ w przedziale ufnoÅ›ci oznaczonym przez czarne pionowe kreski na wykresie. 95% jest tu wartoÅ›ciÄ… umownÄ…, badacz moÅ¼e przyjÄ…Ä‡ dowolny poziom ufnoÅ›ci (Poziom ufnoÅ›ci to \\(1-\\alpha\\), gdzie \\(\\alpha\\) to prawdopodbieÅ„stwo bÅ‚Ä™du I rodzaju).\r\nZwykle gdy dokonujemy pomiaru, nie znamy Å›redniej i wariancji i dopiero estymujemy je z naszej prÃ³by. JeÅ›li bÄ™dziemy wielokrotnie wykonywaÄ‡ pomiar i wyznaczaÄ‡ przedziaÅ‚y ufnoÅ›ci, moÅ¼emy spodziewaÄ‡ siÄ™, Å¼e w 95% w przedziaÅ‚ach ufnoÅ›ci z prÃ³b znajdzie siÄ™ prawdziwa wartoÅ›Ä‡ Å›redniej IQ.\r\n\r\n\r\nset.seed(999)\r\ndata <- data.frame()\r\n\r\nfor(i in 1:50) {\r\n  sample <- rnorm(15, 100, 15)\r\n  data <- rbind(data,data.frame(Mean = mean(sample), se = sd(sample)/sqrt(15)))\r\n}\r\n\r\ndata$Sample <-1:50\r\n\r\nggplot(data, aes(x = Sample, y = Mean)) +       \r\n  geom_errorbar(aes(ymin = Mean - se*qnorm(.975), ymax = Mean + se*qnorm(.975))) +\r\n  coord_flip() +\r\n  geom_hline(yintercept = 100, colour = \"red\" )\r\n\r\n\r\n\r\nNo dobrze, jaki z pÅ‚ynie wniosek? W idealnej sytuacji, gdy nie ma bÅ‚Ä™dÃ³w systematycznych, wszystkie zaÅ‚oÅ¼enia sÄ… speÅ‚nione i prÃ³ba byÅ‚a wylosowana w sposÃ³b reprezentatywny, kiedy wykonujemy wiele pomiarÃ³w Å›redniej, Å›rednio 95% przedziaÅ‚Ã³w ufnoÅ›ci dla prÃ³by powinno zawieraÄ‡ w sobie prawdziwÄ… Å›redniÄ….\r\nZaÅ‚Ã³Å¼my teraz, Å¼e wykonaliÅ›my pomiar IQ w 15 osobowej prÃ³bie i stwÃ³rzmy sobie takie dane:\r\n\r\n\r\niq <- c(87, 112, 83, 95, 106,105, 123, 107, 103, 98, 137, 114, 90, 90, 117)\r\ndata <- data.frame(IQ = \"Our sample\", Mean = mean(iq), se = sd(iq)/sqrt(length(iq)))\r\nggplot(data, aes(x = IQ, y = Mean)) +        \r\n  geom_point() +\r\n  geom_errorbar(aes(ymin = Mean - se*qnorm(.975), ymax = Mean + se*qnorm(.975))) +\r\n  coord_flip()\r\n\r\n\r\n\r\nJakÄ… daje nam to pewnoÅ›Ä‡ co do naszego pomiaru? Na ile moÅ¼emy byÄ‡ pewni, Å¼e prawdziwa Å›rednia leÅ¼y w tym przedziale ufnoÅ›ci? Tu zaczynajÄ… siÄ™ trudnoÅ›ci.\r\nNHST historycznie powstaÅ‚a do stosowania w warunkach przemysÅ‚owych, gdzie wykonujemy bardzo wiele identycznych badaÅ„, na przykÅ‚ad w kontroli jakoÅ›ci produktÃ³w. Na przykÅ‚ad, w Ikei produkowany jest pewien element do konstrukcji szafek. KaÅ¼dy kolejny wyprodukowany egzemplarz tego elementu rÃ³Å¼ni siÄ™ odrobinÄ™ w dÅ‚ugoÅ›ci od schematu, ale dobrze skalibrowana maszyna przycinajÄ…ca elementy dziaÅ‚a tak, Å¼e te rÃ³Å¼nice nie sÄ… na tyle duÅ¼e by element nie pasowaÅ‚ do szafki.\r\nOsoba odpowiedzialna za kontrolÄ™ jakoÅ›ci regularnie wykonuje pomiary dÅ‚ugoÅ›ci \\(n\\) elementÃ³w kaÅ¼dego dnia, by sprawdziÄ‡ czy Å›rednia dÅ‚ugoÅ›Ä‡ elementÃ³w nie wykracza zaÅ‚oÅ¼onego przedziaÅ‚u bezpiecznej rÃ³Å¼nicy w dÅ‚ugoÅ›ci. ObliczyÅ‚ sobie ile elementÃ³w musi zmierzyÄ‡ dziennie, by przedziaÅ‚ ufnoÅ›ci pokrywaÅ‚ siÄ™ z tÄ… bezpiecznÄ… rÃ³Å¼nicÄ… przy ustalonym poziomie ufnoÅ›ci 90%. JeÅ›li maszyna bÄ™dzie dziaÅ‚aÄ‡ poprawnie, kontroler otrzyma Å›rednio 1 faÅ‚szywy alarm na 10 dni.\r\nZasadniczo, w idealnej sytuacji mamy \\(1-\\alpha\\) prawdopodobieÅ„stwo na wylosowanie takiej prÃ³by, ktÃ³rej przedziaÅ‚ ufnoÅ›ci bÄ™dzie zawieraÅ‚ prawdziwÄ… Å›redniÄ…. CzÄ™sto spotykane, choÄ‡ nie do koÅ„ca zasadne, jest twierdzenie, w danej prÃ³bie istnieje prawdopodobieÅ„stwo rÃ³wne \\(1-\\alpha\\), Å¼e w przedziale zawiera siÄ™ Å›rednia. Takie prawdopodbieÅ„stwo mamy przed wylosowaniem prÃ³by, gdy juÅ¼ jÄ… mamy, nie wiemy co wylosowaliÅ›my.\r\nBayesowcy\r\nNo dobrze. Czas na Bayesowskie podejÅ›cie do problemu. Chcemy obliczyÄ‡ \\(P(Hipoteza|Dane)\\). Co jest jednak naszÄ… hipotezÄ…? O ile w przypadku zmiennej binarnej jak choroba i jej brak jest to proste, tutaj nasza hipoteza dotyczy wartoÅ›ci Å›redniej - wartoÅ›Ä‡ Å›redniej jest raczej zmiennÄ… ciÄ…gÅ‚Ä…, moÅ¼e byÄ‡ dowolna. Dlatego \\(P(Hipoteza|Dane)\\) bÄ™dzie rozkÅ‚adem prawdopodobieÅ„stwa dla kaÅ¼dej moÅ¼liwej wartoÅ›ci Å›redniej.\r\nRozkÅ‚ad IQ jest, zgodnie z teoriÄ…, rozkÅ‚adem normalnym. Dlatego \\(P(Dane|Hipoteza)\\) moÅ¼emy zamodelowaÄ‡ jako prawdopodobieÅ„stwo otrzymania danych pod warunkiem okreÅ›lonych wartoÅ›ci Å›redniej (krtÃ³rÄ… oznaczymy jako \\(\\mu\\)) i odchylenia standardowego (ktÃ³re oznaczymy jako \\(\\sigma\\)) rozkÅ‚adu normalnego, ktÃ³re nazywamy parametrami. Gdy mamy duÅ¼o parametrÃ³w, zwykle dla wygody oznaczmy je literkÄ… \\(\\theta\\), ktÃ³ra symbolizuje wektor parametrÃ³w.\r\nMy jednak mamy tylko dwa parametry, wiÄ™c nasze \\(P(Dane|Hipoteza)\\) zapiszemy jako \\(P(y| \\mu ,\\sigma)\\), ktÃ³re jest rozkÅ‚adem prawdopodobieÅ„stwa, mÃ³wi nam jak prawdopodobne jest wylosowanie wartoÅ›ci \\(y\\) z rozkÅ‚adu normalnego \\(N(\\mu,\\sigma)\\). Na przykÅ‚d, gÄ™stoÅ›Ä‡ prawdopodobieÅ„stwa otrzymania wartoÅ›ci 6 z rozkÅ‚adu \\(N(4,2)\\) wynosi:\r\n\r\n\r\ndnorm(6,4,2)\r\n\r\n[1] 0.1209854\r\n\r\nZauwaÅ¼my, Å¼e chcemy by nasza funkcja wiarygodnoÅ›ci dawaÅ‚a prawdopodobieÅ„stwo wszystkich danych jakie mamy. PoniewaÅ¼ dane (obserwacje), ktÃ³re mamy sÄ… losowe i od siebie niezaleÅ¼ne, prawdopodobieÅ„stwo ich wylosowania rÃ³wna siÄ™ ich iloczynowi:\r\n\\[P(A \\land B) = P(A)P(B)\\] Czyli, prawdopodobieÅ„stwo wylosowania caÅ‚ego zbioru danych z rozkÅ‚adu o okreÅ›lonych parametrach bÄ™dzie iloczynem prawdopodobieÅ„stwa wylosowania kaÅ¼dej obserwacji z osobna:\r\n\\[P(y|\\mu,\\sigma) = \\prod_{i=1}^{n}P(y_i|\\mu,\\sigma) = P(y_1|\\mu,\\sigma)*P(y_2|\\mu,\\sigma)...P(y_n|\\mu,\\sigma)\\].\r\nSkoro mamy juÅ¼ naszÄ… funkcjÄ™ wiarygodnoÅ›ci, potrzebujemy jeszcze naszego prawdopodobieÅ„stwa A priori \\(P(\\mu,\\sigma)\\). ZakÅ‚adamy, Å¼e wartoÅ›Ä‡ Å›redniej jest niezaleÅ¼na od wartoÅ›ci odchylenia standardowego, czyli \\(P(\\mu,\\sigma) = P(\\mu) P(\\sigma)\\). Tak jak juÅ¼ wspomniaÅ‚em, prawdopodobieÅ„stwo a priori przedstawia stan wyjÅ›ciowy (np. naszÄ… wczeÅ›niejszÄ… wiedzÄ™).\r\nJakÄ… wiedzÄ™ mamy na temat Å›redniego IQ? Wiemy, Å¼e w populacji wynosi 100 (tak jest skonstruowana ta miara). Raczej nie podejrzewamy duÅ¼ych odstÄ™pÃ³w od tej wartoÅ›ci wiÄ™c moÅ¼emy zamodelowaÄ‡ \\(P(\\mu)\\) rozkÅ‚adem normalnym o Å›redniej 100 i odchyleniu 2 \\(N(100,3)\\). Wiemy teÅ¼, Å¼e odchylenie standardowe wynosi 15, ale na potrzeby tego Ä‡wiczenia udamy, Å¼e tego nie wiemy. A skoro nie mamy Å¼adnej wczeÅ›niejszej wiedzy na temat SD, uznajemy Å¼e kaÅ¼da wartoÅ›Ä‡ SD jest tak samo prawdopodobna i zamodelujemy \\(P(\\sigma)\\) rozkÅ‚adem jednostajnym (prawdopodobieÅ„stwo wylosowania kaÅ¼dej wartoÅ›ci z zadanego przedziaÅ‚u jest takie same).\r\nNasz wzÃ³r moÅ¼emy zapisaÄ‡:\r\n\\[P(\\mu ,\\sigma|y) = \\frac{P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)}{P(y)}\\]\r\nPotrzebujemy jeszcze prawdopodobieÅ„stwa \\(P(y)\\). Ponownie wykorzystujemy prawo sumy, tym razem dla zmiennych ciÄ…gÅ‚ych:\r\n\\[P(y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)d\\mu d\\sigma\\]\r\nChoÄ‡ ta podwÃ³jna caÅ‚ka moÅ¼e wyglÄ…daÄ‡ dla niektÃ³rych przeraÅ¼ajÄ…co, moÅ¼na skonceptualizowaÄ‡ jÄ… sobie jako sumowanie czÅ‚onu \\(P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\) po wszystkich wartoÅ›ciach \\(\\mu\\) i \\(\\sigma\\).\r\nNa marginesie, zasadniczo nie musimy jej obliczaÄ‡ poniewaÅ¼ jest to wartoÅ›Ä‡ staÅ‚a, ktÃ³ra skaluje otrzymany rozkÅ‚ad post priori by suma prawdopodobieÅ„stw rÃ³wnaÅ‚a siÄ™ 1. Dlatego nasz wzÃ³r moÅ¼emy zapisaÄ‡:\r\n\\[P(\\mu ,\\sigma|y) \\propto P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\] Co oznacza, Å¼e nasz rozkÅ‚ad \\(P(\\mu ,\\sigma|y)\\) jest proporcjonalny do \\(P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\).\r\nTeraz moÅ¼emy policzyÄ‡ nasze szukany rozkÅ‚ad prawdopodobieÅ„stwa \\(P(\\mu ,\\sigma|y)\\). W statystyce bayesowskiej popularnoÅ›ciÄ… cieszÄ… siÄ™ rozwiÄ…zania numeryczne, poniewaÅ¼ rozwiÄ…zania analityczne (czyli rozwiÄ…zania danego problemu czysto za pomocÄ… wzorÃ³w) czÄ™sto nie istniejÄ….\r\nBy obliczyÄ‡ nasze rozwiÄ…zanie, teÅ¼ posÅ‚uÅ¼ymy siÄ™ metodÄ… numerycznÄ…. BÄ™dzie ona bardzo prosta, co pozwoli przeÅ›ledziÄ‡ dokÅ‚adnie jak wykonywane sÄ… obliczenia, jednak w praktyce stosuje siÄ™ trochÄ™ bardziej skomplikowane metody, ktÃ³re omÃ³wimy w nastÄ™pnych czÄ™Å›ciach tutorialu.\r\nRÃ³wnania wyglÄ…dajÄ… czasami bardzo abstrakcyjnie, zwÅ‚aszcza jeÅ›li nie moÅ¼na sobie dokÅ‚adnie wyobraziÄ‡ tego co reprezentujÄ…. Kod przydaje siÄ™ w takich momentach. Po kaÅ¼dej wykonanej linijce moÅ¼na sprawdziÄ‡ co nam wyszÅ‚o i powiÄ…zaÄ‡ to z matematycznym zapisem.\r\n\r\n\r\n# Dane z naszej prÃ³by\r\niq <- c(87, 112, 83, 95, 106,105, 123, 107, 103, 98, 137, 114, 90, 90, 117)\r\n\r\n# To sÄ… nasze parametry czyli moÅ¼liwe wartoÅ›ci parametrÃ³w mu i sigma. \r\n# PoniewaÅ¼ jest to proste przybliÅ¼enie numeryczne, ograniczamy moÅ¼liwe wartoÅ›ci Å›redniej \r\n# miÄ™dzy 50 a 150, dla odchylenia standardowego miÄ™dzy 0.1 a 50.\r\npars <- expand.grid(mu = seq(50, 150, length.out = 300), \r\n                    sigma = seq(0.1, 50, length.out = 300))\r\n\r\n# Liczymy marginalne rozkÅ‚ady a priori.\r\npars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 3)\r\npars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)\r\n\r\n# By uzyskaÄ‡ Å‚Ä…czne prawdopodobieÅ„stwo a priori mnoÅ¼ymy je przez siebie.  \r\npars$prior <- pars$mu_prior * pars$sigma_prior\r\n\r\n# Obliczamy wiarygodnoÅ›Ä‡ \r\nfor(i in 1:nrow(pars)) {\r\n  # PrawdopodobieÅ„stwo wylosowania danej obserwacji z rozkÅ‚adu normalnego o parametrach mu i sigma.\r\n  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])\r\n  # MnoÅ¼ymy by uzyskaÄ‡ wiarygodnoÅ›Ä‡\r\n  pars$likelihood[i] <- prod(likelihoods)\r\n}\r\n# MnoÅ¼ymy wiarygodnoÅ›Ä‡ przez prawdopdobieÅ„stwo a priori. \r\npars$probability <- pars$likelihood * pars$prior\r\n\r\n#OtrzymanÄ… wartoÅ›Ä‡ dzielimy przez P(y). \r\npars$probability <- pars$probability/sum(pars$probability)\r\n\r\n# VoilÃ , mamy nasz rozkÅ‚ad prawdopodobieÅ„stwa P(mu, sigma| y)\r\nlibrary(lattice)\r\nlibrary(viridisLite)\r\ncoul <-  viridis(1000)\r\nlevelplot(probability ~ mu * sigma, data = pars, col.regions = coul)\r\n\r\n\r\n\r\nWidzimy tu prawdopodobieÅ„stwo, Å¼e dana kombinacja parametrÃ³w \\(\\mu\\) i \\(\\sigma\\) wygenerowaÅ‚a nasze dane. W odrÃ³Å¼nieniu do statystyki czÄ™stoÅ›ciowej odpowiedziÄ… na nasze pytanie jest nie punktowa wartoÅ›Ä‡ parametru, ale jego rozkÅ‚ad \\(P(\\theta|y)\\). MoÅ¼emy skonstruowaÄ‡ przedziaÅ‚, w ktÃ³rym zawrze siÄ™ 95% prawdopodobieÅ„stwa. Nazywa siÄ™ go przedziaÅ‚em wiarygodnoÅ›ci. W odrÃ³Å¼nieniu do przedziaÅ‚u ufnoÅ›ci, interpretacja przedziaÅ‚u wiarygodnoÅ›ci mÃ³wi: BiorÄ…c pod uwagÄ™ dane, ktÃ³re mamy, oraz prawdopodobieÅ„stwa a priori, mamy 95% prawdopodobieÅ„stwo, Å¼e parametr, ktÃ³ry wygenerowaÅ‚ dane jest w przedziale wiarygodnoÅ›ci.\r\nWeÅºmy rozkÅ‚ad marginalny dla Å›redniej za pomocÄ… prawa sumy:\r\n\r\n\r\nmu = pars %>% group_by(mu) %>% summarise(probability = sum(probability))\r\ncs = cumsum(mu$probability)\r\nboundary_1 = mu$mu[which(cs < 0.025)[length(which(cs < 0.025))]]\r\nboundary_2 = mu$mu[which(cs >= 0.975)[1]]\r\nggplot(mu, aes(x = mu,y = probability))+\r\n  geom_line(colour = 'light blue') +\r\n  xlim(92,111) +\r\n  ylab(\"Density\") +\r\n  geom_area(alpha = 0.75, fill = 'light blue') +\r\n  geom_vline(xintercept = c(boundary_1, boundary_2))\r\n\r\n\r\n\r\nZastanÃ³wmy siÄ™ co by siÄ™ staÅ‚o gdybyÅ›my ponowili nasze badanie. W przypadku statystyki czÄ™stoÅ›ciowej musielibyÅ›my powtÃ³rzyÄ‡ caÅ‚e postÄ™powanie (tak, technicznie moÅ¼emy zebraÄ‡ dane z obu badaÅ„ i policzyÄ‡ z nich przedziaÅ‚ ufnoÅ›ci czy innÄ… statystykÄ™, ale nie byÅ‚oby to zgodne z zaÅ‚oÅ¼eniami testÃ³w statystycznych). W przypadku bayesowskim moÅ¼emy potraktowaÄ‡ nasz rozkÅ‚ad post priori \\(P(\\mu|y)\\) jako rozkÅ‚ad a priori \\(P(\\mu)\\) w nastÄ™pnym badaniu.\r\nÅšrednia IQ w naszej bayesowskiej analizie wyszÅ‚a nieco mniejsza niÅ¼ w czÄ™stoÅ›ciowej, poniewaÅ¼ zastosowaliÅ›my informatywny rozkÅ‚ad a priori. Tu wÅ‚aÅ›nie mamy do czynienia z elementem â€œsubiektywnym.â€ MoÅ¼liwoÅ›Ä‡ arbitralnego doboru wyjÅ›ciowego przekonania co do stanu rzeczy, byÅ‚a dla statystykÃ³w czÄ™stoÅ›ciowych czymÅ› nie do przyjÄ™cia. Jak siÄ™ jednak przekonamy, dobÃ³r prawdopodobieÅ„stwa a prori nie jest aÅ¼ takÄ… arbitralnÄ… decyzjÄ…, a rozkÅ‚ady a priori posiadajÄ… takÅ¼e niesubiektywistycznÄ… interpretacjÄ™.\r\nNo dobrze, wynikiem analizy czÄ™stoÅ›ciowej i bayesowskiej jest jakiÅ› przedziaÅ‚. MoÅ¼e interpretacje siÄ™ rÃ³Å¼niÄ…, ale czy w praktyce nie wychodzi na to samo? OtÃ³Å¼ nie. PrzedziaÅ‚u wiarygodnoÅ›ci uÅ¼ywamy w rÃ³Å¼nych sytuacjach, ale statystyka bayesowska nie sprowadza siÄ™ do bayesowskiego p-value.\r\nPrzykÅ‚ad praktyczny\r\nZaÅ‚Ã³Å¼my, Å¼e korzystamy z internetowej ksiÄ™garni, ktÃ³ra ma system oceny ksiÄ…Å¼ek przez czytelnikÃ³w. Dla uproszczenia zaÅ‚Ã³Å¼my, Å¼e internauci mogÄ… jedynie poleciÄ‡ ksiÄ…Å¼kÄ™ lub jÄ… nie poleciÄ‡. WybraliÅ›my sobie trzy ksiÄ…Å¼ki, ktÃ³re nas interesujÄ…, ale staÄ‡ nas na tylko jednÄ…. Chcemy wiÄ™c dokonaÄ‡ finalnego wyboru sugerujÄ…c siÄ™ ocenami innych. KsiÄ…Å¼kÄ™ A poleca 8 z 10 czytajÄ…cych, ksiÄ…Å¼kÄ™ B poleca 35 z 50, a ksiÄ…Å¼kÄ™ C poleca 60 z 100.\r\nOdsetek poleceÅ„ ksiÄ…Å¼ki w stosunku do wszystkich gÅ‚osÃ³w, moÅ¼emy potraktowaÄ‡ jako ocenÄ™ jakoÅ›ci ksiÄ…Å¼ki. Chcemy wiÄ™c, by ta wartoÅ›Ä‡ byÅ‚a jak najwiÄ™ksza. Ale jednoczeÅ›nie chcemy nasza miara jakoÅ›ci byÅ‚a jak najbardziej pewna. A jaka jest najbardziej pewna? Ta ktÃ³ra ma najwiÄ™cej gÅ‚osÃ³w (danych). Jak wiÄ™c wybraÄ‡ ksiÄ…Å¼kÄ™, ktÃ³ra ma najwiÄ™ksze prawdopodobieÅ„stwa bycia dobrÄ… lekturÄ…? ZauwaÅ¼my, Å¼e prawdopodobieÅ„stwo wylosowania \\(k\\) ocen pozytywnych z \\(n\\) wszystkich ocen danej ksiÄ…Å¼ki, jeÅ›li ksiÄ…Å¼ka ma prawdopodobieÅ„stwo \\(p\\) otrzymania recenzji pozytywnej moÅ¼emy zamodelowaÄ‡ rozkÅ‚adem dwumianowym.\r\n\r\nRozkÅ‚ad dwumianowy  RozkÅ‚ad dwumianowy modeluje prawdopodobieÅ„stwo uzyskania \\(k\\) sukcesÃ³w z \\(n\\) prÃ³b, gdy prawdopodobieÅ„stwo sukcesu wynosi \\(p\\). Dany jest wzorem: \\[ P(k)  =  {{n}\\choose{k}} \\cdot p^k(1-p)^{n-k}\\]\r\n\r\n W statystyce klasycznej najlepszym estymatorem \\(p\\) jest po prostu \\(\\frac{k}{n}\\). Nie uwzglÄ™dnia to jednak niepewnoÅ›ci pomiaru. My jednak chcielibyÅ›my podejÅ›Ä‡ do problemu bayesowsko. Musimy wiÄ™c zdefiniowaÄ‡ wszystkie potrzebne zmienne.\r\nZmiennÄ…, ktÃ³rej rozkÅ‚ad chcemy poznaÄ‡ jest \\(p\\), dlatego \\(p\\) bÄ™dzie naszÄ… hipotezÄ…. Dane to recenzje pozytywne i negatywne. Mamy juÅ¼ dobrego kandydata na funkcjÄ™ wiarygodnoÅ›ci. PrawdopodobieÅ„stwo uzyskania \\(k\\) poleceÅ„ na \\(n\\) ocen pod warunkiem konkretnej wartoÅ›ci \\(p\\) zamodelujemy rozkÅ‚adem dwumianowym.\r\n\\[P(k,n|p) = {{n}\\choose{k}} \\cdot p^k(1-p)^{n-k}\\]\r\nPotrzebujemy jeszcze prawdopodobieÅ„stwa a priori \\(P(p)\\). ZauwaÅ¼my, Å¼e poniewaÅ¼ szukane przez nas \\(p\\) jest prawdopodobieÅ„stwem, moÅ¼e przyjmowaÄ‡ dowolne wartoÅ›ci z przedziaÅ‚u <0,1>. Do modelowania prawdopodobieÅ„stwa prawdopodobieÅ„stwa czÄ™sto wykorzystuje siÄ™ rozkÅ‚ad Beta.\r\n\r\nRozkÅ‚ad Beta  RozkÅ‚ad Beta to rozkÅ‚ad prawdopodobieÅ„stwa, ktÃ³rego noÅ›nik (przedziaÅ‚ dla ktÃ³rego funkcja zwraca wartoÅ›ci wiÄ™ksze od 0) to <0,1>. RozkÅ‚ad ma dwa parametry ksztaÅ‚tu \\(\\alpha\\) i \\(\\beta\\). Dany jest wzorem: \\[ P(x)  =  \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\] W mianowniku mamy funkcjÄ™ Beta. Dla wygody zapisuje siÄ™ jako \\(B(\\alpha,\\beta)\\).  WartoÅ›Ä‡ oczekiwana rozkÅ‚adu Beta to: \\[E(x) = \\frac{\\alpha}{\\alpha+ \\beta}\\]  Na wykresie ksztaÅ‚t rozkÅ‚adu dla rÃ³Å¼nej wartoÅ›ci parametrÃ³w. \r\n\r\nA wiÄ™c \\(P(p)\\) moÅ¼emy zamodelowaÄ‡ rozkÅ‚adem Beta.\r\n\\[ P(p)  =  \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\\]\r\nCo wiemy parametrach tego rozkÅ‚adu? Prawdopodobnie nic, ale moÅ¼emy zastosowaÄ‡ metodÄ™ zwanÄ… empirycznym Bayesem. To znaczy, wyestymujemy rozkÅ‚ad a priori z danych. Nie jest to ortodoksyjne podejÅ›cie Bayesowskie, ktÃ³re zakÅ‚ada zdefiniowanie rozkÅ‚adÃ³w a priori przed spojrzeniem na dane. Niemniej, czÅ‚owiek musi sobie jakoÅ› radziÄ‡.\r\nKsiÄ™garnia internetowa posiada oceny wszystkich ksiÄ…Å¼ek, jakie sÄ… w jej posiadaniu. ZaÅ‚Ã³Å¼my, Å¼e mamy dostÄ™p do tych danych (zasymulujmy je) i stwÃ³rzmy histogram wszystkich ocen.\r\n\r\n\r\nset.seed(123)\r\nx = rbeta(10000,10,10)\r\nhist(x)\r\n\r\n\r\n\r\nNa histogramie widzimy jak czÄ™sto pojawiajÄ… siÄ™ dane oceny. RozkÅ‚ad jest skupiony symetrycznie wokÃ³Å‚ wartoÅ›ci 0.5, ktÃ³re wystÄ™pujÄ… najczÄ™Å›ciej. Im wartoÅ›ci bardziej oddalone od 0.5 tym rzadziej wystÄ™pujÄ…. Oznacza to, Å¼e ksiÄ…Å¼ki bardzo dobre, albo bardzo zÅ‚e sÄ… rzadsze od przeciÄ™tnych.\r\nTen rozkÅ‚ad jest sensownym wyborem rozkÅ‚adu a priori. MÃ³wi nam jak prawdopodobne jest, Å¼e wylosujemy ksiÄ…Å¼kÄ™ o danej ocenie, zanim zobaczymy ocenÄ™ konkretnej pozycji.\r\nWykorzystajmy ten rozkÅ‚ad by dobraÄ‡ wartoÅ›ci \\(\\alpha\\) i \\(\\beta\\). Wyestymujmy parametry rozkÅ‚adu z danych:\r\n\r\n\r\nfit <- function(pars,x) {-sum(log(dbeta(x,pars[1],pars[2])))}\r\nstart <- c(1,1)\r\nnames(start) <-c('a','b')\r\nrecov <- nlminb(start, fit, x = x, lower = -Inf, upper = Inf)\r\nround(recov$par, 2)\r\n\r\n   a    b \r\n9.95 9.96 \r\n\r\nFantastycznie. Mamy wszystko czego potrzebujemy by obliczyÄ‡ nasz rozkÅ‚ad:\r\n\\[P(p|k,n) = \\frac{P(k,n|p)P(p)}{P(k,n)} \\] ZdradzÄ™ wam teraz, Å¼e rozwiÄ…zaniem jest takie, Å¼e \\(P(p|k,n)\\) jest rozkÅ‚adem Beta o parametrach \\(\\alpha = \\alpha_0 + k\\) i \\(\\beta = \\beta_0 + n - k\\), gdzie \\(\\alpha_0\\) i \\(\\beta_0\\) to parametry rozkÅ‚adu a priori.\r\nAnalitycznÄ… forma \\(P(p|k,n)\\) istnieje. RozkÅ‚ad Beta jest zgodnym rozkÅ‚adem a priori  (conjugate prior) dla rozkÅ‚adu dwumianowego. To znaczy, Å¼e w tym wypadku nie musimy przybliÅ¼aÄ‡ rozwiÄ…zania metodami numerycznymi, ale moÅ¼emy je wprost obliczyÄ‡.\r\nJeÅ›li kogoÅ› interesujÄ… obliczenia, ktÃ³re nie sÄ… trudne, a mogÄ… pomÃ³c zrozumieÄ‡ jak rozkÅ‚ady a priori, post priori i wiarygodnoÅ›Ä‡ sÄ… powiÄ…zane, zapraszam do rozwiniÄ™cia tekstu poniÅ¼ej.\r\nKliknij mnie\r\nPodstawmy wszystkie rozkÅ‚ady pod nasze rÃ³wnanie:\r\n\\[P(p|k,n) =\\frac{{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}}{\\int{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}dp} \\]\r\nNie wyglÄ…da zbyt zachÄ™cajÄ…co, prawda? Ale spokojnie, rozwiÄ…zanie tego rÃ³wnania jest bardzo proste. WeÅºmy wszystkie czÅ‚ony rÃ³wnania, ktÃ³re nie zaleÅ¼Ä… od \\(p\\) i zapiszmy je jako \\(C\\).\r\n\\[P(p|k,n) = C*p^k(1-p)^{n-k}p^{\\alpha-1}(1-p)^{\\beta-1} = C* p^{\\alpha-1 +k}(1-p)^{\\beta-1 + n - k}\\] Gdzie:\r\n\\[ C = \\frac{{{n}\\choose{k}}\\frac{1}{B(\\alpha,\\beta)}}{\\int{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}dp} \\]\r\nNo dobra, teraz maÅ‚a sztuczka. ZauwaÅ¼my teraz, Å¼e rozkÅ‚ad Beta jest proporcjonalny do \\(p^{\\alpha-1}(1-p)^{\\beta-1}\\):\r\n\\[\\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\\propto p^{\\alpha-1}(1-p)^{\\beta-1}\\]\r\nZauwaÅ¼my teÅ¼, Å¼e nasz w szukanym prze nas rozkÅ‚adzie, \\(C\\) nie zaleÅ¼y od \\(p\\), jest wiÄ™c jakÄ…Å› wartoÅ›ciÄ… staÅ‚Ä…, skalujÄ…cÄ… rozkÅ‚ad. \\[P(k,n|p) \\propto p^{\\alpha-1 + k}(1-p)^{\\beta-1 + k - n}\\]\r\nPrzyjmijmy, Å¼e \\(\\alpha = \\alpha_0 +k\\) i \\(\\beta = \\beta_0 + n - k\\), gdzie \\(\\alpha_0\\) i \\(\\beta_0\\) to parametry \\(\\alpha\\) i \\(\\beta\\) z powyÅ¼szego wzoru. Teraz juÅ¼ powinniÅ›my zauwaÅ¼yÄ‡, Å¼e nasz rozkÅ‚ad jest proporcjonalny do rozkÅ‚adu Beta. A poniewaÅ¼ nasz \\(P(p|k,n)\\) jest rozkÅ‚adem prawdopodobieÅ„stwa, to jego suma/caÅ‚ka musi rÃ³wnaÄ‡ siÄ™ 1. RozkÅ‚adem proporcjonalnym do rozkÅ‚adu Beta, ktÃ³rego suma/caÅ‚ka wynosi 1, jest wÅ‚aÅ›nie rozkÅ‚ad Beta, a wiÄ™c:\r\n\\[ C = \\frac{1}{B(\\alpha,\\beta)}\\]\r\nNasz rozkÅ‚ad a posteriori \\(P(p,|k,n)\\) jest rozkÅ‚adem Beta o parametrach \\(\\alpha = \\alpha_0 + k\\) i \\(\\beta = \\beta_0 + n - k\\).\r\nWiemy juÅ¼, Å¼e rozkÅ‚ady \\(P(p|k,n)\\) dla kaÅ¼dej ksiÄ…Å¼ki to rozkÅ‚ady Beta o nastÄ™pujÄ…cych parametrach:\r\n\\[A_p \\sim Beta(17.95, 11.96)\\] \\[B_p \\sim Beta(39.93, 19.93)\\] \\[C_p \\sim Beta(64.93, 44.93)\\]\r\nZobaczmy je na wykresie razem z wykresem a priori:\r\n\r\n\r\nlibrary(tidyverse)\r\nfun.1 = function(x){dbeta(x,17.95, 11.96)}\r\nfun.2 = function(x){dbeta(x,44.95, 24.96)}\r\nfun.3 = function(x){dbeta(x,69.95, 49.96)}\r\nfun.4 = function(x){dbeta(x,9.95, 9.96)}\r\ncolors <- c(\"A\" = \"blue\", \"B\" = \"green\", \"C\" = \"red\", \"a priori\" = \"black\")\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \r\n  stat_function(fun = fun.1, aes(color = \"A\")) + \r\n  stat_function(fun = fun.2, aes(color = \"B\")) +\r\n  stat_function(fun = fun.3, aes(color = \"C\")) +\r\n  stat_function(fun = fun.4, aes(color = \"A priori\")) +\r\n  xlab(\"p\") + ylab(\"Density\") + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"RozkÅ‚ad\", values = colors) + \r\n  xlim(0,1) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nCzarna linia to wykres a priori. Symbolizuje naszÄ… wiedzÄ™ na poczÄ…tku, jak prawdopodobne jest wybranie losowej ksiÄ…Å¼ki o danej ocenie. PozostaÅ‚em linie pokazujÄ… jak zmieniÅ‚y siÄ™ nasze przekonania w stosunku do konkretnych ksiÄ…Å¼ek po zobaczeniu ich oceny.\r\nMamy rozkÅ‚ady, ale chcemy podjÄ…Ä‡ decyzjÄ™, a do tego potrzebujemy mieÄ‡ punktowe estymaty \\(p\\). MoÅ¼emy jÄ… otrzymaÄ‡ na kilka sposobÃ³w. MoÅ¼emy wziÄ…Ä‡ takie \\(p\\), ktÃ³re ma najwiÄ™ksze prawdopodobieÅ„stwo. InnÄ… popularnÄ… estymatÄ… jest Å›rednia, poniewaÅ¼ minimalizuje ona kwadratowÄ… funkcjÄ™ bÅ‚Ä™du \\(l(p, \\hat{p}) = E((p - \\hat{p})^2)\\). Ponadto gdy nasz rozkÅ‚ad post priori jest symetryczny, obie te wartoÅ›ci sÄ… rÃ³wne.\r\nPoliczmy sobie Å›rednie rozkÅ‚adÃ³w post priori.\r\n\\[E(A_p) = 0.6\\] \\[E(B_p) = 0.64\\] \\[E(C_p) = 0.58\\]\r\nJak widzimy, ksiÄ…Å¼ka B ma najwiÄ™ksze prawdopodobieÅ„stwo bycia najlepszÄ… ksiÄ…Å¼kÄ…. Co tu siÄ™ wÅ‚aÅ›ciwie zadziaÅ‚o? Wszystkie prawdopodobieÅ„stwa sÄ… mniejsze, niÅ¼ gdybyÅ›my wziÄ™li po prostu czÄ™stoÅ›ciowÄ… estymatÄ™ \\(\\frac{k}{n}\\).\r\nEstymata bayesowska oceny jest mniejsza od czÄ™stoÅ›ciowego odpowiednika dla kaÅ¼dej z ksiÄ…Å¼ek. Dzieje siÄ™ tak dlatego, Å¼e rozkÅ‚ad a priori wskazuje, Å¼e prawdopodobieÅ„stwa bliÅ¼sze 0.5 - Å›redniej rozkÅ‚adu a priori sÄ… bardziej prawdopodobne i â€œÅ›ciÄ…gaâ€ rozkÅ‚ady post priori w swojÄ… stronÄ™. JednoczeÅ›nie bayesowska ocena ksiÄ…Å¼ki C najmniej siÄ™ rÃ³Å¼ni od estymaty czÄ™stoÅ›ciowej, dlatego, Å¼e ocena ksiÄ…Å¼ki C skÅ‚ada siÄ™ z najwiÄ™kszej iloÅ›ci gÅ‚osÃ³w (obserwacji).\r\nÅ»eby to zwizualizowaÄ‡, zaÅ‚Ã³Å¼my teraz, Å¼e wszystkie trzy ksiÄ…Å¼ki majÄ… 0.8 pozytywnych ocen, ale rÃ³Å¼niÄ… siÄ™ liczebnoÅ›ciÄ… jak w poprzednim przykÅ‚adzie i popatrzmy na wykres:\r\n\r\n\r\nlibrary(tidyverse)\r\nfun.1 = function(x){dbeta(x,17.95, 11.96)}\r\nfun.2 = function(x){dbeta(x,49.95, 19.96)}\r\nfun.3 = function(x){dbeta(x,89.95, 29.96)}\r\nfun.4 = function(x){dbeta(x,9.95, 9.96)}\r\ncolors <- c(\"A\" = \"blue\", \"B\" = \"green\", \"C\" = \"red\", \"a priori\" = \"black\")\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \r\n  stat_function(fun = fun.1, aes(color = \"A\")) + \r\n  stat_function(fun = fun.2, aes(color = \"B\")) +\r\n  stat_function(fun = fun.3, aes(color = \"C\")) +\r\n  stat_function(fun = fun.4, aes(color = \"A priori\")) +\r\n  xlab(\"p\") + ylab(\"Density\") + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"RozkÅ‚ad\", values = colors) + \r\n  xlim(0,1) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nIm wiÄ™cej mamy danych (obserwacji), tym mniej istotne staje siÄ™ prawdopodobieÅ„stwo a priori. JednoczeÅ›nie im bardziej informatywny rozkÅ‚ad a priori (im wÄ™Å¼szy, im wiÄ™cej gÄ™stoÅ›ci prawdopodobieÅ„stwa jest skupione wokÃ³Å‚ danej wartoÅ›ci) tym wiÄ™cej danych trzeba by rozkÅ‚ad post priori rÃ³Å¼niÅ‚ siÄ™ od niego.\r\nPodsumowanie\r\nW statystyce czÄ™stoÅ›ciowej dane sÄ… modelowane jako zmienne losowe, a parametry traktowane sÄ… jako staÅ‚e. W analizie staramy siÄ™ oszacowaÄ‡ najlepszÄ… moÅ¼liwÄ… punktowÄ… wartoÅ›Ä‡ tego parametru. Jak zauwaÅ¼yliÅ›my, analiza Bayesowska zwraca nam rozkÅ‚ad prawdopodobieÅ„stwa parametru, ktÃ³rego szukamy. RozkÅ‚ad reprezentuje nasz poziom pewnoÅ›ci co do wartoÅ›ci parametru. W podejÅ›ciu Bayesowskim dane sÄ… staÅ‚e, a parmetry sÄ… zmiennymi losowymi. To znaczy, Å¼e szukane przez nas parametry reprezentujemy jako rozkÅ‚ad statystyczny, pod warunkiem zebranych przez nas danych. Gdy otrzymujemy nowe dane, moÅ¼emy aktualizowaÄ‡ naszÄ… pewnoÅ›Ä‡ co do prawdopodobieÅ„stwa konkretnych wartoÅ›ci parametrÃ³w.\r\nMam nadziejÄ™, Å¼e pokazanymi tu przykÅ‚adami udaÅ‚o mi siÄ™ pokazaÄ‡ na czym polega wnioskowanie Bayesowskie. W nastÄ™pnej czÄ™Å›ci zajmiemy siÄ™ budowaniem i estymowaniem modeli Bayesowskich.\r\n\r\n\r\n\r\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2021). Bayesian data analysis. Chapman; Hall/CRC.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-31-tutorial-bayes/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-12-24T13:48:41+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-13-mixedmotivation/",
    "title": "Dlaczego liniowe modele mieszane?",
    "description": "MaÅ‚a zachÄ™ta do stosowania mieszanych modeli liniowych.",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-05-14",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nDzisiejszy wpis bÄ™dzie takÄ… mini motywacjÄ… do zainteresowania siÄ™ mieszanymi modelami liniowymi (aka hierarchicznymi/wielopoziomowymi modelami liniowymi).\r\nRozwaÅ¼my hipotetyczny eksperyment. Badane osoby oglÄ…dajÄ… rÃ³Å¼ne zdjÄ™cia, a po po obejrzeniu oceniajÄ… jak bardzo zdjÄ™cie byÅ‚o pobudzajÄ…ce. W trakcie badania uczesnicy majÄ… na palcu wskazujÄ…cym i Å›rodkowym elektrody mierzÄ…ce aktywnoÅ›Ä‡ skÃ³rno-galwanicznÄ… (mikropocenie).\r\nNastÄ™pnie moÅ¼emy zadaÄ‡ pytanie czy wiÄ™ksza amplituda sygnaÅ‚u z elektrod jest zwiÄ…zana z wiÄ™kszym pobudzeniem. JeÅ›li pytamy o to, czy osoby z Å›rednio wyÅ¼szÄ… amplitudÄ… sygnaÅ‚u majÄ… wyÅ¼szÄ… Å›redniÄ… ocenÄ™ (efekt miÄ™dzyobiektowy), moÅ¼emy po prostu uÅ›redniÄ‡ pomiary na osobÄ™ i skorelowaÄ‡ je ze sobÄ….\r\nJeÅ›li jednak zapytamy czy Å›rednio im wiÄ™kszy sygnaÅ‚ w pojedynczym pomiarze, tym wyÅ¼sza ocena (efekt wewnÄ…trzobiektowy), takie podejÅ›cie moÅ¼e nie zadziaÅ‚aÄ‡. Czemu? Zasymulujmy sobie takie dane.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(lme4)\r\nlibrary(parameters)\r\nlibrary(datawizard)\r\nlibrary(cowplot)\r\n\r\n\r\nbetween_effect = 0\r\nwithin_effect = 0.5\r\nbetween_sd = 2\r\nwithin_sd = 0.1\r\n\r\nsignal_intercepts = rnorm(100,0,1) \r\nresponse_intercepts = rnorm(100,0, between_sd) + between_effect * signal_intercepts\r\n\r\ndata =  data.frame()\r\nfor (participant in 1:100){\r\n  \r\n  signal = rnorm(100,4,0.1)  \r\n  response =  within_effect*signal + rnorm(100,0,within_sd)\r\n  response = response + response_intercepts[participant]\r\n  signal = signal + signal_intercepts[participant]\r\n  data =  rbind(data, data.frame(signal,response,participant))\r\n  \r\n}\r\ndata_averaged = data %>% group_by(participant) %>% summarise(mean_signal = mean(signal), mean_response = mean(response))\r\n\r\ncat(\"Korelacja pojedynczych pomiarÃ³w\",cor(data$signal, data$response))\r\n\r\n\r\nKorelacja pojedynczych pomiarÃ³w -0.0215814\r\n\r\ncat(\"Korelacja Å›rednich pomiarÃ³w na osobÄ™\",cor(data_averaged$mean_signal, data_averaged$mean_response))\r\n\r\n\r\nKorelacja Å›rednich pomiarÃ³w na osobÄ™ -0.02477697\r\n\r\nNie obserwujemy znaczÄ…cych korelacji w Å¼adnym wypadku. ZerkniÄ™cie na wykres wyjaÅ›ni tajemnicÄ™:\r\n\r\n\r\nplot1 <- ggplot(data, aes(x = signal, y = response, colour =\r\n  as.character(participant))) + geom_point() + theme_bw() + \r\n  theme(legend.position=\"none\")\r\n\r\nplot2 <- ggplot(data_averaged, aes(x = mean_signal, y = mean_response)) +\r\n  geom_point() + theme_bw()\r\n\r\nplot_grid(plot1, plot2, labels = \"AUTO\")\r\n\r\n\r\n\r\n\r\nJeÅ›li spojrzymy na wykres po lewej, zobaczymy rÃ³Å¼nokolorowe zgrupowane punkty. Te kolory to poszczegÃ³lni badani. JeÅ›li siÄ™ im przyjrzymy zauwaÅ¼ymy, Å¼e sÄ… lekko przechylone w prawo, co sugeruje zwiÄ…zek liniowy. Jednak badani sÄ… rozrzuceni po caÅ‚ym wykresie (duÅ¼a wariancja miÄ™dzyobiektowa), poniewaÅ¼ wystÄ™puje duÅ¼e zrÃ³Å¼nicowanie pomiÄ™dzy ich bazowymi amplitudami i ocenami. Wykres ich Å›rednich pokazuje brak zwiÄ…zku, poniewaÅ¼ te bazowe wartoÅ›ci nie sÄ… ze sobÄ… zwiÄ…zane. Jak wiÄ™c wykryÄ‡ zwiÄ…zek?\r\nA gdybyÅ›my tak od wartoÅ›ci naszych zmiennych odjÄ™li Å›redniÄ… dla danego badanego?\r\n\r\n\r\ndata = data %>% group_by(participant) %>% mutate(signal_demeaned = signal - mean(signal), \r\nresponse_demeaned = response - mean(response))\r\n\r\nggplot(data, aes(x = signal_demeaned, y = response_demeaned, colour = as.character(participant))) +\r\ngeom_point() + theme_bw() +  theme(legend.position=\"none\")\r\n\r\n\r\n\r\n\r\nLepiej, co nie? Jak wiÄ™c uwzglÄ™dniÄ‡ to w modelu liniowym?\r\nRegresja liniowa ma postaÄ‡:\r\n\\[ Y_i = \\alpha + \\beta X_i\\] gdzie \\(a\\) to staÅ‚a, a \\(\\beta\\) to wspÃ³Å‚czynnik regresji.\r\nNasz model musimy zmodyfikowaÄ‡ tak by braÅ‚ pod uwagÄ™ Å›rednie amplitudy badanych.\r\n\\[ Y_{ij} = \\alpha + \\beta X_i + b_j z_i\\]\r\ngdzie \\(z_j\\) to zmienna binarna oznaczajÄ…ca czy dany pomiar naleÅ¼y do badanego \\(j\\), a \\(b_j\\) to wspÃ³Å‚czynnik regresji dla \\(z_j\\). PoniewaÅ¼ \\(z_i\\) przyjmuje dla badanego \\(j\\) wartoÅ›Ä‡ 1, a dla wszystkich innych 0, rÃ³wnanie moÅ¼emy przepisaÄ‡:\r\n\\[ Y_{ij} = (\\alpha +b_j) + \\beta X_i\\] Jak widzimy teraz nasz model uwzglÄ™dnia odchylenia od staÅ‚ej dla kaÅ¼dego badanego.\r\nZerknijmy na model mieszany:\r\n\r\n\r\nmodel = lmer(response ~ signal + (1| participant), data = data)\r\nmodel_parameters(model)\r\n\r\n\r\n# Fixed Effects\r\n\r\nParameter   | Coefficient |   SE |        95% CI | t(9996) |      p\r\n-------------------------------------------------------------------\r\n(Intercept) |       -0.02 | 0.19 | [-0.40, 0.36] |   -0.09 | 0.926 \r\nsignal      |        0.51 | 0.01 | [ 0.49, 0.52] |   50.43 | < .001\r\n\r\n# Random Effects\r\n\r\nParameter                   | Coefficient\r\n-----------------------------------------\r\nSD (Intercept: participant) |        1.89\r\nSD (Residual)               |        0.10\r\n\r\nW tabeli Fixed Effects mamy estymaty staÅ‚ej \\(a\\) i wspÃ³Å‚czynnika regresji \\(\\beta\\). NiÅ¼ej w tabeli Random Effects widzimy estymaty odchylenia standardowego staÅ‚ych \\(b\\) (wariancji miÄ™dzyobiektowej) i bÅ‚Ä™du (w tym wypadku wariancji wewnÄ…trzobiektowej). Jak widzimy wspÃ³Å‚czynnik dla sygnaÅ‚u jest pozytywny i wynosi okoÅ‚o 0.5. DziÄ™ki dodaniu dodatkowych staÅ‚ych \\(b_j\\) nasz model liczy teraz efekt wewnÄ…trzobiektowy.\r\nAle czasami chcielibyÅ›my by liczyÅ‚ takÅ¼e efekt miÄ™dzyobiektowy. SpÃ³jrzmy na taki przykÅ‚ad:\r\n\r\n\r\n\r\nJak widzimy na wykresie, wystÄ™puje zarÃ³wno efekt miÄ™dzyobiektowy, jak i wewnÄ…trzobiektowy. Ponadto, te efekty majÄ… przeciwny znak.\r\nPrzykÅ‚adem z Å¼ycia takiej sytuacji jest szybkoÅ›Ä‡ pisania na klawiaturze. Im szybciej Å›rednio dana osoba pisze na klawiaturze tym rzadziej popeÅ‚nia bÅ‚Ä™dy. Jednak kaÅ¼da osoba, im relatywnie szybciej (wzglÄ™dem swojej Å›redniej) pisze, tym wiÄ™cej bÅ‚Ä™dÃ³w popeÅ‚nia.\r\nModel mieszany wykryje tylko efekt wewnÄ…trzobiektowy. JeÅ›li chcielibyÅ›my wykryÄ‡ oba efekty, musimy wykonaÄ‡ pewnÄ… sztuczkÄ™ i rozbiÄ‡ zmiennÄ… niezaleÅ¼nÄ… na dwie (Bell et al., 2019): \\(X_{between}\\) = Å›rednia dla danego badanego i \\(X_{within} = X - X_{between}\\).\r\n\r\n\r\ndata <- cbind(\r\n  data,\r\n  demean(data, select = c(\"signal\"), group = \"participant\")\r\n)\r\n\r\nmodel = lmer(response ~ signal_within + signal_between + (1| participant), data = data)\r\nmodel_parameters(model)\r\n\r\n\r\n# Fixed Effects\r\n\r\nParameter      | Coefficient |   SE |         95% CI | t(395) |      p\r\n----------------------------------------------------------------------\r\n(Intercept)    |       11.71 | 3.19 | [ 5.44, 17.99] |   3.67 | < .001\r\nsignal within  |        0.93 | 0.05 | [ 0.83,  1.03] |  17.86 | < .001\r\nsignal between |       -1.88 | 0.39 | [-2.65, -1.11] |  -4.81 | < .001\r\n\r\n# Random Effects\r\n\r\nParameter                   | Coefficient\r\n-----------------------------------------\r\nSD (Intercept: participant) |        5.27\r\nSD (Residual)               |        1.00\r\n\r\nDziÄ™ki temu nasz model estymuje zarÃ³wno efekt wewnÄ…trzobiektowy, jak i miÄ™dzyobiektowy.\r\nPoniÅ¼ej moÅ¼ecie zobaczyÄ‡ jak zmiana poszczegÃ³lnych parametrÃ³w w symulacji (kod z poczÄ…tku wpisu) wpÅ‚ywa na to jak wyglÄ…dajÄ… dane i jak radzi sobie model mieszany.\r\n\r\n\r\n\r\n\r\nMoÅ¼liwoÅ›Ä‡ estymacji efektÃ³w wewnÄ…trzobiektowych i miÄ™dzyobiektowych to jedna z zalet mieszanych modeli liniowych. PosiadajÄ… one jeszcze inne, ciekawe wÅ‚aÅ›ciwoÅ›ci. Niemniej, tu zakoÅ„czymy tÄ… maÅ‚Ä… zachÄ™tÄ™ do stosowania mieszanych modeli liniowych.\r\nJeÅ›li ktoÅ› jest zainteresowany szczegÃ³Å‚ami, zarÃ³wno matematycznymi jak i bardziej praktycznymi, zachÄ™cam do skorzystania z bazy wiedzy, gdzie znajdujÄ… siÄ™ odnoÅ›niki do dobrych tutoriali i kursÃ³w omawiajÄ…cych tÄ™ klasÄ™ modeli.\r\n\r\n\r\n\r\nBell, A., Fairbrother, M., & Jones, K. (2019). Fixed and random effects models: Making an informed choice. Quality & Quantity, 53(2), 1051â€“1074.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-13-mixedmotivation/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-05-15T08:57:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-05-tranistive/",
    "title": "WiÄ™ksze czy rÃ³wne?",
    "description": "O nieprzechodnoÅ›ci testÃ³w statystycznych.",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2022-05-05",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nWspomnÄ™ jeszcze nie raz, Å¼e nie jestem fanem p-value. Nie dlatego, Å¼e jest ona zÅ‚ym sposobem oceny wnioskÃ³w jakie wyciÄ…gamy z danych, ale z powodu tego, jak jest uÅ¼ywana i naduÅ¼ywana. PoruszyÅ‚em ten temat tutaj.\r\nDzisiejszy wpis bÄ™dzie o sytuacji jakÄ… moÅ¼emy napotkaÄ‡ gdy obcujemy z danymi. Mamy taki zbiÃ³r danych z trzema zmiennymi: wartoÅ›ci jakiejÅ› cechy, grupa do ktÃ³rej obserwacja naleÅ¼y i unikalny identyfikator. Przeprowadzamy analizÄ™ wariancji.\r\n\r\n\r\nlibrary(ez)\r\nlibrary(knitr)\r\nlibrary(broom)\r\n\r\ndata = data.frame(value = c(1,2,3,3,4,5,5,6,7), group = c(\"a\",\"a\",\"a\",\"b\",\"b\",\"b\",\"c\",\"c\",\"c\"), id = 1:9)\r\n\r\nkable(ezANOVA(data = data, dv = value, between = group, wid = id)$ANOVA)\r\n\r\n\r\nEffect\r\nDFn\r\nDFd\r\nF\r\np\r\np<.05\r\nges\r\ngroup\r\n2\r\n6\r\n12\r\n0.008\r\n*\r\n0.8\r\n\r\nIstotny efekt! Wykonujemy wiÄ™c porÃ³wnania posthoc.\r\n\r\n\r\nkable(tidy(pairwise.t.test(data$value, data$group), p.adjust.method = 'holm'))\r\n\r\n\r\ngroup1\r\ngroup2\r\np.value\r\nb\r\na\r\n0.0996505\r\nc\r\na\r\n0.0081410\r\nc\r\nb\r\n0.0996505\r\n\r\nZwizualizujmy takÅ¼e efekt grupy.\r\n\r\n\r\nezPlot(data = data, wid = id, between = group, dv = value, x = group)\r\n\r\n\r\n\r\n\r\nMamy do czynienia z sytuacjÄ…, w ktÃ³rej \\(c\\) jest istotnie wiÄ™ksze od \\(a\\), natomiast \\(b\\) nie jest istotnie rÃ³Å¼ne od \\(a\\) i od â€¦ \\(c\\). I co teraz? Popularna wykÅ‚adnia mÃ³wi: p-value < 0.05 - efekt jest (Å›rednie siÄ™ rÃ³Å¼niÄ…), p-value > 0.05 - efektu nie ma (Å›rednie siÄ™ nie rÃ³Å¼niÄ…). Jednak patrzÄ…c na wszystkie 3 testy moglibyÅ›my dojÅ›Ä‡ do wniosku, Å¼e efekt zarÃ³wno jest jak i go nie ma.\r\nOczywiÅ›cie nie wydarzyÅ‚o siÄ™ tu nic niezwykÅ‚ego. RÃ³Å¼nica Å›rednich pomiÄ™dzy \\(c\\) i \\(b\\) oraz \\(a\\) i \\(c\\) jest za maÅ‚a, by przy tej wariancji wykazaÄ‡ istotny efekt. Wnioski otrzymane za pomocÄ… testÃ³w statystycznych sÄ… nieprzechodne. PrzechodnoÅ›Ä‡ to taka wÅ‚asnoÅ›Ä‡, ktÃ³ra mÃ³wi nam, Å¼e jeÅ›li \\(a<b\\) i \\(b<c\\) to \\(a<c\\).\r\nJak zachowaÄ‡ siÄ™ w takiej sytuacji? Prawdopodobnie najrozsÄ…dniejszym wyjÅ›ciem jest przedstawienie wyniku \\(a\\) i \\(c\\), oraz stwiedzenie, Å¼e nie mamy wiedzy, by orzekaÄ‡ o \\(b\\) (patrzÄ…c na przedziaÅ‚ ufnoÅ›ci na wykresie, widzimy, Å¼e moÅ¼e byÄ‡ zarÃ³wno bliÅ¼ej \\(a\\) jak i \\(c\\)).\r\nGdybyÅ›my mieli do czynienia z sytuacjÄ… w ktÃ³rej \\(a,b,c\\) siÄ™ nie rÃ³Å¼niÄ… statystycznie albo \\(a,b\\) siÄ™ nie rÃ³Å¼niÄ… statystycznie, a \\(c\\) jest od nich istotnie wiÄ™ksze moglibyÅ›my szybko przedstawiÄ‡ nasze wnioski, bez zbÄ™dnej konsternacji.\r\nStwierdzenie, Å¼e coÅ› jest istotnie rÃ³Å¼ne jest uÅ¼ytecznÄ… heurystykÄ… tego, Å¼e coÅ› jest naprawdÄ™ rÃ³Å¼ne, jednak prosty przykÅ‚ad przedstawiony powyÅ¼ej pokazuje nam, Å¼e nie zawsze otrzymane wyniki bÄ™dÄ… spÃ³jne. Warto pamiÄ™taÄ‡, Å¼e p-value jest tylko miarÄ… wskazujÄ…cÄ… jak bardzo dane nie pasujÄ… do danego modelu (w tym przypadku modelu, w ktÃ³rym dana para Å›rednich jest rÃ³wna).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-05-tranistive/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-05-05T20:36:26+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-28-behrens/",
    "title": "Jak mÃ³zg monitoruje zmiennoÅ›Ä‡ Å›rodowiska?",
    "description": "Modelowanie niepewnoÅ›ci w procesie uczenia siÄ™ wartoÅ›ci bodÅºcÃ³w.",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-04-28",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nModelowanie procesÃ³w poznawczych to taka dziedzina, dziÄ™ki ktÃ³rej moÅ¼emy siÄ™gnÄ…Ä‡ do czarnej skrzynki, jakÄ… jest umysÅ‚. Jednym procesÃ³w, ktÃ³ry moÅ¼emy zamodelowaÄ‡, jest uczenie siÄ™. ChoÄ‡ my, ludzie, potrafimy nauczyÄ‡ siÄ™ czynnoÅ›ci poprzez obserwacjÄ™, czy nawet werbalny komunikat, dzielimy ze wszystkimi zwierzÄ™tami starszy (ale nie mniej przydatny) mechanizm. Mowa o warunkowaniu.\r\nPodstawowa zasada warunkowania jest prosta: JeÅ›li po wykonaniu jakiejÅ› czynnoÅ›ci otrzymujemy rezulatat pozytywny, zwiÄ™ksza siÄ™ prawdopodobieÅ„stwo powtÃ³rzenia tej czynnoÅ›ci, jeÅ›li negatywny, zmniejsza siÄ™.\r\nJednym z modeli warunkowania jest model Rescola-Wagner (Sutton & Barto, 2018), ktÃ³ry wyglÄ…da tak:\r\n\r\n\r\n\r\nModel jest bardzo prosty. PrzewidywanÄ… wartoÅ›ciÄ… bodÅºca \\(V\\) w czasie \\(i\\) jest wartoÅ›Ä‡ bodzica w czasie \\(i-1\\) plus bÅ‚Ä…d predykcji, czyli rÃ³Å¼nica pomiÄ™dzy rezultatem (np. wartoÅ›ciÄ… nagrody), a przewidywanÄ… wartoÅ›ciÄ… bodÅºca w czasie \\(i-1\\) , pomnoÅ¼onÄ… przez staÅ‚Ä… \\(a\\). Od wielkoÅ›ci wspÃ³Å‚czynnika uczenia (learning rate) \\(a\\) zaleÅ¼y jak szybko agent aktualizuje wartoÅ›Ä‡ bodÅºca.\r\nModel Rescola-Wagner jest doÅ›Ä‡ prosty i nie odda wszystkich zjawisk zachodzÄ…cych u ludzi i zwierzÄ…t podczas warunkowania. Jednak jego prostota pozwala szybko zrozumieÄ‡ ideÄ™ stojÄ…cÄ… za tÄ… klasÄ… modeli - uczenia ze wzmocnieniem (reinforcement learning models).\r\nTakie modele wykorzystywane sÄ… w kognitywistyce i ekonomii (uczenie siÄ™ i podejmowanie decyzji), a takÅ¼e w biologii (uczenie ze wzmocnieniem to jedna z podstawowych zasad dziaÅ‚ania neuronÃ³w dopaminergicznych) i robotyce. Ja chciaÅ‚ym siÄ™ skupiÄ‡ na badaniu przeprowadzonym przez zespÃ³Å‚ Timothego Behrensa, ktÃ³re wyjÄ…tkowo mi siÄ™ spodobaÅ‚o (Behrens et al., 2007).\r\nBadani wykonywali proste zadanie, w ktÃ³rym mieli wybieraÄ‡ wiekokrotnie jednÄ… z dwÃ³ch opcji. W pierwszej fazie (120 prÃ³b) kaÅ¼da z moÅ¼liwoÅ›ci byÅ‚a zwiÄ…zana ze staÅ‚ym prawdopodbieÅ„stwem uzyskania nagrody. NastÄ™pnie, w drugiej fazie prawdopodobieÅ„stwo uzyskania nagrody zmieniaÅ‚o siÄ™ co 30-40 prÃ³b.\r\nCaÅ‚Ä… procedurÄ™ badani wykonywali w funkcjonalnym rezonansie magnetycznym (fMRI). DziÄ™ki uÅ¼yciu neurobrazowania, moÅ¼na nie tylko sprawdziÄ‡ jak dobrze dany model uczenia ze wzmocniem wyjaÅ›nia nam obserwowalne dane (w tym wypadku decyzje badanych), ale moÅ¼emy sprawdziÄ‡ czy mÃ³zg oblicza wartoÅ›Ä‡ bodÅºca i bÅ‚Ä…d predykcji w podobny sposÃ³b jak model (i jakie oÅ›rodki w mÃ³zgu w tym wspÃ³Å‚uczestniczÄ…).\r\nBehrensa i jego zespÃ³Å‚ zainteresowaÅ‚ problem zmiennoÅ›ci Å›rodowiska. JeÅ›li Å›rodowisko jest stabilne (prawdopodobieÅ„stwa przypisane wyborom/opcjom nie zmieniajÄ… siÄ™), wspÃ³Å‚czynnik uczenia siÄ™ powinien byÄ‡ niÅ¼szy, poniewaÅ¼ nawet jeÅ›li preferowana przez nas opcja akurat nie dostarczy nagrody, jest to raczej wynikiem przypadku. Natomiast jeÅ›li prawdopodobieÅ„stwa zwiÄ…zane z wyborami zmieniajÄ… siÄ™ (a co za tym idzie zmienia siÄ™ wartoÅ›Ä‡ bodÅºcÃ³w), musimy szybko aktualizowaÄ‡ wartoÅ›ci bodÅºcÃ³w, inaczej z uporem wybieralibyÅ›my te, ktÃ³rych wybranie w przeszÅ‚osci dawaÅ‚o czÄ™sto nagrodÄ™.\r\nMoglibyÅ›my tak zmodyfikowaÄ‡ model Rescola-Wagner, by wspÃ³Å‚czynnik uczenia nie byÅ‚ staÅ‚y, ale zmieniaÅ‚ siÄ™ w czasie. UzyskalibyÅ›my wtedy informacjÄ™ jak dana osoba dostosowuje swÃ³j wspÃ³Å‚czynik uczenia. Nie wiedzielibyÅ›my jednak dlaczego to robi. WielkoÅ›Ä‡ wspÃ³Å‚czynnika uczenia bÄ™dzie takÅ¼e zaleÅ¼aÅ‚a od innych czynnikÃ³w.\r\nNa przykÅ‚ad, gdy nie mamy wczeÅ›niejszych informacji na temat wartoÅ›ci bodÅºcÃ³w, wspÃ³Å‚czynnik musi byÄ‡ wysoki, byÅ›my mogli szybko nabyÄ‡ jakieÅ› przekonania, na podstawie ktÃ³rych podejmiemy decyzjÄ™. Natomiast, wraz z postÄ™pem czasu powinien maleÄ‡ (wynika to z chartakterystyki uczenia siÄ™ - wraz z postÄ™pem czasu efekty uczenia malejÄ…).\r\nJeÅ›li chcielibyÅ›my wiedzieÄ‡ jak bardzo wspÃ³Å‚czynnik zmienia siÄ™ w zaleÅ¼noÅ›ci od niestabilnoÅ›ci (volatility), a takÅ¼e czy jej monitorowanie jest odzwierciedlone w sygnale fMRI (co interesowaÅ‚o Behrensa), musimy mieÄ‡ jej jakÄ…Å› estymatÄ™.\r\nBehrens stworzyÅ‚ wiÄ™c algorytm - Bayesowskiego Obserwatora, ktÃ³rego zadaniem byÅ‚o przewidywanie prawdopodobieÅ„stw i monitorowanie niestabilnoÅ›ci. UzyskaÅ‚ wartoÅ›Ä‡ niestabilnoÅ›ci w czasie \\(i\\). DziÄ™ki temu odkryÅ‚, Å¼e w monitorowaniu zmiennoÅ›ci Å›rodowiska uczestniczy pewien obszar mÃ³zgu - przedni zakrÄ™t obrÄ™czy.\r\nPoniewaÅ¼ lubiÄ™ siÄ™ pobawiÄ‡ takimi zabawkami, zaimplementowaÅ‚em go (kod moÅ¼na znaleÅºÄ‡ tutaj). WygenerowaÅ‚em zmiennÄ… \\(y\\) przyjmujÄ…cÄ… wartoÅ›ci 0 (brak nagrody) i 1 (nagroda). Przez pierwsze 120 obserwacji prawdopodobieÅ„stwo nagrody wynosiÅ‚o 90%, nastÄ™pnie przez 220 prÃ³b prawdopodobieÅ„stwo zmieniaÅ‚o siÄ™ miÄ™dzy 80% a 20% co 30 lub 40 prÃ³b. W ostatnich 120 obserwacjach prawdopodbieÅ„stwo wynosiÅ‚o 10%.\r\nModel radzi sobie tak:\r\n\r\n\r\n\r\nPrzy pomocy naszego idealnego obserwatora moÅ¼emy policzyÄ‡ teraz niestabilnoÅ›Ä‡ Å›rodowiska. Przy odrobinie wysiÅ‚ku moÅ¼na zaadaptowaÄ‡ model do innych sytuacji, bÄ…dÅº rozwinÄ…Ä‡ go, by uÅ¼ywaÄ‡ niestabilnoÅ›ci Å›rodowiska jako zmiennej w modelach podejmowania decyzji czy uczenia siÄ™.\r\nEstymowanÄ… niestabilnoÅ›Ä‡ Å›rodowiska moglibyÅ›my nazwaÄ‡ teÅ¼ niepewnoÅ›ciÄ… pierwszego poziomu. Warto wspomnieÄ‡, Å¼e istnieje model uczenia ze wzmocnieniem - Hierarchical Gaussian Filter, ktÃ³ry uwzglÄ™dnia niepewnoÅ›Ä‡ decyzyjnÄ… \\(n\\) poziomÃ³w (wÅ‚aÅ›ciwie tyle ile chcemy), czyli modeluje teÅ¼ niestabinoÅ›Ä‡ niestabilnoÅ›ci - i tak dalej (Mathys et al., 2014).\r\nJak dziaÅ‚a model?\r\nTwierdzenie Bayesa\r\nBayesowskie modele opierajÄ… siÄ™ na twierdzeniu Bayesa (lub inaczej mÃ³wiÄ…c, wzorze na prawdopodobieÅ„stwo warunkowe):\r\n\\[ P(parameters|data) = \\frac{P(data|parameters)P(parameters)}{P(data)}\\] CzÅ‚on \\(P(data|parameters)\\) (wiarygodnoÅ›Ä‡, likehood) oznacza prawdopodobieÅ„stwo, Å¼e dany parametr wygenerowaÅ‚ zaobserwowane dane (czyli na przykÅ‚ad prawdopodbieÅ„stwo tego, Å¼e jeÅ›li prawdopodobieÅ„stwo nagrody wynosi 80%, jakie jest prawdopodobieÅ„stwo, Å¼e zaobserwujemy 1 - czyli nagrodÄ™).\r\n\\(P(parameters)\\) nazywane prawdopodobieÅ„stem a priori, oznacza jakie jest prawdopodobieÅ„stwo danego parametru. CzÄ™sto opisuje siÄ™ go jako wczeÅ›niejsze doÅ›wiadczenie, poniewaÅ¼ moÅ¼emy wiedzieÄ‡, Å¼e wystÄ…pienie jakiejÅ› wartoÅ›ci parametru jest mniej prawdopodobne, niÅ¼ innej (bÄ…dÅº ma pewnien okreÅ›lony rozkÅ‚ad statystyczny). JeÅ›li tego nie wiemy, \\(P(parameters)\\) przyjmujÄ… takÄ… samÄ… wartoÅ›Ä‡ dla wszystkich \\(P(data|parameters)\\).\r\n\\(P(data)\\) jest wÅ‚aÅ›ciwie tylko parametrem skalujÄ…cym (staÅ‚Ä…), bez specjalnej interpretacji. Dlatego powyÅ¼szy wzÃ³r moÅ¼emy zapisaÄ‡:\r\n\\[ P(parameters|data) \\propto P(data|parameters)P(parameters)\\] gdzie \\(\\propto\\) oznacza â€œjest proporcjonalny doâ€\r\nWreszcie czÅ‚on \\(P(parameters|data)\\) jest prawdopodbieÅ„stwem a posteriori, czyli prawdopodbieÅ„stwem wartoÅ›ci parametru modelu przy zaobserowanych danych (na przykÅ‚ad, jeÅ›li zaobserwowaliÅ›my 1, jakie jest prawdopodbieÅ„stwo, Å¼e prawdopodobieÅ„stwo otrzymania nagrody wynosi 66%).\r\nProces Markowa\r\nProces Markowa to proces stochastyczny, w ktÃ³rym prawdopodobieÅ„stwo zdarzenia w czasie \\(i\\) zaleÅ¼y tylko od stanu systemu w czasie \\(i-1\\). Czyli:\r\n\\[P(X_i = x|X_{i-1}, X_{i-2}  ... X_1) = P(X_i = x|X_{i-1})\\]\r\nObserwator Bayesowski\r\nModel Behrensa moÅ¼na zwizualizowaÄ‡ nastÄ™pujÄ…cy sposÃ³b:\r\n\r\n\r\n\\(y_{i+1}\\) czyli nagroda lub jej brak zaleÅ¼y tylko od parametru \\(r_{i+1}\\) - prawdopodobieÅ„stwa jej uzyskania. Z kolei parametr \\(r_{i+1}\\) zaleÅ¼y od siebie krok wczeÅ›niej \\(r_{i}\\) oraz od niestabilnoÅ›ci \\(v_i\\), ktÃ³ra sama zaleÅ¼y od \\(v_{i-1}\\) i staÅ‚ego parametru \\(k\\), ktÃ³ry moglibyÅ›my nazwaÄ‡ niestabilnoÅ›ciÄ… niestabilnoÅ›ci. Jak widzimy, model Behrensa zakÅ‚ada proces Markowa (wartoÅ›ci parametrÃ³w w czasie \\(i\\) zaleÅ¼Ä… tylko od wartoÅ›ci parametrÃ³w w czasie \\(i-1\\)).\r\nSzukamy takich parametrÃ³w, ktÃ³re maksymalizujÄ… prawdopodobieÅ„stwo a posteriori \\(P(r_i,v_i,k|y_i)\\). DziÄ™ki temu bÄ™dziemy poznamy najbardziej prawdopodobne wartoÅ›ci parametrÃ³w \\(r\\), \\(v\\) i \\(k\\).\r\nUÅ¼ywiajÄ…c twierdzenia Bayesa, wzÃ³r na prawdopodobieÅ„stwo a posteriori wyglÄ…da tak:\r\n\\[P(r_i,v_i,k|y_i) \\propto \\int \\int P(y_i|r_i)P(r_{i}|r_{i-1},v_i) P(v_{i}|v_{i-1},k) P(r_{i-1},v_{i-1},k|y_{i-1})dv_{i-1} dr_{i-1}\\]\r\nPotrzebujemy do peÅ‚ni szczÄ™Å›cia jeszcze poszczegÃ³lnych prawdopodobieÅ„stw wystepujÄ…cych we wzorze.\r\nPrawdopodobieÅ„stwo \\(P(y_i|r_i)\\), czyli otrzymanie nagrody, pod warunkiem prawdopodobieÅ„stwa \\(r_i\\) wynosi po prostu \\(r_i\\). MoÅ¼emy wiÄ™c zamodelowaÄ‡ je rozkÅ‚adem dwumianowym:\r\n\\[P(y_i|r_i) \\sim Binomial(r_i)\\]\r\nPrawdpodobieÅ„stwo \\(P(r_{i+1}|r_i,v_i)\\) Behrens zamodelowaÅ‚ rozkÅ‚adem Beta, jednak ja, kierowany lenistwem, zamodelowaÅ‚em je rozkÅ‚adem normalnym ograniczonym na przedziale (0,1): \\[P(r_{i+1}|r_{i},v_{i}) \\sim N^{(0,1)}(r_i,e^{v_{i}})\\]\r\nJak widzimy, prawdopodobieÅ„stwo \\(r_{i+1}\\) dane jest rozkÅ‚adem normalnym o Å›redniej \\(r_i\\) i wariancji \\(e^{v_{i+1}}\\). To jak bardzo prawdopodobny jest wiÄ™kszy przeskok pomiÄ™dzy \\(r_i\\) do \\(r_{i+1}\\) zaleÅ¼ne bÄ™dzie od tego jak duÅ¼a jest wariancja \\(e^{v_{i+1}}\\). WÅ‚aÅ›nie ona jest naszÄ… ukrytÄ… zmiennÄ… reprezentujÄ…cÄ… niestabilnoÅ›Ä‡.\r\nPrawdopodobieÅ„stwo \\(P(v_{i+1}|v_i,k)\\) rÃ³wnieÅ¼ zamodelowane jest rozkÅ‚adem normalnym:\r\n\\[P(v_{i+1}|v_i,k) \\sim N(v_i,e^{k})\\]\r\nCzyli wielkoÅ›Ä‡ przeskokÃ³w pomiÄ™dzy \\(v_{i+1}\\) i \\(v_{i}\\) zaleÅ¼na jest od parametru k - staÅ‚ego dla wszystkich \\(i\\), ktÃ³ry, a jakÅ¼e, teÅ¼ zamodelowany jest rozkÅ‚adem normalnym:\r\n\\[P(k) \\sim N(0,10^{10})\\]\r\nDo uzyskania rozkÅ‚adÃ³w a posteriori uÅ¼yÅ‚em metod ABC (Approximate Bayesian Computation), a konkretniej prÃ³bkowania Gibbsa (przy uÅ¼yciu JAGS).\r\nPo wiÄ™cej detali polecam zajrzeÄ‡ do orginalnego artykuÅ‚u.\r\n\r\n\r\n\r\nBehrens, T. E., Woolrich, M. W., Walton, M. E., & Rushworth, M. F. (2007). Learning the value of information in an uncertain world. Nature Neuroscience, 10(9), 1214â€“1221.\r\n\r\n\r\nMathys, C. D., Lomakina, E. I., Daunizeau, J., Iglesias, S., Brodersen, K. H., Friston, K. J., & Stephan, K. E. (2014). Uncertainty in perception and the hierarchical gaussian filter. Frontiers in Human Neuroscience, 8, 825.\r\n\r\n\r\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (pp. 346â€“349). MIT press.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-28-behrens/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-05-04T21:39:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-15-seria-jak-rozumie-nauk-metodologia-bada-i-statystyka/",
    "title": "Metodologia badaÅ„ i Statystyka",
    "description": "Seria: Jak rozumieÄ‡ naukÄ™?",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "Refleksja"
    ],
    "contents": "\r\n\r\nContents\r\nStatystycznie rzecz biorÄ…c\r\nJak zbadaÄ‡ wszystich PolakÃ³w?\r\nRÃ³Å¼niÄ… siÄ™ czy nie?\r\nKrysys Replikacyjny\r\n\r\nZwykle kursy z metodologii w szkoÅ‚ach wyÅ¼szych zaczyna siÄ™ od filozofii nauki. Wprowadza ona naleÅ¼yty kontekst, historiÄ™ refleksji naukowej. Przedstawia siÄ™ rolÄ™ teorii i modeli w procesie wyjaÅ›niania rzeczywistoÅ›ci. NastÄ™pnie wyjaÅ›nia siÄ™ zaÅ‚oÅ¼enia i metody stosowane w dziedzinie nauki, ktÃ³rÄ… ktoÅ› akurat studiuje. My te i inne tematy odÅ‚oÅ¼ymy sobie na potem, a seriÄ™ jak rozumieÄ‡ naukÄ™ zaczniemy od przyjrzenia siÄ™ statystyce i metodologii badaÅ„.\r\nStatystycznie rzecz biorÄ…c\r\nStatystyka to dziedzina nauki pozwalajÄ…ca na formalne (to znaczy w jÄ™zyku matematyki) przedstawienie interesujÄ…cych nas zjawisk. CzÄ™sto jest przedmiotem znienawidzonym przez studentÃ³w. Uznawana za nudnÄ… i trudnÄ…, czÄ™sto uczona przez wykÅ‚adowcÃ³w, ktÃ³rzy sami jÄ… za takÄ… uwaÅ¼ajÄ…, co zdecydowanie nie pomaga w jej przyswajaniu. ZdecydowaliÅ›my siÄ™ zaczÄ…Ä‡ od statystyki, poniewaÅ¼ jest ona wykorzystywana wÅ‚aÅ›ciwie we wszystkich naukach empirycznych (choÄ‡ niektÃ³rzy fizycy zapewne z pogardÄ… pokrÄ™ciliby gÅ‚owÄ…). Dobrze podsumowaÅ‚ to matematyk John Tukey: â€NajlepszÄ… rzeczÄ… w byciu statystykiem jest to, Å¼e moÅ¼esz grzebaÄ‡ w podwÃ³rku u wszystkich innychâ€ (Lin et al., 2014).\r\nZacznijmy wiÄ™c zagÅ‚Ä™biaÄ‡ siÄ™ w meandry statystyki. Najpierw musimy wykonaÄ‡ pomiar. Pomiar oznacza przypisanie cesze pewnego obiektu wartoÅ›ci liczbowej. Np. ile waÅ¼y czÅ‚owiek w kilogramach, na kogo zagÅ‚osuje w najbliÅ¼szych wyborach lub jaki jest poziom jego otwartoÅ›ci na nowe doÅ›wiadczenia. NastÄ™pnie moÅ¼emy sprawdziÄ‡ czy pomiÄ™dzy mierzonymi zmiennymi zachodzi jakiÅ› zwiÄ…zek. Na przykÅ‚ad, gdybyÅ›my przyjrzeli siÄ™ danym dotyczÄ…cym wieku i wzrostu ludzi do 21 roku, zauwaÅ¼ylibyÅ›my, Å¼e im wiÄ™cej ktoÅ› ma lat, tym jest wyÅ¼szy. ZwiÄ…zek, w ktÃ³rym wartoÅ›Ä‡ jednej zmiennej wzrasta i wartoÅ›Ä‡ drugiej zmiennej takÅ¼e wzrasta (bÄ…dÅº maleje) nazywamy korelacjÄ… liniowÄ….\r\nIstniejÄ… oczywiÅ›cie inne, bardziej subtelne zwiÄ…zki pomiÄ™dzy zmiennymi, ale na razie zostaÅ„my przy relacji liniowej. PewnÄ… mantrÄ…, ktÃ³rÄ… sÅ‚yszy siÄ™ na zajÄ™ciach ze statystyki jest: â€korelacja nie oznacza przyczynowoÅ›ciâ€. WeÅºmy taki przykÅ‚ad: okazuje siÄ™, Å¼e im wiÄ™cej sprzedaje siÄ™ lodÃ³w tym czÄ™Å›ciej ludzi atakujÄ… rekiny. Czy spoÅ¼ycie lodÃ³w w jakiÅ› sposÃ³b zachÄ™ca rekiny do ataku? OczywiÅ›cie nie, pozorny zwiÄ…zek pomiÄ™dzy tymi dwoma zjawiskami wyjaÅ›nia fakt, Å¼e ludzie najczÄ™Å›ciej kupujÄ… lody w upalne dni, a w upalne dni wiÄ™cej ludzi odpoczywa na plaÅ¼y, co przekÅ‚ada siÄ™ na czÄ™stsze ataki rekinÃ³w.\r\nGdy mamy wiÄ™c dane obserwacyjne (nasze wyniki pomiarÃ³w) nie moÅ¼emy mÃ³wiÄ‡ o przyczynowoÅ›ci, gdyÅ¼ widzimy tylko wspÃ³Å‚zmiennoÅ›Ä‡ cech obiektÃ³w, ktÃ³re badamy. By wnioskowaÄ‡ o przyczynowoÅ›ci musimy posÅ‚uÅ¼yÄ‡ siÄ™ eksperymentem. W warunkach kontrolowanych dzielimy losowo badane przez nas obiekty na dwie grupy (bÄ…dÅº wiÄ™cej), zapewniamy im moÅ¼liwie jak najbardziej zbliÅ¼one warunki, a nastÄ™pnie w jednej z nich (zwanej grupÄ… docelowÄ…) wykonujemy jakÄ…Å› interwencjÄ™ X (np. podajemy lek), a w drugiej grupie (nazywanej kontrolnÄ…) nie. NastÄ™pnie w obu grupach wykonujemy pomiar. JeÅ›li pomiary w grupach siÄ™ rÃ³Å¼niÄ…, zakÅ‚adamy, Å¼e rÃ³Å¼nicÄ™ mogÅ‚a spowodowaÄ‡ tylko interwencja X.\r\nCzÄ™sto jednak nie moÅ¼emy posÅ‚uÅ¼yÄ‡ siÄ™ eksperymentem, bo na przykÅ‚Ä…d: jego koszt jest zbyt wysoki, wykonanie byÅ‚oby nieetyczne, lub jest to po prostu niemoÅ¼liwe. Wtedy, by odnaleÅºÄ‡ przyczynowoÅ›Ä‡, posÅ‚ugujemy siÄ™ dodatkowymi metodami, takimi jak wykorzystanie istniejÄ…cych teorii, zgodnoÅ›Ä‡ z modelem, wyeliminowanie innych potencjalnie wpÅ‚ywajÄ…cych zmiennych, modelami zwierzÄ™cymi czy obserwacjÄ… nastÄ™pstw czasowych w badaniach podÅ‚uÅ¼nych. Å»adna z tych metod nie daje jednak takiej pewnoÅ›ci we wnioskowaniu o przyczynowoÅ›ci jak eksperyment. PewnÄ… nadziejÄ™ na rozwiÄ…zanie tego problemu dajÄ… rozwijane od kilkunastu lat metody matematyczne pozwalajÄ…ce na wnioskowanie o przyczynowoÅ›ci z danych obserwacyjnych. Zainteresowanym tym tematem polecam ksiÄ…Å¼kÄ™ â€Przyczyny i Skutkiâ€ Judeâ€™a Pearlâ€™a i Dany Mackenzie (Pearl & Mackenzie, 2021).\r\nJak zbadaÄ‡ wszystich PolakÃ³w?\r\nZajmijmy siÄ™ teraz kolejnym istotnym problemem, mianowicie, jak moÅ¼emy uogÃ³lniaÄ‡ (generalizowaÄ‡) wyniki badaÅ„. ZaÅ‚Ã³Å¼my, Å¼e chcemy powiedzieÄ‡ coÅ› o wadze w populacji PolakÃ³w, wiÄ™c przeprowadzamy badanie. Mierzymy wagÄ™ dziesiÄ™ciu ludzi napotkanych na ulicy, ale od razu napotykamy problem. OtrzymaliÅ›my dokÅ‚adne informacje o wadze tylko tych 10 ludzi. SkÄ…d mamy pewnoÅ›Ä‡, Å¼e ich Å›rednia waga jest jakkolwiek zbliÅ¼ona do Å›redniej w populacji? MoÅ¼e naleÅ¼aÅ‚oby zwiÄ™kszyÄ‡ prÃ³bÄ™? Sto, moÅ¼e tysiÄ…c osÃ³b wystarczyÅ‚oby aby uzyskaÄ‡ odpowiednie przybliÅ¼enie. TakÄ… logikÄ… kierowaÅ‚ siÄ™ magazyn â€Literary Digestâ€, ktÃ³ry przeprowadziÅ‚ sondaÅ¼ w wyborach prezydenckich w Stanach Zjednoczonych w 1936 roku. PrÃ³ba wynosiÅ‚a ponad dwa miliony ludzi. W sondaÅ¼u zwyciÄ™Å¼yÅ‚ Alf Landon z 57% poparciem. Jednak w wyborach zwyciÄ™Å¼yÅ‚ Franklin Delano Roosevelt z 61% poparciem, gdy Alf Landon otrzymaÅ‚ jedynie 8% gÅ‚osÃ³w. Jak to moÅ¼liwe? â€Literary Digestâ€ losowaÅ‚ respondentÃ³w z ksiÄ…Å¼ek telefonicznych. Jednak w 1936 roku telefon posiadaÅ‚y raczej osoby zamoÅ¼ne, dlatego wyniki sondaÅ¼u byÅ‚y bardzo skrzywione. WiÄ™kszoÅ›Ä‡ ludzi w dobie najwiÄ™kszego kryzysu ekonomicznego w Stanach, gÅ‚osowaÅ‚o na prosocjalne reformy Roosevelta (Babbie, 2008).\r\nA jednak dzisiejsze sondaÅ¼e sÄ… zadziwiajÄ…co dokÅ‚adne. Jak siÄ™ to dzieje? OtÃ³Å¼ zachowanie matematycznych obiektÃ³w, a takimi sÄ… nasze pomiary, moÅ¼na policzyÄ‡. JeÅ›li kaÅ¼dy obiekt w populacji (np. PolakÃ³w) ma niezerowe, takie samo prawdopodobieÅ„stwo wylosowania do prÃ³by, moÅ¼emy policzyÄ‡ ile osÃ³b musimy wylosowaÄ‡, by z prawdopodobieÅ„stwem X wylosowaÄ‡ prÃ³bÄ™, w ktÃ³rej Å›rednia wartoÅ›Ä‡ cechy nie bÄ™dzie rÃ³Å¼na wiÄ™cej niÅ¼ Y od Å›redniej wartoÅ›ci cechy w populacji.\r\n\r\nUmoÅ¼liwia nam to jedno z najwaÅ¼niejszych twierdzeÅ„ w statystyce: Centralne Twierdzenie Graniczne.\r\nW praktyce wyglÄ…da to tak, Å¼e gdybyÅ›my chcieli mieÄ‡ 95% szansÄ™ na wylosowanie prÃ³by, w ktÃ³rej procent respondentÃ³w gÅ‚osujÄ…cych na danÄ… partiÄ™, nie rÃ³Å¼ni siÄ™ wiÄ™cej niÅ¼ o 3% od realnego poparcia w spoÅ‚eczeÅ„stwie, powinniÅ›my wylosowaÄ‡ 1067 osÃ³b do prÃ³by. PoniewaÅ¼ kaÅ¼dy Polak musi mieÄ‡ szansÄ™ na wylosowanie, prÃ³bÄ™ powinniÅ›my losowaÄ‡ ze zbioru, ktÃ³ry zawiera wszystkich PolakÃ³w, na przykÅ‚ad zbioru numerÃ³w PESEL.\r\nPrÃ³bÄ™, do ktÃ³rej obiekty zostaÅ‚y przydzielone w sposÃ³b losowy z caÅ‚ej populacji nazywamy prÃ³bÄ… reprezentatywnÄ…. PrÃ³bÄ™ w ktÃ³rej wszystkie obiekty zostaÅ‚y wylosowane z takim samym prawdopodobieÅ„stwem nazywamy doborem prostym losowym. IstniejÄ… jeszcze inne metody doboru prÃ³by reprezentatywnej. Wszystkie Å‚Ä…czy to, Å¼e moÅ¼emy dokÅ‚adnie policzyÄ‡ margines bÅ‚Ä™du dla wynikÃ³w.\r\nSpotkaÅ‚em siÄ™ kiedyÅ› z opiniÄ…, Å¼e jeÅ›li ankieter stanie w centrum miasta i bÄ™dzie podchodziÅ‚ do niektÃ³rych ludzi i prosiÅ‚ ich o wypeÅ‚nienie ankiety, mamy do czynienia z losowym doborem do prÃ³by. Ankieter przecieÅ¼ nie zna tych ludzi, nie moÅ¼e wiÄ™c dobieraÄ‡ ich sobie wedÅ‚ug ich poglÄ…dÃ³w. Niemniej, ludzie Ci znaleÅºli siÄ™ w tym konkretnym miejscu, o tej konkretnej godzinie w sposÃ³b nieprzypadkowy. Przy uczelni bÄ™dzie wiÄ™cej studentÃ³w, w godzinach szczytu bÄ™dzie wiÄ™cej osÃ³b zmierzajÄ…cych do pracy, i tak dalej.\r\nA jednak wÅ›rÃ³d badaÅ„ naukowych prÃ³by reprezentatywne to znaczna mniejszoÅ›Ä‡. Jest to spowodowane wieloma czynnikami. Przede wszystkim, przeprowadzenie badania reprezentatywnego jest kosztowne i trudne. Uzyskanie dostÄ™pu do listy zawierajÄ…cej dane wszystkich obywateli wymaga przejÅ›cia wielu czasochÅ‚onnych procedur. Po drugie, o ile w wypadku ludzi istnieje jakaÅ› lista, w przypadku badaÅ„ nad zwierzÄ™tami nic takiego nie ma. Biolodzy nie majÄ… skÄ…d wylosowaÄ‡ reprezentatywnej prÃ³by szczurÃ³w. Chemicy i fizycy majÄ… w tym wzglÄ™dzie nieco Å‚atwiej. Atom wodoru na wsi bÄ™dzie identyczny z atomem wodoru w mieÅ›cie.\r\nBy poradziÄ‡ sobie z tym problemem stosuje siÄ™ rÃ³Å¼ne metody. W dniu wyborÃ³w, po zakoÅ„czeniu zbierania gÅ‚osÃ³w, zwykle o godzinie 21 ogÅ‚asza siÄ™ sondaÅ¼owe wyniki wyborÃ³w, metodÄ… exit poll. Badanie to nie jest przeprowadzone na prÃ³bie reprezentatywnej, poniewaÅ¼ ankieterzy muszÄ… zadaÄ‡ pytanie o oddany gÅ‚os zaraz po wyjÅ›ciu wyborcy z lokalu wyborczego. A jednak znowu mamy do czynienia z niezwykÅ‚Ä… dokÅ‚adnoÅ›ciÄ…. Badacze wyszczegÃ³lniajÄ… zmienne silnie skorelowane z preferencjami wyborczymi, na podstawie wczeÅ›niejszych badaÅ„ reprezentatywnych. Takie zmienne to zwykle pÅ‚eÄ‡, wielkoÅ›Ä‡ miejscowoÅ›ci zamieszkania, poziom wyksztaÅ‚cenia czy wielkoÅ›Ä‡ dochodÃ³w. Respondenci sÄ… dobierani tak, by procentowo liczba osÃ³b wyksztaÅ‚conych odpowiadaÅ‚a tej w populacji itd. Przy poprawnie dobranych zaÅ‚oÅ¼eniach pozwala to na dokÅ‚adne oszacowanie wynikÃ³w wyborÃ³w. Biolodzy wspÃ³Å‚czeÅ›nie w badaniach nad zwierzÄ™tami podobnie starajÄ… siÄ™ braÄ‡ pod uwagÄ™ wewnÄ…trzgatunkowe zrÃ³Å¼nicowanie genetyczne, Å›rodowisko zwierzÄ™cia i jego historiÄ™, by stworzyÄ‡ prÃ³bÄ™, ktÃ³rej wyniki bÄ™dÄ… daÅ‚y siÄ™ generalizowaÄ‡ (Farrar et al., 2021).\r\nJednoczeÅ›nie w niereprezentatywnych prÃ³bach naleÅ¼y uznaÄ‡, Å¼e wpÅ‚yw pewnych potencjalnych ÅºrÃ³deÅ‚ zmiennoÅ›ci jest zaniedbywalny, to znaczy nie ma znaczenia dla wyniku badaÅ„. Na przykÅ‚ad, Å¼e mechanizm molekularny skurczu miÄ™Å›nia poprzecznie prÄ…Å¼kowanego jest niezaleÅ¼ny od tego czy ktoÅ› mieszka w Warszawie czy San Francisco. Jednak czasami takie zaÅ‚oÅ¼enia sÄ… bÅ‚Ä™dne. Psychologowie dÅ‚ugo twierdzili, Å¼e badajÄ… uniwersalne mechanizmy zachowania i myÅ›lenia ludzkiego, co miaÅ‚o uzasadniaÄ‡ maÅ‚o zrÃ³Å¼nicowane prÃ³by skÅ‚adajÄ…ce siÄ™ gÅ‚Ã³wnie ze studentÃ³w psychologii (obecnie w ramach psychologii miÄ™dzykulturowej zwraca siÄ™ uwagÄ™ na rÃ³Å¼nice wynikajÄ…ce z kultury) (Hanel & Vione, 2016). Problemy z doborem prÃ³by miaÅ‚y takÅ¼e inne dziedziny nauki, jak biologia i medycyna, co jest jednym z powodÃ³w kryzysu replikacyjnego, ktÃ³ry omÃ³wimy w dalszej czÄ™Å›ci tego tekstu.\r\nRÃ³Å¼niÄ… siÄ™ czy nie?\r\nNo dobrze. Jak zauwaÅ¼yliÅ›my, dobÃ³r prÃ³by do badania to nietrywialna sprawa. ZaÅ‚Ã³Å¼my jednak, Å¼e dobraliÅ›my naszÄ… prÃ³bÄ™ odpowiednio i teraz chcemy powiedzieÄ‡ czy Å›rednia waga mÄ™Å¼czyzn i kobiet siÄ™ rÃ³Å¼ni. Nie wystarczy jednak tylko spojrzeÄ‡ na Å›redniÄ… mÄ™Å¼czyzn i kobiet z naszej prÃ³by i zobaczyÄ‡, ktÃ³ra Å›rednia jest wiÄ™ksza. PoniewaÅ¼ losowaliÅ›my naszych badanych, nawet jeÅ›li Å›rednia waga w populacji jest taka sama dla kobiet i mÄ™Å¼czyzn, prawie na pewno nie otrzymamy takich samych Å›rednich w naszej prÃ³bie (zawsze bÄ™dzie miÄ™dzy nimi jakaÅ› rÃ³Å¼nica). Jak wiÄ™c okreÅ›liÄ‡ czy otrzymane wyniki w prÃ³bie istotnie siÄ™ rÃ³Å¼niÄ… w populacji, czy rÃ³Å¼nica jest wynikiem bÅ‚Ä™du losowego.\r\nI tu statystycy opracowali kolejnÄ… ciekawÄ… sztuczkÄ™. Na podstawie wynikÃ³w otrzymanych w prÃ³bie jesteÅ›my w stanie oszacowaÄ‡ prawdopodobieÅ„stwo tego, Å¼e jeÅ›li w populacji mÄ™Å¼czyÅºni i kobiety nie rÃ³Å¼niÄ… siÄ™ wagÄ…, to na ile jest prawdopodobne wylosowanie prÃ³bek rÃ³Å¼niÄ…cych siÄ™ o okreÅ›lonÄ… wartoÅ›Ä‡. JeÅ›li jest to wysoce nieprawdopodobne (zwykle przyjmuje siÄ™, Å¼e to prawdopodobieÅ„stwo wynosi mniej niÅ¼ 5%) przyjmuje siÄ™ to jako argument, Å¼e rÃ³Å¼nica rzeczywiÅ›cie istnieje w populacji. PrawdopodobieÅ„stwo to nazywa siÄ™ wartoÅ›ciÄ… p (p-value), a jeÅ›li jest mniejsze od przyjÄ™tego progu (dopuszczalnego prawdopodobieÅ„stwa otrzymania wyniku faÅ‚szywie pozytywnego), wynik nazywamy istotnym statystycznie.\r\nP-value moÅ¼emy obliczyÄ‡ dla wiÄ™kszoÅ›ci hipotez, nie tylko o rÃ³Å¼nicy Å›rednich, np. czy zmienne sÄ… skorelowane liniowo, szacujÄ…c prawdopodobieÅ„stwo otrzymania wyniku w prÃ³bie, ktÃ³ry w populacji nie istnieje. Nie jest to jedyny sposÃ³b weryfikacji hipotez statystycznych, niemniej z powodÃ³w historycznych oraz faktu, Å¼e obliczenie p-value jest stosunkowo proste, jest to najczÄ™stsza metoda stosowana w praktyce.\r\nNiemniej wokÃ³Å‚ uÅ¼ycia p-value narosÅ‚o wiele kontrowersji. Wynika to miÄ™dzy innymi z automatycznego korzystania z tej miary przez naukowcÃ³w traktujÄ…cych statystykÄ™ jako mechaniczny sposÃ³b na wskazanie czy badanie potwierdza danÄ… tezÄ™. Z jednej z moÅ¼liwoÅ›ci weryfikacji hipotez statystycznych p-value staÅ‚a siÄ™ obowiÄ…zjuÄ…cym standardem w badaniach, choÄ‡ nie zawsze jej uÅ¼ycie jest najbardziej adekwatne (Gigerenzer, 2004).\r\nPrzeszliÅ›my od doboru prÃ³by do prostej analizy statystycznej otrzymanych wynikÃ³w. Czy to jednak zawsze wystarczy by otrzymaÄ‡ rzetelnÄ… informacjÄ™ na temat interesujÄ…cego nas efektu? Niestety sprawa jest bardziej skomplikowana. Rozpatrzmy nastÄ™pujÄ…cy przykÅ‚ad, przeprowadzamy badanie na podstawie danych obserwacyjnych w ktÃ³rym chcemy okreÅ›liÄ‡ wpÅ‚yw aktywnoÅ›ci fizycznej na gÄ™stoÅ›Ä‡ koÅ›ci. Liczymy wskaÅºnik korelacji liniowej, jednak ku naszemu zdziwieniu analiza pokazuje brak zwiÄ…zku, choÄ‡ z innych badaÅ„ wiemy, Å¼e wraz ze wzrostem czÄ™stotliwoÅ›ci uprawiania sportu, gÄ™stoÅ›Ä‡ koÅ›ci powinna rosnÄ…Ä‡.\r\nUÅ¼ywamy wiÄ™c innej metody statystycznej â€“ regresji liniowej, ktÃ³ra pozwala na uwzglÄ™dnienie wpÅ‚ywu wiÄ™cej niÅ¼ jednej zmiennej na zmiennÄ…, ktÃ³ra nas interesuje (gÄ™stoÅ›Ä‡ koÅ›ci). Do analizy uÅ¼ywamy teraz nie tylko czÄ™stotliwoÅ›ci aktywnoÅ›ci fizycznej, ale takÅ¼e wagÄ™ w kilogramach. Okazuje siÄ™, Å¼e obydwie zmienne sÄ… pozytywnie skorelowane z gÄ™stoÅ›ciÄ… koÅ›ci (im wiÄ™ksza czÄ™stotliwoÅ›Ä‡ aktywnoÅ›ci fizycznej bÄ…dÅº waga, tym wiÄ™ksza gÄ™stoÅ›Ä‡ koÅ›ci). Jednak jeÅ›li analizowaÄ‡ zmienne z osobna, nie wykryjemy zwiÄ…zku. Dzieje siÄ™ tak dlatego, Å¼e czÄ™stotliwoÅ›Ä‡ aktywnoÅ›ci fizycznej i waga sÄ… ze sobÄ… negatywnie skorelowane (im czÄ™stsza aktywnoÅ›Ä‡, tym mniejsza waga). Oznacza to, Å¼e wiÄ™ksza gÄ™stoÅ›Ä‡ koÅ›ci u osÃ³b aktywnych jest rÃ³wnowaÅ¼ona przez wiÄ™kszÄ… gÄ™stoÅ›Ä‡ koÅ›ci u osÃ³b waÅ¼Ä…cych wiÄ™cej, przez co wydaje siÄ™, na pierwszy rzut oka, Å¼e aktywnoÅ›Ä‡ fizyczna nie ma zwiÄ…zku z gÄ™stoÅ›ciÄ… koÅ›ci.\r\nZauwaÅ¼my, Å¼e gdybyÅ›my przeprowadzili eksperyment, nie spotkalibyÅ›my siÄ™ z podobnym problemem, poniewaÅ¼ tylko jedna grupa uprawiaÅ‚aby wzmoÅ¼onÄ… aktywnoÅ›Ä‡ fizycznÄ…, co zwiÄ™kszyÅ‚oby Å›redniÄ… gÄ™stoÅ›Ä‡ koÅ›ci. ChoÄ‡ dane obserwacyjne nie pozwalajÄ… nam mÃ³wiÄ‡ o przyczynowoÅ›ci, za pomocÄ… metod statystycznych jesteÅ›my w stanie odseparowaÄ‡ wpÅ‚yw poszczegÃ³lnych zmiennych.\r\nNa koniec tej czÄ™Å›ci warto wspomnieÄ‡ o interpretacji wynikÃ³w analizy statystycznej. Ponownie zaÅ‚Ã³Å¼my, Å¼e przeprowadzamy eksperyment, w ktÃ³rym interesuje nas wpÅ‚yw alkoholu na zachowania agresywne. Dobieramy prÃ³bÄ™, grupie docelowej podajemy alkohol, a nastÄ™pnie mierzymy czÄ™stotliwoÅ›Ä‡ zachowaÅ„ agresywnych. Podczas analizy naszych wynikÃ³w widzimy, Å¼e Å›rednia czÄ™stotliwoÅ›Ä‡ zachowaÅ„ agresywnych jest istotnie wyÅ¼sza w grupie, ktÃ³rej podaliÅ›my alkohol. Z tego punktu wiedzie prosta droga do wniosku, Å¼e alkohol powoduje agresjÄ™.\r\nNie jest to jednak wniosek poprawny. JeÅ›li doÅ‚Ä…czymy do analizy zmiennÄ… wskazujÄ…cÄ… na tendencjÄ™ do zachowaÅ„ agresywnych, okaÅ¼e siÄ™, Å¼e czÄ™stotliwoÅ›Ä‡ agresji wzrasta po alkoholu, ale proporcjonalnie do wczeÅ›niej istniejÄ…cej skÅ‚onnoÅ›ci do agresji (Chiavegatto et al., 2010). OtrzymaliÅ›my zgoÅ‚a inny wniosek, mÃ³wiÄ…cy nam, Å¼e alkohol dziaÅ‚a jako wyzwalacz agresji, jednak jej nie powoduje. Podczas przeprowadzania badaÅ„ naleÅ¼y pamiÄ™taÄ‡ o tym, Å¼e wniosek statystyczny (Å›rednia iloÅ›Ä‡ zachowaÅ„ agresywnych wzrasta), jest czymÅ› innym od wniosku interpretacyjnego (alkohol powoduje agresjÄ™).\r\nKrysys Replikacyjny\r\nZwrÃ³Ä‡my uwagÄ™ teraz na wiÄ™kszy obraz wynikajÄ…cy z naszych rozwaÅ¼aÅ„ nad metodologiÄ… statystycznÄ…. ZaÅ‚Ã³Å¼my, Å¼e w idealnym Å›wiecie, gdzie badacze idealnie przygotowujÄ… swoje eksperymenty na doskonaÅ‚ych prÃ³bach, Å›rednio 5% badaÅ„, w ktÃ³rych badany efekt nie istnieje, bÄ™dzie zawieraÅ‚o faÅ‚szywe wyniki. Czyli jeÅ›li na temat jakiegoÅ› zjawiska pojawiÅ‚o siÄ™ wystarczajÄ…co duÅ¼o badaÅ„, to zawsze znajdziemy artykuÅ‚ przedstawiajÄ…cy dane za, jak i przeciw danej tezie. Dlatego jeÅ›li ktoÅ› przedstawia nam badanie popierajÄ…ce jakÄ…Å› tezÄ™, niekoniecznie oznacza to, Å¼e ma racjÄ™. W nauce waÅ¼ne jest gromadzenie (akumulacja) dowodÃ³w i powtarzanie (replikacja) badaÅ„. DuÅ¼e role odgrywajÄ… w tym metaanalizy czyli wtÃ³rna analiza statystyczna wynikÃ³w wielu badaÅ„ w celu okreÅ›lenia istotnoÅ›ci dowodÃ³w przemawiajÄ…cych za danym zjawiskiem.\r\n\r\nUpraszczam tu sprawÄ™, poniewaÅ¼ mÃ³wiÄ™ tylko o prawdopodobieÅ„stwie wynikÃ³w faÅ‚szywie pozytywnych, a naleÅ¼aÅ‚oby uwzglÄ™dniÄ‡ takÅ¼e sytuacje, w ktÃ³rych nie wykryto efektu faktycznie istniejÄ…cego, czyli faÅ‚szywych wynikach negatywnych. Realnie prawdopodobieÅ„stwo otrzymania wynikÃ³w nieprawdziwych jest wiÄ™ksze.\r\nNo wÅ‚aÅ›nie. Tak by byÅ‚o w idealnym Å›wiecie. Jednak realnie mamy do czynienia z efektem zÅ‚udzenia publikacyjnego, czyli skÅ‚onnoÅ›ci wydawcÃ³w do publikacji artykuÅ‚Ã³w zawierajÄ…cych wyniki Å›wiadczÄ…ce o istnieniu efektu. Brak efektu nie jest sexy, bo w badaniu nie wyszÅ‚o nic interesujÄ…cego. DoprowadziÅ‚o to sytuacji, w ktÃ³rej badania przedstawiajÄ…ce jakiÅ› efekt byÅ‚y znacznie czÄ™Å›ciej publikowane niÅ¼ te, ktÃ³re go nie pokazywaÅ‚y. A tak jak zauwaÅ¼yliÅ›my wczeÅ›niej zawsze pewien odsetek badaÅ„ faÅ‚szywie pokaÅ¼e nam istnienie efektu, ktÃ³rego nie ma. MoÅ¼e prowadziÄ‡ to do sytuacji, w ktÃ³rej dobrze udokumentowany efekt faktycznie nie istnieje. Za przykÅ‚ad moÅ¼e posÅ‚uÅ¼yÄ‡ zjawisko zagroÅ¼enia stereotypem z psychologii spoÅ‚ecznej polegajÄ…ce na obniÅ¼eniu sprawnoÅ›ci wykonywania zadania przez osoby naleÅ¼Ä…ce do grupy objÄ™tej negatywnym stereotypem, gdy wczeÅ›niej â€przypomniâ€ im siÄ™ o istnieniu stereotypu. Na przykÅ‚ad jeÅ›li kobietom (niekoniecznie wprost) powie siÄ™, Å¼e kobiety sÄ… gorsze z matematyki, a nastÄ™pnie przeprowadzi siÄ™ test, okaÅ¼e siÄ™, Å¼e wypadÅ‚y gorzej niÅ¼ mÄ™Å¼czyÅºni.\r\nMimo licznych badaÅ„ potwierdzajÄ…cych ten efekt, metaanalizy przeprowadzane od 2015 roku wykazaÅ‚y istnienie zÅ‚udzenia publikacyjnego (Flore & Wicherts, 2015). Wykrycie zÅ‚udzenia publikacyjnego jest moÅ¼liwe dziÄ™ki temu, Å¼e wyniki badaÅ„ (w formie liczb) powinny zachowywaÄ‡ siÄ™ (statystycznie) w okreÅ›lony sposÃ³b. Od kilkunastu lat rozwijajÄ… siÄ™ metody statystyczne pozwalajÄ…ce na wykrywanie tendencyjnoÅ›ci w publikowaniu artykuÅ‚Ã³w. ZÅ‚udzenia publikacyjne stanowiÄ… jeden z problemÃ³w skÅ‚adajÄ…cych siÄ™ na wspomniany wczeÅ›niej kryzys replikacyjny.\r\nW 2015 roku w prestiÅ¼owym czasopiÅ›mie naukowym â€Scienceâ€ ukazaÅ‚y siÄ™ wyniki badania, w ktÃ³rym autorzy powtÃ³rzyli 100 nigdy wczeÅ›niej niereplikowanych badaÅ„ psychologicznych. Tylko w 39% z nich udaÅ‚o siÄ™ otrzymaÄ‡ wynik taki jak w oryginalnych publikacjach (Open Science Collaboration, 2015). To i podobne badania, wraz z rosnÄ…cÄ… Å›wiadomoÅ›ciÄ… zÅ‚udzeÅ„ publikacyjnych, zapoczÄ…tkowaÅ‚o debatÄ™ na temat kryzysu replikacyjnoÅ›ci i rzetelnoÅ›ci wspÃ³Å‚czesnych praktyk naukowych. Szybko okazaÅ‚o siÄ™, Å¼e problem nie dotyczy tylko psychologii, lecz rÃ³wnieÅ¼ biologii, ekonomii i medycyny.\r\nKryzys replikacyjny ma wiele ÅºrÃ³deÅ‚, jednak najczÄ™Å›ciej wymienia siÄ™ zjawisko â€œpublikuj lub zgiÅ„â€ (publish or perish). WspÃ³Å‚czesna polityka ewaluacji pracownika naukowego i jego dorobku opiera siÄ™ na publikacjach. Im wiÄ™cej jest publikacji, im czÄ™Å›ciej sÄ… cytowane przez innych badaczy, w im lepszych czasopismach sÄ… publikowane, tym lepiej. Od wyniku ewaluacji zaleÅ¼y zatrudnienie i finansowanie badaÅ„ naukowca. Presja publikacyjna wywierana na badaczy w poÅ‚Ä…czeniu z tendencyjnoÅ›ciÄ… publikowania tylko â€œinteresujÄ…cychâ€ wynikÃ³w sprawiÅ‚a, Å¼e niekiedy popeÅ‚niajÄ… oni pewne metodologiczne naduÅ¼ycia. Jakie to naduÅ¼ycia? PomoÅ¼e nam to wyjaÅ›niÄ‡ martwy Å‚osoÅ› w rezonansie magnetycznym.\r\nZespÃ³Å‚ neuronaukowcÃ³w pod kierownictwem Craiga Bennetta przygotowywaÅ‚ siÄ™ do przeprowadzenia badania dotyczÄ…cego przetwarzania w mÃ³zgu emocjonalnych zdjÄ™Ä‡ przy uÅ¼yciu funkcjonalnego rezonansu magnetycznego (fMRI). UrzÄ…dzenie to dziaÅ‚a jak trÃ³jwymiarowa kamera pozwalajÄ…ca na rejestrowanie zmian w natÄ™Å¼eniu pola magnetycznego w voxelach (trÃ³jwymiarowych odpowiednikach pikseli). W zaleÅ¼noÅ›ci od tego czy krew jest utlenowana czy odtlenowana ma inne wÅ‚aÅ›ciwoÅ›ci magnetyczne, pozwala to sprawdzaÄ‡ poziom utlenowania krwi w rÃ³Å¼nych obszarach mÃ³zgu. Im wiÄ™kszÄ… aktywnoÅ›Ä‡ wykonuje dany region mÃ³zgu, tym wiÄ™cej tlenu zuÅ¼ywa. Pozwala to na sprawdzenie jakie obszary mÃ³zgu sÄ… bardziej aktywne w rÃ³Å¼nych warunkach eksperymentalnych.\r\nBennett przed wykonaniem wÅ‚aÅ›ciwych badaÅ„ zdecydowaÅ‚ siÄ™ na przetestowanie procedury wkÅ‚adajÄ…c do skanera trzykilogramowego martwego Å‚ososia. W trakcie pomiaru wyÅ›wietlano na specjalnym ekranie zdjÄ™cia, tak jak miaÅ‚oby to miejsce podczas prawdziwego badania, po czym zadawano Å‚ososiowi pytania, jakie emocje prezentujÄ… osoby na zdjÄ™ciach. Po przeanalizowaniu danych okazaÅ‚o siÄ™, Å¼e mÃ³zg martwego Å‚ososia wykazuje zwiÄ™kszonÄ… aktywnoÅ›Ä‡ pewnych regionÃ³w podczas oglÄ…dania zdjÄ™Ä‡, niÅ¼ w spoczynku (Bennett et al., 2009).\r\nWynikaÅ‚o to ze zjawiska znanego w statystyce jako problem wielokrotnych porÃ³wnaÅ„. W badaniach przy uÅ¼yciu fMRI porÃ³wnuje siÄ™ aktywnoÅ›Ä‡ w kaÅ¼dym voxelu pomiÄ™dzy warunkami. Voxeli w skanie fMRI jest kilkaset tysiÄ™cy. WykonujÄ…c tyle porÃ³wnaÅ„ prawdopodobieÅ„stwo, Å¼e wyjdzie nam choÄ‡ jeden wynik faÅ‚szywie pozytywny wynosi niemal 100%. PrawdopodobieÅ„stwo, Å¼e w kaÅ¼dym pojedynczym porÃ³wnaniu wystÄ…pi wynik faÅ‚szywie pozytywny nadal wynosi 5%, ale jeÅ›li Å›rednio 5% wokseli pokaÅ¼e nam rÃ³Å¼nice pomiÄ™dzy warunkami, to wykryjemy aktywnoÅ›Ä‡ nawet u martwego Å‚ososia (przez szum wynikajÄ…cy z fluktuacji pola magnetycznego). By poradziÄ‡ sobie z tym problemem stosuje siÄ™ wspÃ³Å‚czeÅ›nie odpowiednie poprawki, ktÃ³re utrzymujÄ… niÅ¼sze prawdopodobieÅ„stwo otrzymania wyniku faÅ‚szywie pozytywnego.\r\nCo ma wiÄ™c martwy Å‚osoÅ› do zjawiska â€œpublikuj lub giÅ„?â€ Naukowcy w swoich badaniach zwykle zbierajÄ… wiÄ™cej danych, niÅ¼ te ktÃ³re sÄ… potrzebne do weryfikacji a priori postawionych hipotez (pytaÅ„ badawczych, ktÃ³re stanowiÅ‚y motywacjÄ™ do przeprowadzenia badania). Dodatkowe dane zbiera siÄ™ w celu kontrolowania wpÅ‚ywu zmiennych potencjalnie wpÅ‚ywajÄ…cych na efekt, jak wspominaliÅ›my powyÅ¼ej. Ponadto, z uwagi, Å¼e zwykle przeprowadzenie badania jest kosztowne i czasochÅ‚onne zbiera siÄ™ jak najwiÄ™cej danych w celach eksploracyjnych (dodatkowych analiz nie wynikajÄ…cych bezpoÅ›rednio z hipotez). Presja publikacyjna, razem z tendencjÄ… wydawnictw do publikowania artykuÅ‚Ã³w potwierdzajÄ…cych efekt niÅ¼ mu zaprzeczajÄ…cych, sprawia, Å¼e jeÅ›li badacze nie potwierdzÄ… swoich zaÅ‚oÅ¼onych hipotez, szukajÄ… w danych jakiegoÅ› istotnego efektu, by zwiÄ™kszyÄ‡ szansÄ™ na publikacjÄ™.\r\nW tym momencie powracamy do problemu wielokrotnych porÃ³wnaÅ„. IstniejÄ… oczywiÅ›cie odpowiednie statystyczne Å›rodki zaradcze, niestety czasami badacze by zwiÄ™kszyÄ‡ sexapill artykuÅ‚Ã³w przedstawiajÄ… uzyskane istotne wyniki tak, jakby odpowiadaÅ‚y na ich pytanie badawcze postawione przed jego przeprowadzeniem badania. PomijajÄ… takÅ¼e w tekÅ›cie analizy, ktÃ³re nie wykrywaÅ‚y efektu. Nieodpowiedni dobÃ³r prÃ³by, tendencyjnoÅ›Ä‡ publikacyjna i presja kÅ‚adziona na badaczy skÅ‚ada siÄ™ na kryzys replikacyjny. Od kilku lat stosuje siÄ™ coraz czÄ™Å›ciej Å›rodki zaradcze w postaci czÄ™stszej publikacji replikacji, czasopism, ktÃ³re publikujÄ… tylko artykuÅ‚y mÃ³wiÄ…ce o braku efektu czy prerejestracji - przedstawieniu do wiadomoÅ›ci publicznej w formie krÃ³tkiego artykuÅ‚u, hipotez badawczych i metodologii przed zaczÄ™ciem badania.\r\nCzy kryzys replikacyjny oznacza, Å¼e nie moÅ¼emy ufaÄ‡ nauce? Nie. Nauka nadal pozostaje najlepszym narzÄ™dziem do poznawania otaczajÄ…cego nas Å›wiata i jest procesem samokrytycznym, to znaczy metody i zaÅ‚oÅ¼enia stosowane w nauce sÄ… poddawane ciÄ…gÅ‚ej krytyce przez Å›rodowisko naukowe. NaleÅ¼y tu dodaÄ‡, Å¼e kryzys replikacyjny moÅ¼e byÄ‡ zwiÄ…zany z umasowieniem nauki. Liczba osÃ³b otrzymujÄ…cych doktoraty stale siÄ™ zwiÄ™ksza, i to w paÅ„stwach, w ktÃ³rych liczba ludnoÅ›ci spada. Standaryzacja metod oceny pracownikÃ³w naukowych jest potrzebna, jednak w obecnej formie moÅ¼e nasilaÄ‡ omawiany problem.\r\nNa tym koÅ„czymy pierwszy wpis serii â€œJak rozumieÄ‡ naukÄ™.â€ DotknÄ™liÅ›my tylko wierzchoÅ‚ka gÃ³ry lodowej jeÅ›li chodzi o zagadnienia statystyczne i metodologiczne w nauce, jednak mam nadziejÄ™, Å¼e ten tekst bÄ™dzie stanowiÅ‚ interesujÄ…ce wprowadzenie do zagadnieÅ„ metodologicznych. Osobom zainteresowanym poszerzeniem swojej wiedzy polecamy przejrzenie zasobÃ³w w bazie wiedzy.\r\n\r\n\r\n\r\nBabbie, E. (2008). Podstawy badaÅ„ spoÅ‚ecznych (pp. 209â€“210). PWN.\r\n\r\n\r\nBennett, C. M., Miller, M. B., & Wolford, G. L. (2009). Neural correlates of interspecies perspective taking in the post-mortem atlantic salmon: An argument for multiple comparisons correction. Neuroimage, 47(Suppl 1), S125.\r\n\r\n\r\nChiavegatto, S., Quadros, I., Ambar, G., & Miczek, K. (2010). Individual vulnerability to escalated aggressive behavior by a low dose of alcohol: Decreased serotonin receptor mRNA in the prefrontal cortex of male mice. Genes, Brain and Behavior, 9(1), 110â€“119.\r\n\r\n\r\nFarrar, B. G., Voudouris, K., & Clayton, N. S. (2021). Replications, comparisons, sampling and the problem of representativeness in animal cognition research. Animal Behavior and Cognition, 8(2), 273.\r\n\r\n\r\nFlore, P. C., & Wicherts, J. M. (2015). Does stereotype threat influence performance of girls in stereotyped domains? A meta-analysis. Journal of School Psychology, 53(1), 25â€“44.\r\n\r\n\r\nGigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587â€“606.\r\n\r\n\r\nHanel, P. H., & Vione, K. C. (2016). Do student samples provide an accurate estimate of the general public? PloS One, 11(12), e0168354.\r\n\r\n\r\nLin, X., Genest, C., Banks, D. L., Molenberghs, G., Scott, D. W., & Wang, J.-L. (2014). Past, present, and future of statistical science (p. 44). CRC Press.\r\n\r\n\r\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251).\r\n\r\n\r\nPearl, J., & Mackenzie, D. (2021). Przyczyny i skutki rewolucyjna nauka wnioskowania przyczynowego. Copernicus Center Press.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-seria-jak-rozumie-nauk-metodologia-bada-i-statystyka/naser-tamimi-yG9pCqSOrAg-unsplash.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-13-o-odpowiedzialnoci-za-czyny-w-dobie-lotw-na-marsa/",
    "title": "O odpowiedzialnoÅ›ci za czyny w dobie lotÃ³w na Marsa",
    "description": "OpowieÅ›Ä‡ o wolnej woli",
    "author": [
      {
        "name": "Szymon MÄ…ka",
        "url": {}
      }
    ],
    "date": "2021-09-13",
    "categories": [
      "Refleksja"
    ],
    "contents": "\r\n\r\nContents\r\nBezwolne maszyny\r\nZÅ‚udzenie konieczne\r\nNie ma odpowiedzialnoÅ›ci, nie ma kary\r\nEpilog\r\n\r\nW Polsce (i na Å›wiecie) nie brakuje przykÅ‚adÃ³w postaw spoÅ‚ecznych sprzecznych wspÃ³Å‚czesnej wiedzy naukowej. Za przykÅ‚ad mogÄ… posÅ‚uÅ¼yÄ‡ postawy anty-GMO, anty-szczepieniowe, katastrofa klimatyczna, ktÃ³re sÄ… prawdopodobnie najbardziej znane. Jednak poza nimi wiele aspektÃ³w paÅ„stwa i spoÅ‚eczeÅ„stwa funkcjonuje â€œobokâ€ refleksji naukowej. Przyjrzyjmy siÄ™ mniej popularnemu zagadnieniu - problematyce wolnej woli.\r\nBezwolne maszyny\r\nW XIX wieku fizyka Å›wiÄ™ciÅ‚a triumfy, zwÅ‚aszcza w dziedzinach elektrycznoÅ›ci i magnetyzmu. Ã‰milie du ChÃ¢telet i James Joule niezaleÅ¼nie sformuÅ‚owali zasadÄ™ zachowania energii. Michael Faraday odkryÅ‚ zjawisko indukcji elektromagnetycznej, Georg Ohm opisaÅ‚ zwiÄ…zek pomiÄ™dzy natÄ™Å¼eniem a napiÄ™ciem prÄ…du elektrycznego. James Maxwell poÅ‚Ä…czyÅ‚ elektrycznoÅ›Ä‡ i magnetyzm w jednÄ… dziedzinÄ™ - elektromagnetyzm. Emil du Bois-Reymond odkryÅ‚, Å¼e informacja w ukÅ‚adzie nerwowym przekazywana jest przez sygnaÅ‚y elektryczne. XIX wiek obfitowaÅ‚ rÃ³wnieÅ¼ w nowe wynalazki powstaÅ‚e dziÄ™ki rozwijajÄ…cej siÄ™ wiedzy, m.in.: oÅ›wietlenie elektryczne, kolej parowa, telegram, telefon czy pierwsze aparaty fotograficzne.\r\nNiezwykÅ‚e dokonania naukowe oraz zdolnoÅ›Ä‡ fizyki do dokÅ‚adnego przewidywania zjawisk przyrodniczych zmusiÅ‚a wspÃ³Å‚czesnych do refleksji nad naturÄ… rzeczywistoÅ›ci. WedÅ‚ug Ã³wczesnej wiedzy, rzeczywistoÅ›Ä‡ fizyczna rzÄ…dzona byÅ‚a przez staÅ‚e i uniwersalne prawa fizyki. ZnajÄ…c je i dysponujÄ…c odpowiedniÄ… aparaturÄ…, moÅ¼na byÅ‚o przewidzieÄ‡ z absolutnÄ… dokÅ‚adnoÅ›ciÄ… dowolne zjawisko fizyczne, pomniejszonÄ… tylko o bÅ‚Ä…d pomiaru.\r\nPopularnÄ… metodÄ… rozwiÄ…zywania sporÃ³w jest rzut monetÄ…. To prosty, a przede wszystkim losowy sposÃ³b na decyzjÄ™ pomiÄ™dzy dwoma moÅ¼liwoÅ›ciami. Jednak zgodnie z prawami fizyki klasycznej, losowoÅ›Ä‡ rzutu monetÄ… nie wynika z jakiejÅ› rzeczywistej losowoÅ›ci. Przypadek jest tylko niedostatkiem informacji. BiorÄ…c pod uwagÄ™ wszystkie parametry fizyczne: siÅ‚Ä™ wyrzutu, prÄ™dkoÅ›Ä‡ kÄ…towÄ…, wiatr, grawitacjÄ™ etc, moÅ¼na by obliczyÄ‡ czy moneta upadnie awersem czy rewersem do doÅ‚u. Fizycy twierdzili, Å¼e wszystkie zdarzenia w Å›wiecie fizycznym, a wiÄ™c i dziaÅ‚anie czÅ‚owieka, sÄ… powiÄ…zane zwiÄ…zkiem przyczynowo-skutkowym - to znaczy, obecne zdarzenia miaÅ‚y byÄ‡ zdeterminowane przez zdarzenia przeszÅ‚e.\r\nPierre-Simon Laplace w roku 1814 dokonaÅ‚ pewnego eksperymentu myÅ›lowego: jeÅ›li wyobrazimy sobie istotÄ™ - demona, ktÃ³ry zna wszystkie obecne parametry fizyczne kaÅ¼dego atomu we wszechÅ›wiecie, bÄ™dzie on w stanie przewidzieÄ‡ wszystko co siÄ™ zdarzy w przyszÅ‚oÅ›ci. OznaczaÅ‚oby to, Å¼e wszechÅ›wiat jest caÅ‚kowicie zdeterminowany, istnieje tylko jedna przyszÅ‚oÅ›Ä‡, do ktÃ³rej wszyscy zmierzamy, a nasze dziaÅ‚ania sÄ… tylko skutkami zdarzeÅ„ z dalekiej przeszÅ‚oÅ›ci.\r\nTaki obraz Å›wiata nie byÅ‚ przychylny koncepcji wolnej woli - idei ciÄ™Å¼kiej do zdefiniowania, a w uproszczeniu oznaczajÄ…cej, Å¼e czÅ‚owiek, pomijajÄ…c ograniczenia fizyczne, jest w stanie caÅ‚kowicie niezaleÅ¼nie decydowaÄ‡ o swoich czynach. JedynÄ… przyczynÄ… jego zachowania jest Å›wiadoma decyzja,ktÃ³ra sama nie ma przyczyny - przynajmniej w Å›wiecie fizycznym. Termin ten ma silne ugruntowanie zarÃ³wno w tradycji chrzeÅ›cijaÅ„skiej, jak i filozoficznej. KartezjaÅ„ska opozycja umysÅ‚u i materii na lata zakorzeniÅ‚ siÄ™ tak w myÅ›li powszechnej jak i naukowej. Obie tradycje odwoÅ‚ujÄ… siÄ™ do niematerialnego bytu - duszy bÄ…dÅº umysÅ‚u, ktÃ³ry nieograniczony zasadami obejmujÄ…cymi materiÄ™ jest zdolny do â€˜â€™wolnejâ€™â€™ decyzji. ChoÄ‡ wspÃ³Å‚czeÅ›nie rzadziej widaÄ‡ odwoÅ‚ania do niematerialnego umysÅ‚u, wciÄ…Å¼ widaÄ‡ wpÅ‚yw dualizmu kartezjaÅ„skiego np. w opozycji myÅ›lenia racjonalnego i emocji (Damasio, 2011). Idea wolnej woli jest dla nas naturalna i wiÄ™kszoÅ›Ä‡ z nas doÅ›wiadcza poczucia sprawczoÅ›ci objawiajÄ…cego siÄ™ przekonaniem, Å¼e to my jesteÅ›my przyczynÄ… dziaÅ‚ania np. ruchu rÄ™ki.\r\nKoncept wolnej woli krytykowany byÅ‚ rÃ³wnieÅ¼ od strony filozoficznej. Artur Schopenhauer zauwaÅ¼yÅ‚, Å¼e ludzie w swojej naiwnoÅ›ci dowodzÄ… wolnoÅ›ci swej woli twierdzÄ…c: â€œmogÄ™ robiÄ‡ co chcÄ™.â€ Schopenhauer zapytaÅ‚ wtedy: skoro moÅ¼na robiÄ‡ co siÄ™ chce, czy moÅ¼na chcieÄ‡ co siÄ™ chce? Nawet jeÅ›li przyjÄ™libyÅ›my odpowiedÅº twierdzÄ…cÄ…, pytanie moÅ¼na rozszerzyÄ‡ do: â€czy moÅ¼na chcieÄ‡ tego, co chce siÄ™ chcieÄ‡?â€. W ten sposÃ³b Schopenhauer pokazaÅ‚, Å¼e nie mamy wpÅ‚ywu na nasze â€˜â€™chceniaâ€™â€™, ktÃ³re sÄ… przyczynÄ… naszych dziaÅ‚aÅ„ (Schopenhauer, 1991).\r\nOdkrycie fizyki kwantowej, Å¼e rzeczywistoÅ›Ä‡ na poziomie subatomowym jest probabilistyczna, zachwiaÅ‚a wiarÄ… w determinizm fizyczny. ByÅ‚a to idea tak niepojÄ™ta dla wielu fizykÃ³w, Å¼e, jak stale powtarza fizyk Andrzej Dragan w swoich wykÅ‚adach, wielu odkrywcÃ³w efektÃ³w kwantowych nie wierzyÅ‚o w swoje odkrycia aÅ¼ do Å›mierci. Sam problem losowoÅ›ci i determinizmu jest niezwykle ciekawy. Badania wykorzystujÄ…ce twierdzenie Bella wykazujÄ…, Å¼e stan kwantowych czÄ…stek splÄ…tanych nie wynika z ich historii, tzn. stanu jaki miaÅ‚y wczeÅ›niej, lecz w sposÃ³b probabilistyczny objawia siÄ™ podczas pomiaru. Eksperymenty nie dajÄ… caÅ‚kowitej pewnoÅ›ci (jak to zwykle bywa w nauce) o istnieniu losowoÅ›ci na kwantowym poziomie. Wynika to z problemÃ³w metodologicznych, takich jak: moÅ¼liwe wady eksperymentu czy niepewnoÅ›ci pomiarowe. Niemniej, wyniki tych badaÅ„ zdajÄ… siÄ™ mocno sugerowaÄ‡ prawdziwoÅ›Ä‡ postulatÃ³w fizyki kwantowej1.\r\nNiektÃ³rzy myÅ›liciele dopatrywali siÄ™ w zjawiskach kwantowych mechanizmÃ³w umoÅ¼liwiajÄ…cych istnienie wolnej woli oraz Å›wiadomoÅ›ci. Roger Penrose uwaÅ¼a, Å¼e zjawiska kwantowe zachodzÄ…ce w mÃ³zgu generujÄ… Å›wiadomoÅ›Ä‡. Z kolei Johnjoe McFadden uwaÅ¼a, Å¼e za wolnÄ… wolÄ™ i Å›wiadomoÅ›Ä‡ odpowiada pole elektromagnetyczne wytwarzane przez pracujÄ…cy mÃ³zg. NaleÅ¼y tu poczyniÄ‡ pewnÄ… uwagÄ™. Mechanika kwantowa jest najbardziej podstawowÄ… teoriÄ… naukowÄ… jakÄ… posiadamy, dotyczy zachowania mikroczÄ…stek, z ktÃ³rych zbudowane sÄ… wszystkie wiÄ™ksze obiekty fizyczne i podlegajÄ… jej prawom. W takim ujÄ™ciu wszystko jest â€œkwantowe.â€ Gdy jednak mowa o kwantowych efektach w mÃ³zgu, zwykle chodzi o nietrywialne efekty kwantowe takie jak tunelowanie, splÄ…tanie czy superpozycja. Efekty te potrzebujÄ… odpowiednich warunkÃ³w do zaistnienia. Obliczenia pokazujÄ…, Å¼e te efekty kwantowe w mÃ³zgu ulegajÄ… dekoherencji zbyt szybko, by wpÅ‚ywaÄ‡ lub zarzÄ…dzaÄ‡ pracÄ… neuronÃ³w (Seife, 2000)2. Nawet jeÅ›li dopuÅ›cimy pewnÄ… losowoÅ›Ä‡ w pracy mÃ³zgu, problem z wolnÄ… wolÄ… pozostaÅ‚ zasadniczo ten sam. Nawet jeÅ›li istnieje pewna losowoÅ›Ä‡ w dziaÅ‚aniu umysÅ‚u, to nie umysÅ‚ jÄ… generuje. JeÅ›li decyzja jest dzieÅ‚em przypadku, nie moÅ¼e byÄ‡ wolna, poniewaÅ¼ jest przypadkowa. Ponadto pojawia siÄ™ teÅ¼ pytanie - jak taka losowoÅ›Ä‡ miaÅ‚aby siÄ™ manifestowaÄ‡. Na poziomie neuronalnym, gdyby losowe neurony losowo generowaÅ‚yby potencjaÅ‚y czynnoÅ›ciowe, zakÅ‚Ã³ciÅ‚oby to pracÄ™ mÃ³zgu. LosowoÅ›Ä‡ musiaÅ‚aby byÄ‡ niezwykle precyzyjna. Nie znajdziemy jej jednak teÅ¼ na poziomie Å›wiadomym, gdyÅ¼ ludzie nie sÄ… w stanie np. wygenerowaÄ‡ losowych sekwencji liczb (Figurska et al., 2008; Schulz et al., 2012).\r\nPoszukiwania kwantowej Å›wiadomoÅ›ci przypominajÄ… pod pewnymi wzglÄ™dami problem homunkulusa. Hipotetyczny homunkulus to maÅ‚y zarzÄ…dca w naszym mÃ³zgu, ktÃ³ry â€œoglÄ…daâ€ informacje dostarczane przez zmysÅ‚y, integruje je i podejmuje decyzje. W toku historii wielokrotnie poszukiwano owego homunkulusa czy to w postaci niematerialnej duszy czy jakiegoÅ› konkretnego obszaru w mÃ³zgu, ktÃ³ry odpowiada za Å›wiadomoÅ›Ä‡. Nie odnaleziono jednak niczego takiego, a Å›wiadomoÅ›Ä‡ wydaje siÄ™ wyÅ‚aniaÄ‡ ze zsynchronizowanej pracy caÅ‚ego mÃ³zgu. W naukowym obrazie Å›wiata, zawieszeni pomiÄ™dzy deterministycznymi i stochastycznymi procesami, nie znajdziemy miejsca dla wolnoÅ›ci przez duÅ¼e â€œW.â€\r\nHomunkulus\r\nProblemem organu zarzÄ…dczego jest to, Å¼e jego dziaÅ‚eniem rÃ³wnieÅ¼ powinno coÅ› zarzÄ…dzaÄ‡, i tak ad infinitum.\r\nZÅ‚udzenie konieczne\r\nMimo to, wielu ludzi - w tym naukowcÃ³w, nie godzi siÄ™ z takim stanem rzeczy. DoÅ›wiadczenie sprawczoÅ›ci jest tak potÄ™Å¼ne, Å¼e stanowi samoistny argument za istnieniem wolnej woli. Hannah Arendt pisaÅ‚a â€œuznajÄ™ wewnÄ™trzne Å›wiadectwo Â»ja chcÄ™Â« za dostateczny dowÃ³d realnoÅ›ci fenomenu woliâ€ (Arendt, 1996), jakoby samo istnienie subiektywnego poczucia woli byÅ‚oby wystarczajÄ…ce, by udowodniÄ‡ istnienie wolnych decyzji. Mgliste stwierdzenie, Å¼e struktura rzeczywistoÅ›ci, w ktÃ³rej siÄ™ znajdujemy, zaprzecza moÅ¼liwoÅ›ci istnienia takiego fenomenu, moÅ¼e byÄ‡ dla wielu nieprzekonujÄ…ca, gdy tak Å¼ywo czujemy naszÄ… wÅ‚asnÄ… sprawczoÅ›Ä‡. NiektÃ³rzy myÅ›liciele, by znaleÅºÄ‡ empiryczne dowody za lub przeciw wolnej woli, zdecydowali poszukaÄ‡ ich w prÄ™Å¼nie rozwijajÄ…cej siÄ™ dziedzinie zajmujÄ…cej siÄ™ badaniem umysÅ‚u - szeroko pojÄ™tej neuronauki.\r\nNajsÅ‚ynniejszymi eksperymentami dotyczÄ…cymi wolnej woli sÄ… eksperymenty Libeta i pÃ³Åºniej Haynesa, ktÃ³rzy wykazali, Å¼e w przypadku prostych decyzji, takich jak naciÅ›niÄ™cie jednego z dwÃ³ch guzikÃ³w, badacz jest w stanie przewidzieÄ‡ na podstawie aktywnoÅ›ci mÃ³zgu co wybierze badany, zanim uÅ›wiadomi on sobie swojÄ… decyzjÄ™ (Soon et al., 2013). ChoÄ‡ te eksperymenty sÄ… najbardziej znane, poniewaÅ¼ explicite poruszaÅ‚y tematykÄ™ wolnej woli, nie znajdziemy wielu badaÅ„ studiujÄ…cych wolnÄ… wolÄ™. Wydaje siÄ™ wrÄ™cz (pozornie), Å¼e na temat wolnej woli neuronauka ma niewiele do powiedzenia. Jest to podyktowane faktem, Å¼e na pytanie o wolnÄ… wolÄ™ (choÄ‡ nie zostaÅ‚o zadane) zostaÅ‚a juÅ¼ udzielona odpowiedÅº. ZaÅ‚oÅ¼enia metodologiczne neuronauk i uzyskana dziÄ™ki nim wiedza, spÃ³jna z resztÄ… naszej wiedzy o Å›wiecie, wykluczajÄ… istnienie fenomenu, ktÃ³ry by jej jawnie przeczyÅ‚. MÃ³zg jest tylko (i aÅ¼) biologicznym komputerem. Laptop, na ktÃ³rym piszÄ™ ten tekst teÅ¼ potrafi podejmowaÄ‡ decyzje, niemniej nie posiada z tego tytuÅ‚u wolnej woli. Wydaje siÄ™, Å¼e wolna wola jest pojÄ™ciem, ktÃ³ry ciÄ™Å¼ko wpisaÄ‡ we wspÃ³Å‚czesnÄ… naukÄ™.\r\n\r\nCelowo nie rozwijam problemu Å›wiadomoÅ›ci, ktÃ³ry, moim zdaniem, jest bardziej skomplikowany niÅ¼ problematyka wolnej woli. Na szczÄ™Å›cie ta ostatnia nie wymaga prÃ³b odpowiedzenia na pytanie â€œczym jest Å›wiadomoÅ›Ä‡?â€\r\nMÃ³zg determinuje nasze decyzje poprzez obliczenia, a owe obliczenia determinowane sÄ… przez interakcje genÃ³w i Å›rodowiska. Bez odwoÅ‚ania do metafizycznych efektÃ³w, nie ma miejsca na Ã³w wolny element. Niemniej, filozof Phillipe Meyer w ksiÄ…Å¼ce â€œZÅ‚udzenie konieczneâ€ zauwaÅ¼a, Å¼e chociaÅ¼ neuronaukowcy udowodnili materialnoÅ›Ä‡ funkcjonowania mÃ³zgu w kaÅ¼dym aspekcie, wielu naukowcÃ³w, w tym niektÃ³rzy neuronaukowcy, odrzucajÄ… czysto materialny charakter pracy mÃ³zgu i to niekoniecznie w oderwaniu od swojej pracy naukowej (Meyer, 1998). John Eccles, laureat nagrody Nobla z fizjologii i medycyny (za badania nad synapsami), napisaÅ‚ artykuÅ‚ w ktÃ³rym twierdziÅ‚, Å¼e â€[mÃ³zg odznacza siÄ™] wraÅ¼liwoÅ›ciÄ… innego rodzaju niÅ¼ jakikolwiek instrument fizycznyâ€ oraz Å¼e â€umysÅ‚ osiÄ…ga poÅ‚Ä…czenie z mÃ³zgiem za poÅ›rednictwem przestrzennoczasowych pÃ³l oddziaÅ‚ywania, ktÃ³re stajÄ… siÄ™ aktywne dziÄ™ki tej wyjÄ…tkowej [â€¦] funkcji pobudzonej kory mÃ³zgowejâ€. Znany neuronaukowiec Michael Gazzaniga tak skomentowaÅ‚ poglÄ…dy Eccelsa: â€œNo, no! PrzecieÅ¼ to czyste wudu, opisane wyszukanym jÄ™zykiem. Eccles zastÄ…piÅ‚ KartezjuszowÄ… szyszynkÄ™ tajemniczÄ… wraÅ¼liwoÅ›ciÄ… pobudzonej kory mÃ³zgowej. DwieÅ›cie lat po Kartezjuszu kontynuowaÅ‚ kartezjaÅ„skÄ… tradycjÄ™ dualizmu, mimo Å¼e spÄ™dzaÅ‚ szeÅ›Ä‡dziesiÄ…t godzin tygodniowo na badaniu i rejestrowaniu aktywnoÅ›ci neuronÃ³w i choÄ‡ we wszystkich innych sprawach byÅ‚ gorliwym wyznawcÄ… determinizmu. To po prostu niepojÄ™te.â€ (Gazzaniga, 2020).\r\nWydaje siÄ™, Å¼e mamy do czynienia tutaj przykÅ‚adem desperackiej prÃ³by ocalenia tego, w co doÅ›Ä‡ dÅ‚ugo wierzyliÅ›my, Å¼e nas ludzi wyrÃ³Å¼nia. Wspomniany juÅ¼ Kartezjusz wierzyÅ‚, Å¼e zwierzÄ™ta sÄ… maszynami napÄ™dzanymi przez skomplikowane mechanizmy (Descartes, 1980). Nie chciaÅ‚ jednak dopuÅ›ciÄ‡, Å¼e podobnie moÅ¼e byÄ‡ w wypadku ludzi. Mimo oporÃ³w wobec materialistycznych i mechanicystycznych wyjaÅ›nieÅ„ dziaÅ‚ania ludzkiego umysÅ‚u, neuronauki zdajÄ… siÄ™ sukcesywnie pokazywaÄ‡, Å¼e nasza Å›wiadomoÅ›Ä‡, poczucie sprawczoÅ›ci i ciÄ…gÅ‚oÅ›ci â€œja,â€ sÄ… sprytnymi zÅ‚udzeniami.\r\nBy zobaczyÄ‡ jakie problemy nastrÄ™cza pojÄ™cie wolnej woli, przyjrzyjmy siÄ™ kilku badaniom naukowym. Michael Gazzaniga badaÅ‚ pacjentÃ³w po komisurotomi - chirurgicznym rozszczepieniu pÃ³Å‚kul mÃ³zgu wykonanym w celu zÅ‚agodzenia ciÄ™Å¼kich atakÃ³w epilepsji. W wyniku tego obie pÃ³Å‚kule dostawaÅ‚y te same dane wejÅ›ciowe z obszarÃ³w podkorowych, ale dziaÅ‚ajÄ… niezaleÅ¼nie od siebie. PÃ³Å‚kule nie dzielÄ… informacji wzrokowej, prawe pole widzenia dochodzi tylko do lewej pÃ³Å‚kuli, a lewe do prawej. JednoczeÅ›nie u wiÄ™kszoÅ›ci ludzi obszary zwiÄ…zane z mowÄ… mieszczÄ… siÄ™ w lewej pÃ³Å‚kuli, sÅ‚owa produkowane sÄ… bez udziaÅ‚u prawej pÃ³Å‚kuli. Pozwala to na dostarczanie rÃ³Å¼nych informacji obu pÃ³Å‚kulom: â€œPokazaliÅ›my pacjentowi dwa obrazki: w prawej poÅ‚owie jego pola widzenia umieÅ›ciliÅ›my wizerunek kurzej Å‚apy, tak aby lewa pÃ³Å‚kula mÃ³zgu widziaÅ‚a tylko ten obrazek, a w lewej poÅ‚owie â€“ obraz zaÅ›nieÅ¼onego podwÃ³rka, tak aby pÃ³Å‚kula prawa nie widziaÅ‚a niczego poza nim. NastÄ™pnie poÅ‚oÅ¼yliÅ›my przed pacjentem kilkanaÅ›cie rysunkÃ³w, widocznych dla obu pÃ³Å‚kul mÃ³zgu, i poprosiliÅ›my, Å¼eby wybraÅ‚ spoÅ›rÃ³d nich obrazki kojarzÄ…ce mu siÄ™ z tym, co przed chwilÄ… zobaczyÅ‚. Jego lewa rÄ™ka wskazaÅ‚a szuflÄ™ (ktÃ³ra stanowiÅ‚a najlepszÄ… odpowiedÅº na widok zaÅ›nieÅ¼onego podwÃ³rka), a prawa â€“ kurÄ™ (byÅ‚a to najbardziej trafna reakcja na rysunek kurzej Å‚apy). Kiedy zapytaliÅ›my, dlaczego wybraÅ‚ wÅ‚aÅ›nie te obrazki, jego oÅ›rodek mowy zlokalizowany w lewej pÃ³Å‚kuli odparÅ‚: â€Och, to bardzo proste. Kurza Å‚apa kojarzy mi siÄ™ z kurÄ…,â€ z Å‚atwoÅ›ciÄ… wyjaÅ›niajÄ…c to, co wiedziaÅ‚. ZobaczyÅ‚ kurzÄ… Å‚apÄ™. NastÄ™pnie, spojrzawszy na swojÄ… lewÄ… dÅ‚oÅ„ wskazujÄ…cÄ… szuflÄ™, mÄ™Å¼czyzna dodaÅ‚ bez wahania: â€A szufla jest potrzebna do posprzÄ…tania kurnikaâ€[â€¦] InteresujÄ…cy wydawaÅ‚ siÄ™ fakt, Å¼e lewa pÃ³Å‚kula nie odpowiedziaÅ‚a: â€Nie wiemâ€, co byÅ‚oby zgodne z prawdÄ…. Zamiast tego wymyÅ›liÅ‚a odpowiedÅº pasujÄ…cÄ… do sytuacji. KonfabulowaÅ‚a, skÅ‚adajÄ…c informacje, ktÃ³rymi dysponowaÅ‚a, w sensownÄ… odpowiedÅº. NazwaliÅ›my ten lewopÃ³Å‚kulowy proces interpretatorem.\" (Gazzaniga, 2013)\r\nGazzaniga kontynuowaÅ‚ badania nad interpretatorem. Jest to moduÅ‚ wyspecjalizowany w tworzeniu spÃ³jnej narracji, umoÅ¼liwia rÃ³wnieÅ¼ tworzenie abstrakcyjnych relacji przyczynowo-skutkowych. Pozwala, na podstawie rÃ³Å¼nych przesÅ‚anek, wyciÄ…gaÄ‡ logiczne wnioski, wychodzÄ…ce poza czystÄ… percepcjÄ™ rzeczywistoÅ›ci. LewopÃ³Å‚kulowy interpretator nieustannie szuka potencjalnych przyczyn rÃ³Å¼nych zdarzeÅ„, jednak struktura, znajdujÄ…ca siÄ™ prawdopodobnie w prawym pÅ‚acie ciemieniowym, hamuje dziaÅ‚anie interpretatora, kiedy ten zaczyna tworzyÄ‡ historie zbytnio odstajÄ…ce od rzeczywistoÅ›ci. Gdy pÃ³Å‚kule sÄ… rozdzielone, sygnaÅ‚ hamujÄ…cy nie dochodzi do lewej pÃ³Å‚kuli, a interpretator moÅ¼e nieskrÄ™powanie snuÄ‡ swoje historie. Prowadzi to do systematycznych konfabulacji pacjentÃ³w z rozdzielonymi pÃ³Å‚kulami, poniewaÅ¼ nawet najmniej prawdopodobne wyjaÅ›nienia dostajÄ… siÄ™ do Å›wiadomoÅ›ci jako wiarygodne przyczyny. PrzykÅ‚adowo, gdy Gazzaniga zaprezentowaÅ‚ prawej pÃ³Å‚kuli pacjentki przeraÅ¼ajÄ…cy film, ta zaaktywizowaÅ‚a ukÅ‚ad wspÃ³Å‚czulny, wprowadzajÄ…c fizjologicznÄ… reakcjÄ™ strachu - przyÅ›pieszone bicie serca, potliwoÅ›Ä‡ i aktywnoÅ›Ä‡ miÄ™Å›niowÄ…. PoniewaÅ¼ obszary podkorowe przekazujÄ… informacje o aktywnoÅ›ci ukÅ‚adu wspÃ³Å‚czulnego prawej i lewej pÃ³Å‚kuli, lewa pÃ³Å‚kula â€œwiedziaÅ‚a,â€ Å¼e organizm jest przestraszony, nie wiedziaÅ‚a jednak dlaczego. Interpretator na podstawie dostÄ™pnych informacji - obecnoÅ›ci Gazzanigi w pomieszczeniu - natychmiast wytworzyÅ‚ wyjaÅ›nienie tego stanu: to doktor Gazzaniga jest przeraÅ¼ajÄ…cy.\r\nDalsza praca zespoÅ‚u Gazzanigi pokazaÅ‚a role lewopÃ³Å‚kulowego interpretatora u ludzi zdrowych. Sprawia on, Å¼e mamy poczucie spÃ³jnej narracji w naszych dziaÅ‚aniach. Interpretator jest jednak tak dobry jak informacje, ktÃ³re do niego docierajÄ…, a wyniki eksperymentÃ³w pokazujÄ…, Å¼e docierajÄ… do niego rezultaty, czy teÅ¼ wyniki dziaÅ‚aÅ„ innych moduÅ‚Ã³w mÃ³zgu, nie ma on jednak dostÄ™pu do ich obliczeÅ„. PrzykÅ‚adem jest coÅ›, co nazywamy pamiÄ™ciÄ… procesualnÄ…. W odrÃ³Å¼nieniu do pamiÄ™ci deklaratywnej, czyli takiej, do ktÃ³rej mamy Å›wiadomy dostÄ™p, pewne czynnoÅ›ci potrafimy wykonywaÄ‡ doskonale, ale nie jesteÅ›my wÅ‚aÅ›ciwie w stanie wytÅ‚umaczyÄ‡ jak to robimy. PrzykÅ‚adem moÅ¼e byÄ‡ jazda na rowerze. WyjaÅ›nienie komuÅ› jak naleÅ¼y jeÅºdziÄ‡ na rowerze jest skazane na poraÅ¼kÄ™. Jest to wbrew pozorom bardziej skomplikowane niÅ¼ pedaÅ‚owanie i krÄ™cenie kierownicÄ…. Wymaga poczucia rÃ³wnowagi i odpowiedniego balansowania ciaÅ‚em. Jednak osoby, ktÃ³re potrafiÄ… jeÅºdziÄ‡ na rowerze wcale nie czujÄ…, by coÅ› takiego robiÅ‚y.\r\nInnym przykÅ‚adem moÅ¼e byÄ‡ wiedza ekspercka. Ludzie specjalizujÄ…cy siÄ™ w pewnych zadaniach, potrafiÄ…cy wykonywaÄ‡ je z niesamowitÄ… biegÅ‚oÅ›ciÄ…, czÄ™sto nie majÄ… wglÄ…du w to jak to robiÄ…. Gazzaniga zaprosiÅ‚ do laboratorium mistrza szachowego Patricka Wolffa, ktÃ³ry miaÅ‚ za zadanie odtworzyÄ‡ ukÅ‚ad pionkÃ³w na szachownicy, oglÄ…dajÄ…c go przez 5 sekund. Wolff byÅ‚ w stanie to zrobiÄ‡, jeÅ›li ukÅ‚ad, ktÃ³ry oglÄ…daÅ‚ byÅ‚ sensowny z szachowego punktu widzenia. JeÅ›li jednak pionki byÅ‚y ustawione losowo, radziÅ‚ sobie z podobnÄ… skutecznoÅ›ciÄ… jak kaÅ¼dy inny czÅ‚owiek. Nie miaÅ‚ wiÄ™c Å›wietnej pamiÄ™ci wzrokowej, lecz jego moduÅ‚ zajmujÄ…cy siÄ™ grupowaniem percepcyjnym nauczony wieloletniÄ… grÄ… w szachy bÅ‚yskawicznie rozpoznawaÅ‚ zÅ‚oÅ¼one wzorce szachowe. Sam Wolff byÅ‚ tego nieÅ›wiadomy, nie byÅ‚ w stanie powiedzieÄ‡ jak udaje mu siÄ™ osiÄ…gnÄ…Ä‡ dobry wynik w jednym przypadku i przeciÄ™tny w drugim. Jego interpretator dostawaÅ‚ tylko informacje o koÅ„cowym wyniku (odtworzeniu szachownicy), nie miaÅ‚ jednak informacji jak udaÅ‚o siÄ™ to osiÄ…gnÄ…Ä‡.\r\nInformacje przekazywane do Å›wiadomoÅ›ci przez mÃ³zg sÄ… selektywne. Nie wszystko co mÃ³zg zauwaÅ¼y i przetworzy dostanie siÄ™ do naszej Å›wiadomoÅ›ci, czy teÅ¼ jakby powiedziaÅ‚ Gazzaniga, do interpretatora. JeÅ›li badanym zostanie zaprezentowana przez kilka-kilkanaÅ›cie milisekund przestraszona twarz, badani jej Å›wiadomie nie zauwaÅ¼Ä…, jednak dostrzeÅ¼emy zwiÄ™kszonÄ… aktywnoÅ›Ä‡ ciaÅ‚a migdaÅ‚owatego - struktury przetwarzajÄ…cej bodÅºce zagraÅ¼ajÄ…ce (Whalen et al., 1998). Jest to kolejny z przykÅ‚adÃ³w, Å¼e nie mamy Å›wiadomego dostÄ™pu do pewnych informacji przetwarzanych przez nasz mÃ³zg.\r\nMoÅ¼na powiedzieÄ‡, Å¼e mÃ³zg regularnie nas â€œoszukuje.â€ JeÅ›li dotkniemy palcem do nosa, jednoczeÅ›nie poczujemy dotyk w nosie i w palcu, mimo Å¼e sygnaÅ‚ z palca dotrze do mÃ³zgu znacznie pÃ³Åºniej. MÃ³zg tworzy wÅ‚asnÄ… reprezentacjÄ™ czasu przekazywanÄ… Å›wiadomoÅ›ci. Badani, u ktÃ³rych po wykonaniu spontanicznej akcji zaaplikowano silny impuls magnetyczny na pole przedruchowe, postrzegali, Å¼e intencja wykonania ruchu pojawiÅ‚a siÄ™ wczeÅ›niej niÅ¼ gdy nie otrzymali impulsu (Lau et al., 2007).\r\nSposÃ³b przetwarzania informacji przez mÃ³zg ma krytyczne znaczenie nie tylko dla postrzegania sprawczoÅ›ci u siebie samych, ale i u innych ludzi. ZespÃ³Å‚ Rebecki Saxe zakÅ‚Ã³ciÅ‚ badanym dziaÅ‚anie pewnego obszaru mÃ³zgu - prawego styku skroniowo-ciemieniowego, za pomocÄ… impulsu magnetycznego. NastÄ™pnie przedstawiÅ‚a im cztery historie, w ktÃ³rych Grace podaÅ‚a swojemu przyjacielowi cukierniczkÄ™ myÅ›lÄ…c, Å¼e jest tam cukier/trucizna, gdy naprawdÄ™ byÅ‚ tam cukier/trucizna. Badani, ktÃ³rzy nie otrzymali impulsu, negatywnie oceniali Grace, gdy ta myÅ›laÅ‚a, Å¼e w cukierniczce jest trucizna, niezaleÅ¼nie od tego czy faktycznie tam byÅ‚a. Badani ktÃ³rzy impuls otrzymali, Å‚agodniej oceniali Grace, gdy ta myÅ›lÄ…c, Å¼e podaje truciznÄ™, podaÅ‚a cukier (Young et al., 2010). Badani, u ktÃ³rych praca styku skroniowo-ciemieniowego zostaÅ‚a zakÅ‚Ã³cona, skupiali siÄ™ na skutkach dziaÅ‚aÅ„ Grace, nie biorÄ…c pod uwagÄ™ jej intencji. Prawy styk ciemieniowo-skroniowy jest czÄ™Å›ciÄ… obwodu nerwowego zajmujÄ…cego siÄ™ teoriÄ… umysÅ‚u - zdolnoÅ›ciÄ… pozwalajÄ…cÄ… miÄ™dzy innymi na przejÄ™cie perspektywy innych osÃ³b. SposÃ³b w jaki badani dokonujÄ… oceny moralnej Grace zaleÅ¼y od pracy tego obwodu. Gdyby mieli zadecydowaÄ‡ czy i jakÄ… karÄ™ wymierzyÄ‡ Grace, informacje przetwarzane przez styk skroniowo-ciemieniowy miaÅ‚by spore znaczenie dla tej decyzji.\r\nNie ma odpowiedzialnoÅ›ci, nie ma kary\r\nPowyÅ¼sze przykÅ‚ady miaÅ‚y za zadanie pokazaÄ‡, Å¼e odczuwana przez nas rzeczywistoÅ›Ä‡ jest sprytnÄ… iluzjÄ…, ktÃ³ra nie zawsze przystaje do rzeczywistoÅ›ci fizycznej. ChoÄ‡ najbardziej spektakularnie widaÄ‡ to u ludzi z uszkodzeniami mÃ³zgu, postrzeganie ludzi â€œzdrowychâ€ teÅ¼ jest wynikiem pracy mÃ³zgu, pracy niezaleÅ¼nej od nas samych. Wydaje siÄ™, Å¼e taki brutalny mechanicyzm odziera Å›wiat z jakiegokolwiek sensu. Jednak wiedza o tym, Å¼e â€œwolna wolaâ€ nie istnieje, raczej nie zmieni sposobu w jaki zachowujemy siÄ™ na co dzieÅ„. Ma to jednak znaczenie dla praktyk spoÅ‚ecznych. Jednym z przykÅ‚adÃ³w moÅ¼e byÄ‡ system penitencjarny. System penitencjarny stawia sobie rÃ³Å¼ne cele: odizolowanie jednostek niebezpiecznych od reszty spoÅ‚eczeÅ„stwa, resocjalizacji czy karze. Kara opiera siÄ™ na pojÄ™ciu odpowiedzialnoÅ›ci, jest odpÅ‚atÄ… (sankcjonowanÄ… zemstÄ…) za przestÄ™pstwo.\r\nOkoÅ‚o 180 lat temu stworzono reguÅ‚Ä™ Mâ€™Naghtena - precyzyjne pojÄ™cie niepoczytalnoÅ›ci. StaÅ‚o siÄ™ to podczas procesu Daniela Mâ€™Naghtena w Wielkiej Brytanii, oskarÅ¼onego o zabÃ³jstwo i uniewinnionego z powodu choroby psychicznej. Wyrok ten wywoÅ‚aÅ‚ ogromne kontrowersje, w wyniku czego powoÅ‚ano komisjÄ™, ktÃ³ra miaÅ‚a okreÅ›liÄ‡ dokÅ‚adne kryteria niepoczytalnoÅ›ci. Osoba, ktÃ³ra nie jest Å›wiadoma, Å¼e popeÅ‚nia przestÄ™pstwo, nie wie, Å¼e to co zrobiÅ‚a jest â€œzÅ‚eâ€ w chwili popeÅ‚nienia czynu nie moÅ¼e podlegaÄ‡ karze. ByÅ‚ to przeÅ‚om spoÅ‚eczny, zdecydowano bowiem, Å¼e istniejÄ… wyjÄ…tki od odpowiedzialnoÅ›ci (MaroÅ„, 2018).\r\n\r\nNie byÅ‚ to jednak pierwszy wyrok biorÄ…cy pod uwagÄ™ poczytalnoÅ›Ä‡ sprawcy. IstniaÅ‚ precedens (Rex v. Arnold z 1724) zwany â€œstandardem dzikiej bestii,â€ jednak dopiero po sprawie Mâ€™Naghtena zestandaryzowano wskaÅºniki niepoczytalnoÅ›ci.\r\nIdea ta, choÄ‡ z oporem, na staÅ‚e zagoÅ›ciÅ‚a w zachodnich systemach prawnych. Powoli pojawiÅ‚a siÄ™ w innych obszarach spoÅ‚ecznych - sto piÄ™Ä‡dziesiÄ…t lat po sprawie Mâ€™Naghtena KoÅ›ciÃ³Å‚ Katolicki - dla ktÃ³rego pojÄ™cie wolnej woli i odpowiedzialnoÅ›ci za czyny sÄ… niezwykle waÅ¼ne, rÃ³wnieÅ¼ uznaÅ‚ owy argument. Jak moÅ¼emy przeczytaÄ‡ w Katechizmie: â€œCiÄ™Å¼kie zaburzenia psychiczne, strach lub powaÅ¼na obawa przed prÃ³bÄ…, cierpieniem lub torturami mogÄ… zmniejszyÄ‡ odpowiedzialnoÅ›Ä‡ samobÃ³jcyâ€ (Katechizm KoÅ›cioÅ‚a Katolickiego, 2015), uznajÄ…c, Å¼e depresja zmniejsza opdowiedzialnoÅ›c za czyn uwaÅ¼any jako grzech ciÄ™Å¼ki.\r\nChoÄ‡ Ã³wczeÅ›nie idea, Å¼e czÅ‚owiek moÅ¼e nie odpowiadaÄ‡ za swoje czynny byÅ‚a myÅ›lÄ… nowatorskÄ…, funkcjonuje do dziÅ› w stanie praktycznie nie zmienionym, mimo znacznego postÄ™pu nauki. ZnaleÅºliÅ›my siÄ™ wspÃ³Å‚czeÅ›nie w miejscu, gdzie dzielimy ludzi na zdolnych i tych niezdolnych do odpowiedzialnoÅ›ci. Kryterium rozrÃ³Å¼nienia polega na poprawnoÅ›ci funkcjonowania mÃ³zgu. Problem w tym, Å¼e wedÅ‚ug wiedzy neuronaukowej ciÄ™Å¼ko jest mÃ³wiÄ‡ o odpowiedzialnoÅ›ci kogokolwiek (Farah, 2005). Przytoczone wczeÅ›niej przykÅ‚ady miaÅ‚y pokazaÄ‡, Å¼e zachowanie zaleÅ¼ne jest od pracy mÃ³zgu, na ktÃ³rÄ… jednostka nie ma wpÅ‚ywu. Neurobiolog Robert Sapolsky poddaÅ‚ pod wÄ…tpliwoÅ›Ä‡ obecnÄ… definicje niepoczytalnoÅ›ci skupiajÄ…cÄ… siÄ™ na nieÅ›wiadomoÅ›ci sprawcy, Å¼e popeÅ‚nia czyn zabroniony. Istotnym zespoÅ‚em oÅ›rodkÃ³w mÃ³zgÃ³w odpowiedzialnych za hamowanie zachowaÅ„ impulsywnych jest kora przedczoÅ‚owa. Osoby z zaburzonym jej funkcjonowaniem doskonale zdajÄ… sobie sprawÄ™ z swoich zachowaÅ„, nie sÄ… jednak w stanie siÄ™ od nich powstrzymaÄ‡ (Sapolsky, 2006).\r\nWÅ›rÃ³d wielu neuronaukowcÃ³w i przedstawicieli innych dziedzin istnieje poglÄ…d, Å¼e kara oparta na odpowiedzialnoÅ›ci, ktÃ³ra jest sankcjonowanÄ… prawnie zemstÄ…, jest co najmniej wÄ…tpliwa. Richard Dawkins stwierdziÅ‚ â€œOdpÅ‚ata jako zasada moralna jest niezgodna z naukowym poglÄ…dem na ludzkie zachowanie. Jako naukowcy wierzymy, Å¼e ludzkie mÃ³zgi, chociaÅ¼ mogÄ… nie dziaÅ‚aÄ‡ w taki sam sposÃ³b, jak komputery stworzone przez czÅ‚owieka, sÄ… tak samo rzÄ…dzone prawami fizyki. Kiedy komputer dziaÅ‚a nieprawidÅ‚owo, nie karzemy go. Odnajdujemy problem i naprawiamy go, zwykle poprzez wymianÄ™ uszkodzonego komponentu, sprzÄ™towego lub programowegoâ€ (Dawkins, 2006). Joshua Green i Jonathan Cohen uwaÅ¼ajÄ…, Å¼e naleÅ¼y odrzuciÄ‡ koncept retrybucji, a zamiast tego skupiÄ‡ siÄ™ na odseparowaniu od spoÅ‚eczeÅ„stwa (jeÅ›li to potrzebne) i resocjalizacji. PostulujÄ…, Å¼e wraz ze zrozumieniem przyczyn dziaÅ‚aÅ„ przestÄ™pcÃ³w, powinniÅ›my zmieniÄ‡ stosunek do nich (Greene & Cohen, 2004). Jest to logicznÄ… konsekwencjÄ… faktu, Å¼e retrybucje wobec osÃ³b psychicznie chorych uwaÅ¼amy za niehumanitarne (poniewaÅ¼ nie majÄ… wpÅ‚ywu na to co robiÄ…), to zgodnie z wiedzÄ… neuronaukowÄ… nie powinniÅ›my stosowaÄ‡ retrybucji wobec nikogo.\r\nNiekoniecznie jednak brak â€œwolnej woliâ€ czÅ‚owieka implikuje koniecznoÅ›Ä‡ zrezygnowania z retrybucyjnej funkcji kary. Psychiatra Sally Satel i psycholog Scott O. Lilienfeld w ksiÄ…Å¼ce â€œPranie mÃ³zguâ€ argumentujÄ…, Å¼e niezaleÅ¼nie od statusu wolnej woli, ludzie majÄ… poczucie wolnoÅ›ci wyboru i dziaÅ‚ajÄ… podÅ‚ug niego, a kara ma ewolucyjnÄ… funkcjÄ™ spoÅ‚ecznÄ… (Satel & Lilienfeld, 2017). JuÅ¼ maÅ‚e dzieci odbierajÄ… zachowanie innych ludzi w kategoriach intencjonalnoÅ›ci i majÄ… silne wrodzone zachowania moralne, wÅ‚Ä…cznie z karaniem Åºle zachowujÄ…cych siÄ™ jednostek (Hamlin, 2013). Eksperymenty przeprowadzone przez ekonomistÃ³w behawioralnych pokazaÅ‚y, Å¼e wielu ludzi dyscyplinuje jednostki zachowujÄ…ce siÄ™ â€œnie-fair,â€ nawet jeÅ›li byÅ‚y tylko biernymi obserwatorami takiego zachowania i sÄ… w stanie poÅ›wiÄ™ciÄ‡ na to wÅ‚asne zasoby (Fehr & Fischbacher, 2004). Pozwala to ludziom na osiÄ…ganie celÃ³w wymagajÄ…cej wspÃ³Å‚pracy wielu osÃ³b i minimalizacji â€œoszustÃ³w.â€\r\nPonadto Satel i Lilienfeld przywoÅ‚ujÄ… eksperyment psycholoÅ¼ki i prawniczki Kenworthey Bilz, w ktÃ³rym wykazaÅ‚a, Å¼e nieukaranie sprawcy gwaÅ‚tu obniÅ¼a status spoÅ‚eczny ofiary (Bilz, 2016). SugerujÄ…, Å¼e brak moralnego zadoÅ›Ä‡uczynienia przy uÅ¼yciu proporcjonalnej kary moÅ¼e prowadziÄ‡ do zachwiania rÃ³wnowagi spoÅ‚ecznej. PrzywoÅ‚ujÄ… hipotezÄ™ â€œsprawiedliwego Å›wiataâ€ sformuÅ‚owanÄ… przez Melvina Lernera. Jest to bÅ‚Ä…d poznawczy polegajÄ…cy na tendencji do wiary, Å¼e ludzie zasÅ‚ugujÄ… na nieszczÄ™Å›cia i sukcesy, ktÃ³re ich spotykajÄ…. W eksperymencie przeprowadzonym przez Lernera badani obserwowali aktorkÄ™ wykonujÄ…cÄ… zadanie pamiÄ™ciowe, jednoczeÅ›nie otrzymujÄ…cÄ… bolesne impulsy elektrycznie (udawane, o czym badani nie wiedzieli). Gdy badani mogli zdecydowaÄ‡ o przerwaniu eksperymentu lub dowiadywali siÄ™, Å¼e aktorka otrzyma wynagrodzenie pieniÄ™Å¼ne oceniali jÄ… znacznie wyÅ¼ej, niÅ¼ w sytuacji gdy biernie przyglÄ…dali siÄ™ sytuacji bez moÅ¼liwoÅ›ci wpÅ‚ywu na niÄ… (Lerner & Miller, 1978). Lerner doszedÅ‚ do wniosku, Å¼e widok osoby cierpiÄ…cej, ktÃ³ra nie ma szans na rekompensatÄ™ skÅ‚ania ludzi do deprecjonowania ofiary. Satel i Lilienfeld sugerujÄ…, Å¼e rezygnacja ze sprawiedliwej odpÅ‚aty byÅ‚aby negatywna w skutkach zarÃ³wno dla ofiary jak i moralnoÅ›ci spoÅ‚ecznej.\r\nInni wyraÅ¼ajÄ… bardziej umiarkowany poglÄ…d, Å¼e kwestia wolnej woli jest w gruncie rzeczy nieistotna dla prawa, dostrzegajÄ… jednak, Å¼e wÅ‚Ä…czenie wiedzy o czÅ‚owieku do procesu wymierzania sprawiedliwoÅ›ci moÅ¼e poskutkowaÄ‡ bardziej empatycznym traktowaniem przestÄ™pcÃ³w (przynajmniej tych o ktÃ³rych wiemy, Å¼e majÄ… znacznie zakÅ‚Ã³cone procesy podejmowania decyzji), przy zachowaniu psychologicznej potrzeby retrybucji ofiar i spoÅ‚eczeÅ„stwa (Goodenough & Prehn, 2004). Jest to prÃ³ba pewnego kompromisu, wskazujÄ…ca na potrzebÄ™ zmian w systemie prawnym, niemniej nie polegajÄ…ca na odrzuceniu retrybucyjnej funkcji kary.\r\nEpilog\r\nChoÄ‡ odpÅ‚ata jest gÅ‚Ä™boko zakorzeniona w ludzkiej kulturze i biologii, nie oznacza to, Å¼e nie jesteÅ›my w stanie zmieniÄ‡ naszego podejÅ›cia. CzÅ‚owiek wielokrotnie wprowadzaÅ‚ zmiany spoÅ‚eczne, jak wÃ³wczas mÃ³wiono, niezgodne z jego naturÄ… czy uderzajÄ…ce w porzÄ…dek spoÅ‚eczny. JeÅ›li jako gatunek mielibyÅ›my dziaÅ‚aÄ‡ tak jak aktualnie postrzegamy rzeczywistoÅ›Ä‡, niewiele byÅ›my osiÄ…gnÄ™li. Green i Cohen podajÄ… przykÅ‚ad tego jak nasza percepcja fizycznej rzeczywistoÅ›ci rozmija siÄ™ z tym jak jest faktycznie. PrzykÅ‚adem moÅ¼e byÄ‡ czas. Czas jest wzglÄ™dny, zaleÅ¼ny od pola grawitacyjnego i prÄ™dkoÅ›ci z jakÄ… ukÅ‚ad odniesienia siÄ™ porusza. Jednak nasze codzienne doÅ›wiadczenie mÃ³wi nam, Å¼e czas jest uniwersalny i pÅ‚ynie liniowo. Ma to silne przyczyny ewolucyjne. Czy to oznacza, Å¼e jesteÅ›my na zawsze ograniczeni przez nasze liniowe postrzeganie czasu? Zdecydowanie nie. Wykorzystujemy zdobytÄ… wiedzÄ™ w tworzeniu rzeczywistoÅ›ci np. w systemach GPS. Podobnie jest w przypadku omawianego problemu. MoÅ¼emy wykorzystaÄ‡ wiedzÄ™, ktÃ³rÄ… mamy, by zmieniÄ‡ nasze zachowania spoÅ‚eczne.\r\n\r\n\r\n\r\nArendt, H. (1996). Wola (p. 28). Czytelnik.\r\n\r\n\r\nBilz, K. (2016). Testing the expressive theory of punishment. Journal of Empirical Legal Studies, 13(2), 358â€“392.\r\n\r\n\r\nDamasio, A. R. (2011). BÅ‚Ä…d kartezjusza: Emocje, rozum i ludzki mÃ³zg. Dom Wydawniczy\" Rebis\".\r\n\r\n\r\nDawkins, R. (2006). Letâ€™s all stop beating basilâ€™s car. https://www.edge.org/response-detail/11416\r\n\r\n\r\nDescartes, R. (1980). Rozprawa o metodzie (pp. 72â€“73). PaÅ„stwowy Instytut Wydawniczy.\r\n\r\n\r\nFarah, M. J. (2005). Neuroethics: The practical and the philosophical. Trends in Cognitive Sciences, 9(1), 34â€“40.\r\n\r\n\r\nFehr, E., & Fischbacher, U. (2004). Third-party punishment and social norms. Evolution and Human Behavior, 25(2), 63â€“87.\r\n\r\n\r\nFigurska, M., StaÅ„czyk, M., & Kulesza, K. (2008). Humans cannot consciously generate random numbers sequences: Polemic study. Medical Hypotheses, 70(1), 182â€“185.\r\n\r\n\r\nGazzaniga, M. S. (2013). Kto tu rzÄ…dzi-ja czy mÃ³j mÃ³zg?: Neuronauka a istnienie wolnej woli (pp. 74â€“76). Smak SÅ‚owa.\r\n\r\n\r\nGazzaniga, M. S. (2020). Instynkt Å›wiadomoÅ›ci jak z mÃ³zgu wyÅ‚ania siÄ™ umysÅ‚? (pp. 89â€“90). Smak SÅ‚owa.\r\n\r\n\r\nGoodenough, O. R., & Prehn, K. (2004). A neuroscientific approach to normative judgment in law and justice. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 359(1451), 1709â€“1726.\r\n\r\n\r\nGreene, J., & Cohen, J. (2004). For the law, neuroscience changes nothing and everything. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 359(1451), 1775â€“1785.\r\n\r\n\r\nHamlin, J. K. (2013). Moral judgment and action in preverbal infants and toddlers: Evidence for an innate moral core. Current Directions in Psychological Science, 22(3), 186â€“193.\r\n\r\n\r\nKatechizm koÅ›cioÅ‚a katolickiego. (2015). 2280, 2282.\r\n\r\n\r\nLau, H. C., Rogers, R. D., & Passingham, R. E. (2007). Manipulating the experienced onset of intention after action execution. Journal of Cognitive Neuroscience, 19(1), 81â€“90.\r\n\r\n\r\nLerner, M. J., & Miller, D. T. (1978). Just world research and the attribution process: Looking back and ahead. Psychological Bulletin, 85(5), 1030.\r\n\r\n\r\nMaroÅ„, G. (2018). ZabÃ³jstwo â€z rozkazu bogaâ€ a niepoczytalnoÅ›Ä‡ sprawcy w Å›wietle orzecznictwa sÄ…dÃ³w USA ( murder on the command of god versus a perpetratorâ€™s insanity in the light of the case law of u.s. courts). 2018.\r\n\r\n\r\nMeyer, P. (1998). ZÅ‚udzenie konieczne (pp. 187â€“188). PaÅ„stwowy Instytut Wydawniczy.\r\n\r\n\r\nSapolsky, R. M. (2006). The frontal cortex and the criminal justice system. Law and the Brain, 227.\r\n\r\n\r\nSatel, S., & Lilienfeld, S. O. (2017). Pranie mÃ³zgu. Uwodzicielska moc (bezmyÅ›lnych) neuronauk (pp. 191â€“228). CiS.\r\n\r\n\r\nSchopenhauer, A. (1991). O wolnoÅ›ci ludzkiej woli (pp. 16â€“17). bis.\r\n\r\n\r\nSchulz, M.-A., Schmalbach, B., Brugger, P., & Witt, K. (2012). Analysing humanly generated random number sequences: A pattern-based approach. PloS One, 7(7), e41531.\r\n\r\n\r\nSeife, C. (2000). Cold numbers unmake the quantum mind. Science, 287(5454), 791â€“791.\r\n\r\n\r\nSoon, C. S., He, A. H., Bode, S., & Haynes, J.-D. (2013). Predicting free choices for abstract intentions. Proceedings of the National Academy of Sciences, 110(15), 6217â€“6222.\r\n\r\n\r\nWhalen, P. J., Rauch, S. L., Etcoff, N. L., McInerney, S. C., Lee, M. B., & Jenike, M. A. (1998). Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge. Journal of Neuroscience, 18(1), 411â€“418.\r\n\r\n\r\nYoung, L., Camprodon, J. A., Hauser, M., Pascual-Leone, A., & Saxe, R. (2010). Disruption of the right temporoparietal junction with transcranial magnetic stimulation reduces the role of beliefs in moral judgments. Proceedings of the National Academy of Sciences, 107(15), 6753â€“6758.\r\n\r\n\r\nCo interesujÄ…ce, pojawia siÄ™ tu dodatkowy problem. W wielu dziedzinach nauki wykorzystujemy zaÅ‚oÅ¼enie o niezaleÅ¼noÅ›ci pomiarÃ³w, ktÃ³ry uzyskujemy zwykle dziÄ™ki np. generatorom liczb pseudolosowych czy podwÃ³jnie Å›lepej prÃ³bie. Zwykle to wystarcza by uzyskaÄ‡ rzetelne wyniki. JeÅ›li jednak chcemy zbadaÄ‡ fundamentalnÄ… wÅ‚aÅ›ciwoÅ›Ä‡ jakÄ… jest samo istnienie losowoÅ›ci, nie moÅ¼emy wykorzystaÄ‡ liczb pseudolosowych do ustawienia parametrÃ³w urzÄ…dzeÅ„, poniewaÅ¼ liczby pseudolosowe sÄ… deterministyczne. PrawdziwÄ… losowoÅ›Ä‡ uzyskuje siÄ™ podczas pomiarÃ³w efektÃ³w kwantowych, ale tu zakÅ‚adamy losowoÅ›Ä‡ zjawisk kwantowych, by udowodniÄ‡ losowoÅ›Ä‡ zjawisk kwantowych. Fizycy starajÄ… siÄ™ poradziÄ‡ sobie z tym problemem na wiele sposobÃ³w, np. mierzÄ…c fotony pochodzÄ…ce z gwiazd oddalonych o 600 lat Å›wietlnych czy proszÄ…c internautÃ³w o gÅ‚osowanie w wyborze parametrÃ³w. Zabezpiecza to przez zarzutem prostego Å‚aÅ„cucha deterministycznego, nie wyklucza jednak superdeterminizmu, to znaczy moÅ¼liwoÅ›ci, Å¼e takie, a nie inne zachowanie kwantowych czÄ…steczek podczas pomiarÃ³w wynika z mechanizmu przyczynowo-skutkowego, ktÃ³rego nie rozumiemy. WykÅ‚ad o twierdzeniu Bella autrostwa MichaÅ‚a Ecksteina - https://www.youtube.com/watch?v=eRDxsl06f30&t=194sâ†©ï¸\r\nInteresujÄ…cy przeglÄ…d zagadnieÅ„ zwiÄ…zanych z zjawiskami kwantowymi w biologii moÅ¼na znaleÅºÄ‡ w ksiÄ…Å¼ce Paula Davisa â€œDemon w maszynie.â€â†©ï¸\r\n",
    "preview": "posts/2021-09-13-o-odpowiedzialnoci-za-czyny-w-dobie-lotw-na-marsa/pexels-suzy-hazelwood-1422673.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-12-odwied-baz-wiedzy/",
    "title": "OdwiedÅº BazÄ™ Wiedzy!",
    "description": {},
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {}
      }
    ],
    "date": "2021-09-12",
    "categories": [
      "Sortownia"
    ],
    "contents": "\r\n\r\n\r\nOdwiedÅº BazÄ™ Wiedzy!\r\nOtwieramy pierwszy moduÅ‚ Sortowni Wiedzy! Jest niÄ…â€¦ Baza Wiedzy. Sprawa jest bardzo prosta â€“ zbieramy przydatne linki w jedno miejsce. Tylko tyle i aÅ¼ tyle. JeÅ›li chcecie dowiedzieÄ‡ siÄ™ od czego zaczÄ…Ä‡ poszukiwania artykuÅ‚Ã³w naukowych, jeÅ›li chcecie poznaÄ‡ podkasty popnaukowe, jeÅ›li chcecie poznaÄ‡ ciekawe kanaÅ‚y na Youtube, albo dotrzeÄ‡ do przydatnych ÅºrÃ³deÅ‚ z wybranych dziedzin â€“ zajrzyjcie i przetestujcie wyszukiwarkÄ™.\r\n\r\nWe wspÃ³Å‚czesnym Å›wiecie bardzo wiele moÅ¼na nauczyÄ‡ siÄ™ nie wychodzÄ…c z domu (i za darmo). Internet jest peÅ‚en zasobÃ³w i narzÄ™dzi do nauki, weryfikacji informacji i edukujÄ…cej rozrywki. Na niektÃ³re Å‚atwo trafiÄ‡. Inne znajduje siÄ™ po dÅ‚ugich poszukiwaniach lub przypadkiem. MenedÅ¼ery zakÅ‚adek w naszych przeglÄ…darkach wypchane sÄ… przydatnymi linkami, czÄ™sto nieopisanymi lub w zÅ‚ych podfolderach. ChÄ™ci na uporzÄ…dkowanie tego zrodziÅ‚a pomysÅ‚ Bazy Wiedzy.\r\n\r\nLinki sÄ… zorganizowane wedÅ‚ug kategorii gatunkowo-funkcjonalnych (np. podkasty, serwisy informacyjne, strony zawierajÄ…ce informacje o grantach). Jest rÃ³wnieÅ¼ specjalna kategoria â€œtematyczne/dziedzinoweâ€, ktÃ³ra rozwinie dalsze okno wyboru wg dziedzin naukowych (np. lingwistyka, neuronauki). Ta z kolei pozwoli na jeszcze dokÅ‚adniejsze zawÄ™Å¼enie poszukiwaÅ„, wg tagÃ³w (dowolne hasÅ‚a, lepiej kategoryzujÄ…ce strony).\r\nBaza Wiedzy jest i bÄ™dzie w ciÄ…gÅ‚ym rozwoju, a znajdowaÄ‡ siÄ™ na niej bÄ™dÄ… rÄ™cznie wybrane linki do istotnych stron. ZaleÅ¼y nam na stworzeniu prostego, funkcjonalnego przybornika do Å›wiata wiedzy. Kategorie zorganizowaliÅ›my tak, aby rÃ³Å¼ne osoby mogÅ‚y znaleÅºÄ‡ coÅ› przydatnego. Dla osÃ³b zaczynajÄ…cych poszukiwanie wiedzy stworzyliÅ›my kategoriÄ™ â€œGdzie zaczÄ…Ä‡â€, na ktÃ³rej znajdziecie odnoÅ›niki do baz z zasobami i wyszukiwarek. Dla osÃ³b szukajÄ…cych rozrywki i wiadomoÅ›ci naukowych mamy kategorie podkastÃ³w, kanaÅ‚Ã³w wideo i serwisÃ³w informacyjnych. Z kolei osoby juÅ¼ eksplorujÄ…ce wybrane dziedziny naukowe, mamy kategoriÄ™ â€œdziedzinyâ€, ktÃ³ra pozwala wybraÄ‡ specyficzne pole badawcze.\r\nWy teÅ¼ moÅ¼ecie doÅ‚oÅ¼yÄ‡ swoje trzy grosze do rozwoju Bazy Wiedzy! JeÅ›li znacie przydatne strony, albo jeÅ›li macie wiedzÄ™ eksperckÄ… z zakresu jakiejÅ› dziedziny â€“ piszcie do nas! Informacje w zakÅ‚adce â€œKontaktâ€.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-12-odwied-baz-wiedzy/books-1655783_1280.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-12-witajcie-w-sortowni/",
    "title": "Witajcie w Sortowni",
    "description": {},
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {}
      }
    ],
    "date": "2021-09-11",
    "categories": [
      "Sortownia"
    ],
    "contents": "\r\nOtwieramy stronÄ™! Witajcie na Sortowni Wiedzy (a moÅ¼e powinniÅ›my powiedzieÄ‡ - w Sortowni?). Ta strona ma byÄ‡ przede wszystkim swego rodzaju punktem przesiadkowym. WÄ™zÅ‚em, z ktÃ³rego dalej moÅ¼na przeskoczyÄ‡ do innych stron z wiedzÄ… naukowÄ…. My w pierwszej kolejnoÅ›ci chcemy zgromadziÄ‡ przydatne odnoÅ›niki i nakierowaÄ‡ Was na te najbardziej wartoÅ›ciowe. JuÅ¼ teraz moÅ¼ecie zajrzeÄ‡ do naszej Bazy Wiedzy. Co prawda wciÄ…Å¼ jeszcze panuje tam chaos, ktÃ³ry prÃ³bujemy opanowaÄ‡ - ale wraz z mijajÄ…cym czasem, powinno zaczÄ…Ä‡ siÄ™ tam przejaÅ›niaÄ‡. Cel i funkcja Bazy Wiedzy sÄ… niezwykle proste. WrÄ™cz obraÅºliwie proste. To po prostu baza przydatnych linkÃ³w. Tyle i aÅ¼ tyle. OdnoÅ›niki zebrane w jednym miejscu, opisane i podzielone na kategorie. Znajdziecie tam linki do stron popnaukowych (serwisy, blogi, podkasty), jak i linki do przydatnych repozytoriÃ³w i baz naukowych. Linki sÄ… naszym subiektywnym wyborem tego, co uwaÅ¼amy za przydatne i istotne. I tu otwarcie mÃ³wimy - to nie jest obiektywna selekcja, choÄ‡ staramy siÄ™ do obiektywizmu dÄ…Å¼yÄ‡.\r\nZachÄ™camy Was do podsyÅ‚ania nam nowych linkÃ³w na adres sortownia.wiedzy@gmail.com. SiÅ‚Ä… rzeczy, nie znamy siÄ™ na wiÄ™kszoÅ›ci dziedzin naukowych, wiÄ™c bÄ™dziemy niezmiernie wdziÄ™czni za pomoc w uzupeÅ‚nianiu bazy.\r\nBaza Wiedzy to jednak tylko poczÄ…tek. JuÅ¼ teraz mamy w planach kolejne moduÅ‚y strony, rÃ³wnieÅ¼ zwiÄ…zane z rozpowszechnianiem nauki. BÄ™dziemy publikowaÄ‡ teksty opiniotwÃ³rcze na blogu i poradniki (m.in. z wiedzÄ… o tym jak korzystaÄ‡ z nauki, jak szukaÄ‡ artykuÅ‚Ã³w, jak rozpoznawaÄ‡ strategie retoryczne itd.). Mamy tysiÄ…c pomysÅ‚Ã³w na nowe, przydatne narzÄ™dzia do poszukiwania wiedzy naukowej â€“ te jednak bÄ™dziemy powoli i ostroÅ¼nie rozwijaÄ‡, wraz z rozrastaniem siÄ™ strony.\r\nDla kogo w ogÃ³le to wszystko robimy? Jaki to ma sens? No wiÄ™c tak â€“ po pierwsze, tak zebrana wiedza jest po prostu przydatna, nawet dla nas samych. JuÅ¼ teraz zdarza siÄ™ nam szukaÄ‡ linkÃ³w w Bazie kiedy szukamy (tego co gdzieÅ› tam dawno temu daliÅ›my do zakÅ‚adek w wyszukiwarce, ale nikt nie pamiÄ™ta gdzie i kiedy to byÅ‚o). Baza Wiedzy, nawet w swojej podstawowej formie, pozwala doÅ›Ä‡ szybko znaleÅºÄ‡ przydatne strony zwiÄ…zane z naukÄ….\r\nPo drugie, ze strony mogÄ… korzystaÄ‡ studen_ i naukowc_, poszukujÄ…cy jakiegoÅ› punktu wyjÅ›ciowego do poszukiwaÅ„. Osoby nie zaznajomione z poszukiwaniem wiedzy naukowej znajdÄ… u nas wiele przydatnych odnoÅ›nikÃ³w do najwaÅ¼niejszych stron, od ktÃ³rych naleÅ¼y zaczÄ…Ä‡ zbieranie bibliografii. Po trzecie, osoby zupeÅ‚nie niezwiÄ…zane ze Å›wiatem nauki mogÄ… byÄ‡ zainteresowane naszym wyborem stron popnaukowych. W szczegÃ³lnoÅ›ci polecamy sekcjÄ™ podkastÃ³w. Po czwarte, bÄ™dziemy kierowaÄ‡ czÄ™Å›Ä‡ materiaÅ‚Ã³w do influencer_ i popularyzator_ nauki. Jednym z naszych celÃ³w jest zwiÄ™kszenie iloÅ›ci wiedzy naukowej w debacie publicznej. W zwiÄ…zku z tym, jeÅ›li masz platformÄ™ i wÅ‚asnÄ… publicznoÅ›Ä‡, na naszej stronie znajdziesz materiaÅ‚y, ktÃ³re pomogÄ… Ci w dalszym przekazywaniu informacji naukowych.\r\nMamy nadziejÄ™, Å¼e to co robimy bÄ™dzie przydatne. To dla nas najwaÅ¼niejsze. Chcemy zrobiÄ‡ coÅ› pozytywnego w wymiarze spoÅ‚ecznym, nawet jeÅ›li bÄ™dzie to bardzo maÅ‚a rzecz. Piszcie do nas z uwagami, pomysÅ‚ami, reakcjami na adres sortownia.wiedzy@gmail.com.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-12-witajcie-w-sortowni/hero-image.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  }
]
