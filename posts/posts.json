[
  {
    "path": "posts/2023-02-05-baseball/",
    "title": "Dlaczego średnia uderzeń graczy nie przewiduje sukcesu w pojedyńczym uderzeniu?",
    "description": "O psychologach i małych efektach",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2023-08-02",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nPrzeczytałem dwa lata temu artykuł Davida Fundera “Evaluating Effect Size in Psychological Research: Sense and Nonsense” w ramach obowiązkowych zajęć z metodologii dla doktorantów (Funder & Ozer, 2019). Artykuł skupia się na problematyce małych wielkości efektów w psychologii i ich interpreteacji. Jest cytowany w prawie 1500 innych artykułach, co świadczy o jego dużej popularności. Funder krytykuje standardy dotyczące wielkości efektów opartych na współczynniku korelacji liniowej zaproponowanych przez Jacoba Cohena i proponuje nowe kryteria, jednocześnie podając możliwe przyczyny występowania małych wielkości efektów w psychologii.\r\nChoć w artykule słusznie argumentuje, że wielkość wielkości efektów powinna być oceniana w zależności od badanego zjawiska, artykuł zawierał dużo twierdzeń, które były dla mnie niepokojące metodologicznie.\r\nNajbardziej jednak zadziwił mnie wnioskek Fundera, że mały efekt korelacji liniowej sumuje się w jakiś sposób w czasie. Co to znaczy? Początkowo sądziłem, że być może chodzi o uśrednianie wielkorotnych pomiarów u pojedyńczej osoby, by uzyskać mniej zaszumioną zmienną. Na przykład gdy mierzymy czas reakcji badanego podczas wykonywania zadania na komputerze, badany w pojedyńczej próbie może się zamyślić i zaareagować wolniej. Może też akurat patrzeć tam gdzie pojawi się bodziec i zareagować szybciej niż gdyby musiał tego bodźca szukać. Uśredniając czas reakcji pozbywamy się szumu wynikającego z takich zdarzeń. Jednak Funderowi nie o to chodziło.\r\nFunder oparł to twierdzenie na artykule z lat osiemdziesiątych psychologa Roberta Abelsona (Abelson, 1985). Abelson obliczył korelację między sukcesem w pojedynczym pojedynczym podejściu profesjonalnych bejsbolistów z Major League, z ich średnią uderzeń w sezonie. Z jego obliczeń wyszło, że średnia w sezonie wyjaśniała tylko 0.00317 wariancji sukcesu w pojedyńczym podejściu. Abelson zdziwiony stwierdził, że ponieważ niewątpliwie sukces w pojedyńczym podejściu jest silnie związany ze średnią w sezonie, to owa korelacja jest zdecydowanie za mała, a on ma doczynienia z jakimś paradoksem.\r\nFunder stwierdził, że skoro najlepsza dostępna miara zdoloności zawodnika (jego średnia w sezonie) wyjaśnia tak mało wariancji sukcesu w pojedyńczym rzucie, a niewątpliwie jego średnia jest silnie związana z sukcesem w pojedyńczym rzucie, to związek ten musi objawiać (kumulować) się w czasie. W pojedyńczym rzucie nie jest taki istotny, jak w dwudziestu rzutach. Innymi słowy wnioskowanie Fundera wyglądało mniej więcej tak: średnią w sezonie można potraktować jako miarę jakieś cechy osobowości np. neurotyczności czy ekstrawertyzmu, a sukces w pojedyńczym podejściu jako zachowanie powiązane z tą cechą. Funder twierdzi, że małe korelacje cech osobowości z danymi zachowaniami, wcale nie są małe, ponieważ ich prawdziwa siła objawia się w dłuższej perspektywie czasowej. Czyli, że w jakiś sposób się kumulują.\r\nTaki wniosek może być podbudowujący. Małe efekty w psychologii wcale nie są małe. Jednak jest to wniosek zupełnie nieporawny. Wynik Abelsona można prosto wyjaśnić statystycznie i żaden sposób nie oznacza to, że małe efekty nie są małe.\r\nŻeby zrozumieć skąd wziął się tak mały wynik Abelsona, prześledzmy jego rozumowanie. Średnie profesjonalnych graczy zawierały się między 0.2 a 0.3 więc uznał arbitralnie, że średnia średnich graczy to 0.27, a odchylenie standardowe średniej średnich to 0.25.\r\nPrzypomnijmy sobie, że wariancję zmiennej możemy rozłożyć:\r\n\\[ Var(X) = Var(E(X|Y)) + E(Var(X|Y))\\]\r\nGdzie \\(Var(E(X|Y))\\) to wariancja przewidywania, a \\(E(Var(X|Y))\\) to wariancja błędu.\r\nProcent wyjaśnionej wariancji przez zmienną \\(Y\\) możemy obliczyć:\r\n\\[eta^2 = \\frac{Var(E(X|Y))}{Var(X)}\\]\r\nZ tego wzoru wynika, że procent wariancji zostanie wyjaśnionej zależy od tego jak bardzo średnie X pod warunkiem różych wartości Y różnią się od siebie.\r\nZasymujmy dane Abelsona przy użyciu języka R. Powiedzmy, że mamy 1000 profesjonalnych graczy. Dla każdego losujemy jego prawdziwą średnią pojedyńczych sukcesów ze skróconego rozkładu normalnego (między 0.2 a 0.3), o średniej 0.27 i odchyleniu standardowym 0.25. Natępnie każdemu graczowi losujemy 520 uderzeń (tyle jest w sezonie), a prawdopodobieństwo sukcesu wyznacza jego średnia.\r\nMamy zmienne: \\(X\\) - zmienna binarna (1 - sukces w pojedyńczyn uderzeniu, 0 - brak sukcesu), \\(Y\\) średnia liczba uderzeń w sezonie, \\(Z\\) - poszczególny gracz.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(truncnorm)\r\nlibrary(knitr)\r\n\r\nmeans = rtruncnorm(1000, 0.2,0.3, 0.27, 0.25)\r\ndata = data.frame()\r\n\r\nfor( i in 1:1000) {\r\n  X = sample(c(0,1),520,TRUE, prob = c(1-means[i], means[i]))\r\n  true_mean = rep(means[i],520)\r\n  Y = rep(mean(X),520)\r\n  Subject = rep(i,520)\r\n  data = rbind(data,data.frame(X,Y,true_mean,Subject))\r\n}\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data$X,data$Y),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.078.\r\n\r\ncat( paste(\"Wariancja błędu wynosi\",\r\nround(sum(tapply(data$X, data$Y, Var)*tapply(data$X, data$Y, length)/length(data$X)),3)),\".\",sep = \"\")\r\n\r\nWariancja błędu wynosi 0.186.\r\n\r\ncat(paste(\"Wariancja przewidywania wynosi\", round(Var(data$X) \r\n- sum(tapply(data$X, data$Y, Var)*tapply(data$X, data$Y, length)/length(data$X)),3),\".\"))\r\n\r\nWariancja przewidywania wynosi 0.001 .\r\n\r\nJak widzimy korelacja jest tutaj bardzo mała. Ale co się stanie jeśli zasymulujemy dane, w których średnie graczy będą losowane z rozkładu z taką samą średnią i odchyleniem standardowym ale z ograniczeniem przedziału od 0 do 1.\r\n\r\n\r\nmeans = rtruncnorm(1000, 0,1, 0.27, 0.25)\r\ndata2 = data.frame()\r\nfor( i in 1:1000) {\r\n  \r\n  X = sample(c(0,1),520,TRUE, prob = c(1-means[i], means[i]))\r\n  true_mean = rep(means[i],520)\r\n  Y = rep(mean(X),520)\r\n  Subject = rep(i,520)\r\n  data2 = rbind(data2,data.frame(X,Y,true_mean,Subject))\r\n\r\n}\r\ncat(paste(\"Korelacja wynosi \", round(cor(data2$X,data2$Y),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.419.\r\n\r\ncat( paste(\"Wariancja błędu wynosi\",\r\nround(sum(tapply(data2$X, data2$Y, Var)*tapply(data2$X, data2$Y, length)/length(data2$X)),3)),\".\",sep = \"\")\r\n\r\nWariancja błędu wynosi 0.184.\r\n\r\ncat(paste(\"Wariancja przewidywania wynosi\", round(Var(data2$X) \r\n- sum(tapply(data2$X, data2$Y, Var)*tapply(data2$X, data2$Y, length)/length(data2$X)),3),\".\"))\r\n\r\nWariancja przewidywania wynosi 0.039 .\r\n\r\nOtrzymaliśmy dużo wyższą korelację. Zobaczmy co się tu stało. Wariancja przewidywania znacznie wzrosła, natomiast wariancja błędu zmalała. Dlaczego tak się stało skoro związek pomiędzy zmiennymi jest taki sam w obu symulacjach?\r\nOdpowiedź na to pytanie zawiera się w samym wzorze na wyjaśniną wariancję. Przyjrzyjmy się najpierw wariancji przewidywania \\(Var(E(X|Y))\\). Ponieważ \\(Y = E(X|Z)\\) to wtedy \\(Var(E(X|Y)) = Var(E(X|Z)) = Var(Y)\\).\r\nTo znaczy, że wysokość wariancji przewidywania zależy tylko od tego jak bardzo średnie graczy się różnią. Im bardziej gracze są do siebie podobni, tym mniejsza wariancja przewidywania w stosunku do wariancji błędu. Innymi słowy jeśli wszyscy gracze mają bardzo podobną średnią uderzeń, to nie będzie ona dobrze wyjaśniać różnic pomiędzy nimi.\r\nWariancja wyjaśniana (całkowita) składa się z wariancji błędu i wariancji przewidywania. Ponieważ X jest zmienną binarną to\r\n\\[Var(X) = E(X)(1-E(X))\\]\r\nDla przykładu Abelsona wariancja całkowita to \\(Var(X) = 0.27(1-0.27) = 0.1971\\).\r\nWariancja przewidywania w przykładzie Abelsona zależy od tego jak różne od siebie są średnie graczy. skoro wiemy, że średnie graczy znajdują się w przedziale <0.2, 0.3> i mają średnią 0.27, moglibyśmy się pokusić o pytanie ile maksymalnie ta zmienna mogłaby wyjaśnić wariancji pojedyńczego sukcesu?\r\nJeśli mamy zmienną w przedziale \\(<0, c>\\) to: \\[\\displaystyle \\sum_i x_i^2 = \\sum_i x_i\\cdot x_i \\leq \\sum_i c\\cdot x_i = cn\\bar{x}\\]\r\nponadto:\r\n\\[\\begin{align*}\r\nn\\cdot \\text{Var}(\\mathbf{x}) &= \\sum_i (x_i - \\bar{x})^2= \\sum_i x_i^2 - 2x_i\\bar{x} + \\bar{x}^2\\\\\r\n&= \\sum_i x_i^2 - 2\\bar{x}\\sum_i x_i + n\\bar{x}^2= \\sum_i x_i^2 - n\\bar{x}^2\\\\\r\n&\\leq cn\\bar{x} - n\\bar{x}^2 = n\\bar{x}(c-\\bar{x})\r\n\\end{align*}\\]\r\nwięc\r\n\\[\\text{Var}(\\mathbf{x}) \\leq \\bar{x}(c-\\bar{x})\\]\r\nśrednia uderzeń graczy Abelsona\r\n\\[eta^2 = \\frac{Var(Y)}{Var(X)} \\le \\frac{0.07(0.1-0.07)}{0.1971} = 0.01065\\]\r\nmoże w najlepszym wypadku wyjaśnić 1% wariancji. Widzimy ze wzoru, że zachowując wartość średniej sukcesów jako stałą, rozszerzanie przedziału, w którym znajdują się możliwe średnie graczy, będzie zwiększało procent możliwej do wyjaśnienia wariancji.\r\nWydaje mi się, że zdziwienie Abelsona wynikało z faktu, że spodziewał się, że procent wyjaśnionej wariancji w pojedyńczym sukcesie przez średnią gracza powinen być wysoki, ponieważ przewidujemy zachowanie na podstawie średniej miary tegoż zachowania, czyli jego średniej w dłuższym okresie (analgoicznie do neurotyzmu mającego przewidywać zachowania neurotyczne).\r\nJednak Abelson licząc procent wyjasnionej wariancji zadał inne pytanie, mianowicie jak dobrze możemy przewidzieć sukces gracza w pojedyńczej próbie na podostawie jego średniej. Jak zauważyliśmy, nie będzie dobrze przewidywać, ponieważ zmienna Y nie różnicuje wystarczająco dobrze pomiędzy graczami względem wielkości błędu. Można to rozumieć w katagoriach niskiej rzetelności.\r\nTo zjawisko generalizuje się na inne przykłady. Żeby sobie to zwizualizować zobaczmy średnie graczy w sezonie z symulacji pierwszej (średnia z 520 prób pojedyńczych odbić) naprzeciw prawdziwych średnich (prawdopodobieństw, których użyłem do wygenerowania danych, możemy je potraktować jako ich prawdziwą zdolność gry w bejsbol).\r\n\r\nGeneralizuje się fakt, że im bardziej różne od siebie są średnie warunkowe E(X|Y), tym więcej Y wyjaśni wariancji X.\r\n\r\n\r\ndata_plot <- data %>% group_by(Subject) %>% summarise(skill = min(true_mean), mean = min(Y))\r\n\r\n\r\nplot(data_plot$mean,data_plot$skill)\r\n\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data_plot$skill,data_plot$mean),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.815.\r\n\r\nTeraz to samo z symulacją 2.\r\n\r\n\r\ndata2_plot <- data2 %>% group_by(Subject) %>% summarise(skill = min(true_mean), mean = min(Y))\r\n\r\nplot(data2_plot$mean,data2_plot$skill)\r\n\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data2_plot$skill,data2_plot$mean),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.996.\r\n\r\nJak widzimy korelacja jest większa w drugiej symulacji, mimo, że związek pomiędzy zmiennymi jest taki sam. Mamy tu do czynienia ze zjawiskiem Restricted Range Corelation - czyli sytuacji w której zmienna w próbie ma mniejszą wariancję niż w populacji. Ponieważ obserwacje są do siebie bardziej podobne (są bliżej siebie na wykresie), wpływ błędu jest większy. Przybliżmy teraz drugi wykres tak, by widzieć wartości prawdziwej średniej z zakresu między 0.2 a 0.3.\r\n\r\n\r\ndata3_plot = data2_plot %>% filter(skill >0.2 & skill < 0.3)\r\n\r\nplot(data3_plot$mean,data3_plot$skill)\r\n\r\n\r\ncat(paste(\"Korelacja wynosi \", round(cor(data3_plot$skill,data3_plot$mean),3),\".\", sep = \"\"))\r\n\r\nKorelacja wynosi 0.835.\r\n\r\nWidzimy, że ten wykres przypomina pierwszy wykres. Zawęziliśmy zakres zmiennej, więc błąd wydaje się relatywnie większy.\r\nCo natomiast możemy powiedzieć o małych wielkościach efektu w psychologii? Mały efekt, to po prostu mały efekt. Nie kumuluje się w czasie. Ale to nie znaczy, że jest nieistony. Jeśli badamy zachowanie człowieka, byłoby wręcz dziwnie, jeśli efekt jednej zmiennej byłby duży, zwłaszcza w przypadku złożonych zachowań.\r\n\r\n\r\n\r\nAbelson, R. P. (1985). A variance explanation paradox: When a little is a lot. Psychological Bulletin, 97(1), 129.\r\n\r\n\r\nFunder, D. C., & Ozer, D. J. (2019). Evaluating effect size in psychological research: Sense and nonsense. Advances in Methods and Practices in Psychological Science, 2(2), 156–168.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-05-baseball/Statystyczne_Dygresje.jpg",
    "last_modified": "2023-05-01T14:22:09+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-18-tutorial-bayes-v/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część V: Selekcja modeli, wybór rozkładów a priori i inne przydatne rzeczy",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2023-06-18",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWstęp\r\nSelekcja modeli\r\nGarść wniosków\r\n\r\n\r\nWstęp\r\nDzisiejszy wpis będzie nie będzie w formie zamkniętej, będę dodawał do niego treści gdy czas pozwoli. Poruszymy problematykę selekcji modeli, wyboru rozkładów a priori i innych rzeczy.\r\nSelekcja modeli\r\nWyprodukujmy sobie dane powiązane kwadratowo.\r\n\r\n\r\nset.seed(123)\r\nx = rnorm(100,1,1)\r\ny = 0.5*x^2 + rnorm(100,0,1)\r\nplot(x,y)\r\n\r\n\r\n\r\nNastępnie zamodelujemy związek x i y liniowo i kwadratowo.\r\n\r\n\r\nlibrary(rjags)\r\nlibrary(coda)\r\nlibrary(MCMCvis)\r\n\r\nmod_lin = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(y)) {\r\n    y[i] ~ dnorm(a + b * x[i], sigma^-2)\r\n  }}\"\r\n\r\nmod_sq = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(y)) {\r\n    y[i] ~ dnorm(a + b * x[i]^2, sigma^-2)\r\n  }}\"\r\n\r\n\r\nparams = c(\"a\",\"b\",\"sigma\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 3000\r\nnt = 1\r\nnc = 3\r\n\r\nmodel_lin = jags.model(file = textConnection(mod_lin), data = list(x = x, y = y), n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(model_lin, n.iter=ni, by=1)\r\nsamples_lin = coda.samples(model_lin, params, n.iter = nb, thin = nt)\r\n\r\nmodel_sq = jags.model(file = textConnection(mod_sq), data = list(x = x, y = y), n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(model_sq, n.iter=ni, by=1)\r\nsamples_sq = coda.samples(model_sq, params, n.iter = nb, thin = nt)\r\n\r\n\r\nZerknijmy na przedziały wiarygodności dla współczynników regresji.\r\n\r\n\r\nsummary(samples_lin)\r\n\r\n\r\nIterations = 3101:6100\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean      SD  Naive SE Time-series SE\r\na     -0.2599 0.16924 0.0017839      0.0034188\r\nb      1.0637 0.11957 0.0012604      0.0024116\r\nsigma  1.0846 0.07813 0.0008235      0.0008773\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%     50%     75%   97.5%\r\na     -0.5962 -0.3719 -0.2589 -0.1466 0.07121\r\nb      0.8262  0.9833  1.0639  1.1423 1.29861\r\nsigma  0.9458  1.0309  1.0785  1.1331 1.24936\r\n\r\n\r\n\r\nsummary(samples_sq)\r\n\r\n\r\nIterations = 3101:6100\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean      SD  Naive SE Time-series SE\r\na     -0.02662 0.13115 0.0013825      0.0022044\r\nb      0.46027 0.04316 0.0004550      0.0007105\r\nsigma  0.98054 0.07148 0.0007535      0.0008375\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%      50%     75%  97.5%\r\na     -0.2868 -0.1154 -0.02372 0.06243 0.2257\r\nb      0.3759  0.4309  0.46096 0.48936 0.5443\r\nsigma  0.8522  0.9304  0.97654 1.02500 1.1327\r\n\r\nW obydwu przypadkach przedziały nie zawierają zera. Musimy rozsądzić, który model jest lepszy. Moglibyśmy zrobić to przy pomocy już omówionego Czynnika Bayesa. Jednak, tak jak już wspominałem, obliczenie go w bardziej skomplikowanych modelach jest problematyczne.\r\nW modelach częstościowych używa się kryteriów informacyjnych takich jak AIC i BIC do selekcji modeli. W statystyce Bayesowskiej do oceny dopoasowania modelu możemy użyć Deviance Information Cryterion (DIC).\r\n\\[\\text{DIC} = D(\\bar{\\theta}) + 2*p_D\\]\r\ngdzie:\r\n\\[ D(\\theta) =  -  2\\log ({\\rm{P}}(y|\\theta ))\\]\r\n\\[ p_D= D(\\bar{\\theta}) -  \\overline{D(\\theta)}\\]\r\nDIC jest zdefiniowane jako różnica między (ujemnym) logarytmem funkcji wiarygodności zewaluowanym dla średniej wartości parametrów \\(\\theta\\) z rozkładu post priori \\(D(\\bar{\\theta})\\) (gdzie dla symetrycznych rozkładów otrzymujemy najbardziej wiarygodne wartości) a dewiacją \\(p_D\\). Dewiacja, z kolei, jest obliczana jako różnica między wartością funkcji wiarygodności \\(D(\\bar{\\theta})\\) a średnią wartością logarytmu funkcji wiarygodności dla wszystkich parametrów.\r\nInnymi słowy, wartość \\(D(\\bar{\\theta})\\) dostarcza informacji na temat skupienia gęstości prawdopodobieństwa wokół dominującej wartości \\(\\bar{\\theta}\\). Im lepszy jest model, tym mniejsza jest ta wartość. Jednakże, gdy liczba parametrów w modelu wzrasta, wartość \\(D(\\bar{\\theta})\\) może również maleć, ponieważ bardziej skomplikowany model ma większą elastyczność w dopasowaniu się do danych. Dlatego drugi składnik, \\(\\overline{D(\\theta)}\\), mierzy stopień rozproszenia wiarygodności wokół dominującej wartości. Im więcej parametrów, tym większa wartość tego składnika.\r\nPodsumowując, modele z mniejszą wartością DIC są preferowane, ponieważ sugerują większe skupienie gęstości prawdopodobieństwa wokół dominującej wartości, jednocześnie uwzględniając liczbę parametrów w modelu.\r\nPoliczmy DIC dla naszych modeli.\r\n\r\n\r\nDic_lin = dic.samples(model_lin,1000)\r\nDic_lin\r\n\r\nMean deviance:  298.6 \r\npenalty 3.053 \r\nPenalized deviance: 301.6 \r\n\r\n\r\n\r\nDic_sq = dic.samples(model_sq,1000)\r\nDic_sq\r\n\r\nMean deviance:  278.3 \r\npenalty 3.061 \r\nPenalized deviance: 281.3 \r\n\r\n\r\n\r\ndiffdic(Dic_sq,Dic_lin)\r\n\r\nDifference: -20.32408\r\nSample standard error: 9.939601\r\n\r\nWidzimy, że model z kwadratowym związkiem pomiedzy x i y jest lepiej dopasowany do danych.\r\nPrócz DIC, w statystyce Bayesowskiej narzędziami do oceny dopasowania modeli są Widely Applicable Information Criterion (WAIC) oraz Leave-one-out cross-validation (LOO-CV). Są to metody, które nie są zaimplementowane w JAGS, jednak warto je poznać, ponieważ posiadają lepsze właściwości (Gelman et al., 2014).\r\n## Wybór rozkładów a priori\r\nTo chyba najtrudniejszy temat w statystyce Bayesowskiej. Zasadniczo zgodnie z ortodoksyjną filozofią Bayesowską rozkłady a priori powinny odzwierciedlać naszą wcześniejszą wiedzę. Filozofia ta może wydawać się kusząca, ponieważ tak właśnie powinna działać nauka. Akumulować wiedzę i następnie po otrzymaniu nowych danych aktualizować swoje oczekiwania. Jednak gdy przechodzimy do modelowania pojawia się problem.\r\nJak przedstawić naszą wiedzę w postaci rozkładów a priori? Czy jeśli użyjemy informatywnego rozkładu dla współczynnika regresji, powiedzmy \\(N(100,0.001)\\), i po otrzymaniu rozkładu a posteriori przedział wiarygodności nie będzie zawierał 0, czy właśnie nie oszukaliśmy by uzyskać “istotny” współczynnik regresji?\r\nZ kolei jeśli użyjemy zbyt informatywnego rozkładu skoncentrowanego wokół zera, możemy ograniczyć naszą zdolność do wykrycia istniejącego efektu.\r\nRozważmy następujący przypadek:\r\n\r\nWidzimy tu przykład zastosowania bardzo restrykcyjnego rozkładu a priori. Rozkład posteriori nie bardzo przypomina rozkład a priori. Z kolei gdybyśmy brali pod uwagę tylko wiarygodność (tzn, zastosowalibyśmy płaski rozkład a priori), różniłby się on znacznie od rozkładu a priori.\r\nPrzypomnijmy sobie jaką interpretacje ma rozkład a posteriori i zastosujmy ją do powyższego wykresu: Pod warunkiem naszych zaobserwowanych danych, zaobserwowane przez nas dane mają znikome prawdopodobieństwo zaobserwowania.\r\nHmm. Tak jak wspominałem ustalenie właściwych rozkładów to rzecz nietrywialna.\r\nZasadniczo, możliwe podejścia do rozkładów a priori:\r\nUżywanie nieinformaywnych priorów (np. rozkładów jednostajnych) - częste i najprostsze podejście. Zalety: Dobre do celów eksploracyjnych. Wady: To głębszy temat, jednak w wielu przypadkach użycie słabych priorów może znacząco zbiasować wyniki naszej analizy. Rozszerzę ten punkt, a na razie odsyłam do wpisu Andrewa Gelmana.\r\nEmpiryczny Bayes. Możemy użyć danych by wyestymować rozkłady a prior. Przykład możecie znaleźć w pierwszej części tutorialu (przykład z księgarnią). Jest to użyteczne narzędzie, jednak niekoniecznie zawsze najlepsze do wnioskowania statystycznego. To znaczy, należy rozważyć na ile możemy sobie pozwolić na “zajrzenie” do danych, zanim użyjemy docelowego modelu.\r\nWybieranie informatywnych rozkładów a priori . W jaki sposób? Zwykle posiadamy jakieś informacje na temat badanego zjawiska. Jakiej wielkości efektu możemy się spodziewać? Jeśli współczynnik regresji 0.1 to maksymalna rozsądna wartość na jaką możemy liczyć, ponieważ wiemy, że inne zbadane w literaturze efekty znacznie ważniejszych predykatorów osiągają maksymalnie taki efekt, to \\(N(0,1)\\) nie będzie najlepszym priorem, ponieważ ponad 90% gęstości prawdopodobieństwa znajduje się poza przedziałem [-0.1,0.1]. Słaby prior mógłby na przykład sprawić, że wykrylibyśmy efekt “istotny, ale trywialny”, jak na przykład korelację między atrakcyjnością rodziców a prawdopodobieństwem urodzenia dziewczynki. Ten pozorny efekt nie został by uznany za istotny, gdyby użyto informatywnego rozkładu a priori, który uwzględniał wielkości efektów dla zmiennych takich jak np. wiek matki (Gelman et al., 2017).\r\nOgólne wskazówki dotyczące wybierania rozkładów a priori (niezależne od wyżej wymienionych strategii):\r\nRozkład a priori powinien kształtem przypominać rozkład modelowanego parametru. Jeśli parametr jest średnią - to zwykle zamodelujemy go rozkładem normalnym. Co w przypadku innych parametrów? Warto zobaczyć jak takie parametry się zachowują. Policzmy sobie wariancję z 300^2 prób, w których losujemy 25 obserwacji z rozkładu normalnego.\r\n\r\n\r\nvariances = replicate(300^2,var(rnorm(25,0,1)))\r\nplot(density(variances))\r\n\r\n\r\n\r\nJak widzimy, rozkład wariancji jest prawoskośny. Rozkładem prawdopodobieństwa, którym często modeluje się wariancję jest rozkład Gamma.\r\n\r\n\r\ncurve(dgamma(x,7,7),0,3)\r\n\r\n\r\n\r\nGarść wniosków\r\nStatystyka Bayesowska ma swoje wady i zalety. W tutorialu skupiliśmy się głównie na metodzie numerycznej, jaką jest MCMC, do modelowania. Ta metoda umożliwia także estymację modeli, które nie mają rozwiązań analitycznych (albo ich nie znamy). Moglibyśmy estymować parametry takich modeli za pomocą zwykłej optymalizacji. Jednakże, wtedy pojawia się problem, ponieważ nie możemy obliczyć przedziałów ufności. Mnie interesuje mnie głównie wnioskowanie statystyczne, a podejście Bayesowskie dostarcza nie punktową wartość parametru tylko jego rozkład, co pozwala ocenić niepewność związana z tymi parametrami!\r\n\r\n\r\n\r\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for bayesian models. Statistics and Computing, 24(6), 997–1016.\r\n\r\n\r\nGelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19(10), 555.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-18-tutorial-bayes-v/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-06-24T01:09:21+02:00",
    "input_file": "tutorial-bayes-v.knit.md"
  },
  {
    "path": "posts/2023-04-22-radiosortownia/",
    "title": "Radio Sortownia",
    "description": "Czego słuchać podczas czytania tekstów naukowych?",
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2023-04-25",
    "categories": [],
    "contents": "\r\nPost może być aktualizowany po publikacji – jeśli pojawią się nowe ciekawe źródła, dodamy je.\r\n\r\nCzego słuchać podczas czytania artykułów lub podczas nauki? Każdy może słuchać czego chce i tego co lubi. Temat zamknięty!\r\nAle nie kończmy jeszcze. Wszyscy mamy jakieś preferencje co do muzyki (lub jej braku) do pracy i nauki. To może też zmieniać się w czasie – o jednej porze dnia cisza i zatyczki, kiedy indziej jak najgłośniejszy pop, edm i top100 miesiąca. Jazz-metal, Motown, disco-polo, trap, french touch – każdy gatunek jest dobry, jeśli działa. Różne są też potrzeby. Możemy chcieć wprowadzić się w stan flow, kiedy jesteśmy hiper-kreatywni, łatwo łączymy wiele elementów i ogarniamy cały kontekst naraz. Albo, możemy dążyć do uzyskania głębokiego spokoju i wysokiej koncentracji (warto połączyć z 5-10 minutami głębokiego oddychania/medytacji).\r\nW kontekście skutecznej muzyki do nauki często wymienia się ambient, lo-fi, techno, muzykę klasyczną, ścieżki dźwiękowe do gier i filmów. A więc muzyka albo spokojna, albo rytmiczna i pozbawiona słów.\r\nRóżne playlisty znajdziecie na dowolnej platformie streamingowej. W Radiu Sortownia podejdziemy jednak do sprawy inaczej. Pokażemy mniej i bardziej znane alternatywne projekty soniczne. Wspólnotowo-spółdzielcze rozgłośnie, ambientowe kolaże, nieskończone rozmowy filozoficzne. Dźwięki rozpięte w czasie jak żagle, rozgłośnie z Madagaskaru i sposoby na sen.\r\nRadio Sortownia zaprasza na następujące doświadczenia:\r\nSomaFMhttps://somafm.com/\r\nSomaFM to niezależne radio (bez reklam!), obejmujące wiele stacji tematycznych z muzyką niezależną. Ambient, downtempo, vaporwave, IDM, jazz, doom metal, folk etc. Są również kanały z muzyką dla szpiegów i z muzyką dla hakerów. Space Station Soma sprawdza się w codziennej pracy astronautów, a także o szóstej nad ranem, kiedy trzeba oddać esej z etnografii na ostatni termin.\r\nRadia wspólnotowehttps://monoskop.org/Community_radio\r\nPod powyższym linkiem znajduje się lista niezależnych rozgłośni z całego świata. Pomiędzy nimi jest polskie Radio Kapitał, które poza graniem muzyki jest też platformą dyskusji nt. kultury, sztuki i społeczeństwa. Dużą zaletą takich rozgłośni jest fakt, że muzykę dobierają w nich DJe i DJki z głęboką wiedzą o kontekście muzyki i subkulturach, scenach – to coś, czego nie dostarczą algorytmy Spotify.\r\nRadio Gardenhttp://radio.garden/\r\nRadio Garden to aplikacja przeglądarkowa i na telefony, pozwalająca latać po kuli ziemskiej, w poszukiwaniu odległych stacji radiowych. Można sprawdzić co grają w Boliwii, Tajlandii, Nashville, Kirgistanie. Można posłuchać dziwnych reklam z RPA, gadanego radia z Japonii, romantycznych indonezyjskich ballad. Radio Garden przyda się też w nauce języków, np. możemy wybrać się w długą podróż po Francji i Hiszpanii, przy okazji osłuchując się z lokalną wymową. Jeśli przytłoczą nas (zdecydowanie najczęstsze) stacje popowe, możemy zajrzeć na playlisty z wybranymi stacjami radiowymi. Na Independent Sounds znajdziemy m.in. legendarne NTS, na Rare Tongues posłuchamy rzadkich i zagrożonych języków.\r\nRadiooooohttps://radiooooo.com/\r\nTa aplikacja pozwala trenować nie tylko przesunięcia geograficzne, ale i temporalne. Możemy wybrać kraj oraz… dekadę. Możemy wyobrazić sobie jakby to było żyć w Polsce w latach 70., albo w Iranie lat 60. Czego słuchano w Chinach po II Wojnie Światowej? Czy w Egipcie ludzie tańczyli do disco w latach 80.? Czym żyła australijska scena muzyczna w 1913 roku?\r\nYou are listening to, Lofi Air Traffic Controlhttp://youarelistening.to/https://www.lofiatc.com\r\nYou are listening to (_____) jest kolażem dźwiękowym, łączącym ambient z miastami. Strona nakłada na siebie muzykę z publicznie dostępnymi transmisjami nadawanymi przez policję, straże pożarne i lotniska. Lofi Air Traffic Control jest podobnym pomysłem, tylko za ambient wskakuje lofi.\r\nRadia akademickiehttp://radiocentrum.pl/redakcja/polskie-rozglosnie-akademickie/ https://pl.wikipedia.org/wiki/Kategoria:Studenckie_rozg%C5%82o%C5%9Bnie_radiowe_w_Polsce\r\nWarto przypomnieć też o istnieniu rozgłośni akademickich. Pojawiają się w nich autorskie audycje z selekcją muzyki, ale też dyskusje i wywiady. Społecznościowy i lokalny charakter tych rozgłośni nie kłóci się z tym, że mają one profesjonalny poziom produkcji.\r\nWorld Concert Hall https://www.worldconcerthall.com/\r\nNie zapominając o muzyce klasycznej, połączmy się z salami koncertowymi na całym świecie, dzięki World Concert Hall. Tu znajdziemy linki do streamów audio na żywo m.in. z Madrytu, Wiednia i Katowic. Jeśli to za mało, warto zajrzeć na https://www.openculture.com/ – w panelu po prawej jest kategoria Free music, gdzie można znaleźć więcej linków do projektów związanych z muzyką klasyczną.\r\nSleep with me Podcasthttp://www.sleepwithmepodcast.com/\r\nJeśli potrzebujemy zasnąć lub na naszą koncentrację pozytywnie wpływa gadanie w tle, można spróbować Sleep with me. Są to wielogodzinne gawędziarskie przypływy i odpływy, potoki bezsensu i niesamowitych historii. Coś podobnego zrobił też kiedyś Jeff Bridges: http://www.dreamingwithjeff.com/#music-section\r\nInfinite conversationhttps://infiniteconversation.com/\r\nWchodząc w sferę dźwięków generowanych, możemy natrafić na nieskończoną dyskusję Slavoja Žižka z Wernerem Herzogiem. Modele neuronowe słoweńskiego filozofa i apokaliptycznego reżysera gadają ze sobą o życiu, śmierci i pieszych wędrówkach. Jest to równie głębokie, co płytkie. Śmieszne i niepokojące. “Yes, we will simply die. It’s an empty gesture, but maybe this is the only way to live in peace.”\r\nMykoLycohttps://www.youtube.com/@MycoLyco\r\nZastanawialiście się kiedyś – co się stanie, jeśli podłączy się grzyby do syntezatorów? Na tym kanale znajdziecie odpowiedź. Różne rodzaje grzybów!\r\nListen to Wikipediahttp://listen.hatnote.com/\r\nBells are additions, strings are subtractions. Na tej stronie możemy posłuchać na żywo zmian dokonywanych na Wikipedii. Każda zmiana ma tu reprezentację dźwiękową oraz wizualną. Uspokajają nie tylko dźwięki, ale i świadomość, że największa encyklopedia świata jest dynamicznym, wiecznie rozwijającym się organizmem społecznym.\r\nApollo in Real-time https://apolloinrealtime.org/\r\nTu z kolei można posłuchać trzech misji księżycowych – w takim czasie, w jakim one naprawdę trwały, tzn. w całości. Są to transmisje rozmów między astronautami a kontrolą lotów, ale – są one zsynchronizowane z wideo i wykonywanymi przez misję zdjęciami!\r\n﹏﹏﹏﹏﹏﹏﹏﹏﹏﹏\r\nZnacie inne podobne projekty? Napiszcie! Dodamy do listy.\r\n❖❖❖\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-22-radiosortownia/boom.png",
    "last_modified": "2023-05-01T14:22:21+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-06-narzeczepub/",
    "title": "Na rzecz formatu epub w polskiej nauce",
    "description": "Dlaczego przytłaczająca większość polskich artykułów naukowych jest dostępna wyłącznie w formacie PDF?",
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2023-03-06",
    "categories": [
      "Mikromanifesty"
    ],
    "contents": "\r\nOd PDF do EPUB, ikonki via pngegg.comZacznijmy od różnic, a potem dopiero zastanówmy się nad ich znaczeniem.\r\nPDF: - pozwala na stworzenie struktury i formatowania, które zawsze będą takie same, - nie pozwala na powiększanie i zmianę wyświetlanego fontu (czcionki), odstępów itd., - obrazki i tabele będą zawsze w tym miejscu, w którym zaplanowała to osoba autorska, - plik może być skomplikowany graficznie (np. tekst opływający obraz, pola z kolorowym tłem), - drukowanie jest proste, dokument po wydrukowaniu będzie wyglądał tak jak plik.\r\nEPUB: - wygląd dokumentu zależy od preferencji użytkowników, a także od urządzenia wyświetlającego plik, rozmiar strony jest tylko umowny, - dostosowywać można m.in. wielkość tekstu, interlinię, marginesy, krój pisma, - obrazki i tabele mogą się rozjeżdżać, jeśli nie zostaną dobrze sformatowane, - plik może zawierać urozmaicenia graficzne, ale te będą zależeć od zakodowania ich w XML, HTML/CSS, więc podlegają innej logice konstruowania, - drukowanie również jest proste, ale plik po wydruku nie będzie tak schludnie sformatowany, jak to byłoby w przypadku pliku PDF.\r\nPrzytłaczająca większość polskiej nauki została zaklęta w format PDF. Jest to zrozumiałe, format PDF pozwala na mimikrę drukowanych artykułów, czyli będą one wyglądać tak samo w drukowanym zbiorze artykułów, jak i na ekranie. Podkreślam frazę tak samo, ponieważ przeczuwam, że jest ona rozumiana jako fraza – to jest to samo. Skoro coś wygląda identycznie, to można uznać, że jest to ta sama rzecz. Identyczność nie opiera się na identyczności tekstu, tylko na graficznych pozorach podobieństwa. Artykuły naukowe cechuje wysoki stopień oficjalności, dotyczy to również formatowania. Poszczególne pisma mają swoje wytyczne co do formatowania, może to być nawet ich cecha rozpoznawcza. Publikacja cyfrowa jest więc domyślnie poddawana reżimowi formatowania tekstu. Spekuluję, że ten sam tekst w pliku DOCX lub TXT będzie uznany za mniej poważny, niż tekst opublikowany w formacie PDF. To kwestia legitymizacji, stałe formatowanie dodaje tekstowi naukowości, powagi. Czy nie jest to jednak jedynie iluzja? Jeśli tekst jest dobry i badania są dobre, ich wartość się nie zwiększy przez dodanie oficjalnych nagłówków, stopek i kolumn. Czy zmiana formatowania byłaby transgresją, przeciw instytucjonalnej oficjalności? Czy artykuł straciłby na wartości akademickiej?\r\nProblem\r\nGłówna zaleta formatu PDF (niemodyfikowalność) jest jednocześnie największą wadą, w kontekście pewnego, dość istotnego procesu – czytania. Jeśli narzucone formatowanie jest niewygodne do czytania, musimy to cierpliwie znieść. Jeśli używamy urządzenia, które niezbyt dobrze wyświetla pliki PDF – musimy to znieść. Jeśli układ dwu-, trzykolumnowy jest dla nas trudny do czytania – musimy to znieść. Jeśli edytorzy wybrali nieczytelną czcionkę i/lub skandalicznie małe odstępy pomiędzy wierszami – musimy to znieść.\r\nFormat tekstu nie powinien nie może być dodatkowym utrudnieniem, przy już i tak trudnej do zrozumienia treści\r\nTo może być irytujące dla osób sprawnych fizycznie i kognitywnie. Jednak dla osób niedowidzących, czy dla dyslektyków – to coś więcej niż irytacja. To już kwestia dostępności. Brak możliwości zwiększenia czcionki (lub zmiany czcionki na specjalnie dostosowaną do potrzeb) może dosłownie być barierą, utrudniającą lub uniemożliwiającą dostęp do nauki.\r\nRozwiązanie\r\nProblemu tak naprawdę nie ma, jeśli uznamy, że oficjalne formatowanie nie jest cechą konieczną dla wartości artykułu. Wystarczy opublikować dwa pliki: PDF i EPUB.\r\nWystarczy opublikować dwa pliki: PDF i EPUB.\r\nEPUB-y też mogą być ładnie sformatowane. Oferują pod tym względem nawet ciekawsze rozwiązania (albo po prostu inne), ponieważ dobrze sformatowany plik EPUB powinien mieć jasno podzielone sekcje z hiperlinkami, odsyłającymi do spisu treści, oraz dobrze zorganizowane hiperlinki do przypisów. Format ten pozwala na dużo lepsze skupienie na treści tekstu, a nie na jego wyglądzie – który w tym przypadku jest względny.\r\nPrzykład wzorowej dystrybucji plików, czyli wiele formatów do wyboru na stronie Wolnych Lektur\r\nPliki EPUB pozwalają na wygodne tworzenie zakładek, zaznaczanie tekstu i dodawanie komentarzy. Pliki EPUB można też zabezpieczać znakami wodnymi lub innym rodzajem zabezpieczeń (DRM). Fakt, umieszczenie skomplikowanych tabel i grafik w epubie może dać rozczarowujący efekt (choć również zależny od inteligentnego sformatowania pliku). W tym jednak rzecz, format pdf wciąż zostaje pod ręką. Konieczne jest danie ludziom wyboru. Tworzenie EPUB-ów nie wymaga ponoszenia kosztów – jest darmowe oprogramowanie, są konwertery. Najwięcej wysiłku będzie wymagać stworzenie pierwszego “pipeline’u”, potoku przetwarzania: od czystego manuskryptu, do sformatowanego EPUB-a. Każdy następny plik będzie jedynie formalnością. Zresztą, powracając do poruszonej na początku kwestii legitymizacji – zindywidualizowane formatowanie EPUB pod pismo naukowe może stać się nowym znakiem rozpoznawczym pisma. Warto spróbować!\r\nRepozytorium Biblioteka Nauki pozwala pobrać artykuły w wielu formatach\r\nA czemu nie publikacje w formacie HTML, lub PDF-y z dodatkowymi funkcjami dostępności? EPUB-y są pod każdym względem prostszym, bardziej poręcznym i powszechnym rozwiązaniem. Pojawią się przypadki, w których każdy z tych formatów będzie najlepszy – ale to tylko przesłanka do tego, by publikować treści w różnych formatach. Dodatkowo warto zapoznać się z wyspecjalizowanymi formatami (BRF, DAISY, NIMAS).\r\nPodsumowując – dominacja formatu PDF w nauce wynika prawdopodobnie w znacznym stopniu z przyzwyczajenia i instytucjonalnej inercji. Nie jest to zły format – sprawdza się dobrze w tym, czym jest, czyli w przechowywaniu dokumentów, bez możliwości dostosowywania do czytelników. Nie uwzględnia jednak tego, że czytelnicy są różni i mają różne potrzeby. I w tym miejscu doskonałym uzupełnieniem staje się EPUB – uniwersalny format do publikowania tekstu. EPUB pozwala czytelnikom m.in. na dostosowanie wielkości liter, odstępów i marginesów, dzięki czemu czytanie jest bardziej komfortowe, lub – w ogóle możliwe. Pisma, które zdecydują się dystrybuować swoje artykuły w wielu formatach wykażą się, oczywiście, myślą o potrzebach czytelników – ale, może przede wszystkim, szacunkiem dla swoich czytelników.\r\n\r\n⬥ ⬥ ⬥\r\n\r\n\r\nI to jest koniec tekstu. To nie jest poradnik tworzenia epubów, tylko mikromanifest.  Więcej informacji można znaleźć poniżej, załączam przydatne linki, zgrupowane w kategorie:\r\nOprogramowanie do tworzenia EPUB-ów: 💿 Sigil - polecany i darmowy program:https://sigil-ebook.com/sigil/  💿 Lista kilkunastu różnych programów:https://medevel.com/17-open-source-epub-and-ebook-creators/ \r\nOprogramowanie do czytania EPUB-ów: 📀 Calibre - darmowy, wielofunkcyjny program:https://calibre-ebook.com/download  📀 Sumatra - darmowy, również wielofunkcyjny program:https://www.sumatrapdfreader.org/free-pdf-reader  📀 Thorium - kolejna alternatywa:https://www.edrlab.org/software/thorium-reader/ 📀 Okular - i jeszcze jedna alternatywa:https://okular.kde.org/pl/ \r\nLaTeX do EPUB:https://minireference.com/blog/generating-epub-from-latex/https://pandoc.org/epub.html https://www.ctan.org/pkg/tex4ebook?lang=en\r\nKsiążki:\r\n📚 Coolidge, A., Doner, S., Robertson, T., & Gray, J. (2018). Accessibility Toolkit—2nd Edition. BCcampus. https://opentextbc.ca/accessibilitytoolkit/ ➡ ta publikacja jest darmowa! 📚 Garrish, M. (2012). Accessible EPUB 3. O’Reilly Media, Inc. ➡ ta publikacja jest darmowa! (https://helion.pl/ksiazki/accessible-epub-3-matt-garrish,e_2g0p.htm#format/e)📚 Garrish, M., & Gylling, M. (2013). EPUB 3 Best Practices. O’Reilly Media, Inc.📚 Paszkiewicz, D., & Dębski, J. (2013). Dostępność serwisów internetowych: Dobre praktyki w projektowaniu serwisów internetowych dostępnych dla osób z różnymi rodzajami niepełnosprawności. Stowarzyszenie Przyjaciół Integracji. https://depot.ceon.pl/handle/123456789/5609\r\nMateriały w języku polskim:\r\n💬 Wytyczne dla dostępności informacji technologie informacyjno-komunikacyjne (TIK) w zapewnianiu dostępności informacji w procesie uczenia się (ICT4IAL):https://www.ict4ial.eu/guidelines-accessible-information \r\n💬 Fundacja Instytut Rozwoju Regionalnego - E-podręcznik dostępny dla wszystkich. Poradnik dla twórców elektronicznych materiałów edukacyjnych:www.power.gov.pl%2Fmedia%2F13591%2Fe_podrecznik_dostepny_dla_wszystkich.pdf \r\n💬 Dominik Paszkiewicz, Jakub Dębski - Podręcznik Dostępność serwisów internetowych. Dobre praktyki w projektowaniu serwisów internetowych dostępnych dla osób z różnymi rodzajami niepełnosprawności:http://www.niepelnosprawni.pl/ledge/x/249472 \r\n💬 Polska Akademia Dostępności:https://pad.widzialni.org/start \r\n💬 Kategoria: Równość i dostępność w nauce w Sortowni Wiedzy:https://szpm.shinyapps.io/baza_wiedzy/\r\n💬 eTechnologie - Standardy dostępności w e-learningu (prezentacja):https://etechnologie.pl/standardy-dostepnosci-w-e-learningu/ \r\n💬 Dostępność serwisów internetowych (różne materiały):https://niepelnosprawni.gov.pl/a,60,dostepnosc-serwisow-internetowych\r\n💬 Zespół Promocji ORE, Standardy przygotowywania i publikowania treści oraz projektowania serwisów internetowych zgodnie z wytycznymi WCAG 2.0 na poziomie AA:https://www.ore.edu.pl/wp-content/uploads/2020/09/standardy-przygotowywania-tresci-zgodnie-z-wytycznymi-wcag-2.1_ore_2020.pdf + inne materiały: https://www.ore.edu.pl/materialy-do-pobrania/ \r\n💬 BON UW: EPUB – ważny standard książek elektronicznych (i inne teksty tematyczne):https://bon.uw.edu.pl/epub-%C2%AC-wazny-standard-ksiazek-elektronicznych/ \r\n💬 Beata Rędziak/Niepełnosprawni.pl - Książka (nie) dla wszystkich:http://www.niepelnosprawni.pl/ledge/x/119055\r\n💬 Mateusz Różański/Niepełnosprawni.pl - Nie widzę, nie słyszę – oglądam:http://www.niepelnosprawni.pl/ledge/x/1329924 \r\nMateriały w języku angielskim:\r\n💬 Top Tips for Creating Accessible EPUB 3 Files:http://diagramcenter.org/54-9-tips-for-creating-accessible-epub-3-files.htmlhttps://guides.cuny.edu/accessibility/epub\r\n💬 Web Accessibility Evaluation Tools List:https://www.w3.org/WAI/ER/tools/  💬 EPUB Accessibility 1.1. Conformance and Discoverability Requirements for EPUB publications:https://www.w3.org/TR/epub-a11y-11/ \r\n💬 Accessible Formats, Accessible Instructional Materials Center of Virginia (informacje m.in. o formatach BRF, DAISY, NIMAS):https://www.readingrockets.org/article/accessible-formats\r\n💬 Understanding the Accessibility Differences in EPUB and PDF Reading Experiences (prezentacja do pobrania):https://accessinghigherground.org/understanding-the-accessibility-differences-in-epub-and-pdf-reading-experiences/\r\n💬 EPUB Adoption in Academic Libraries–Progress and Obstacles:https://inclusivepublishing.org/blog/epub-adoption-in-academic-libraries-progress-and-obstacles/\r\n💬 Making digital journals and books more accessible with EPUB:https://insights.taylorandfrancis.com/social-justice/epub\r\n💬 Use Calibre to improve ePub accessibility:https://kb.iu.edu/d/bgeh\r\n💬 Accessible PDFs:https://www.adobe.com/accessibility/pdf/pdf-accessibility-overview.html https://helpx.adobe.com/pl/indesign/using/creating-accessible-pdfs.html \r\nAlternatywa online dla formatu EPUB w nauce, czyli PubReaderhttps://www.ncbi.nlm.nih.gov/pmc/about/pubreader/  (tutaj zarzut: jeśli strona nie pozwala ściągać plików via PubReader, to będziemy musieli zaufać, że serwis będzie zawsze działać – niestety strony naukowe bardzo często wypadają z obiegu, ponieważ dana instytucja przestała utrzymywać serwer lub zmieniły się adresy odnośników. Serwisy online-only powinny być traktowane z ograniczonym zaufaniem):\r\nRóżne deklaracje dostępności:\r\nhttps://doc.bibliotekanauki.pl/pl/accessibility/\r\nhttps://www.power.gov.pl/deklaracja-dostepnosci/\r\nhttps://pad.widzialni.org/oswiadczenie-o-dostepnosci \r\nhttps://wcag.uj.edu.pl/lista/-/journal_content/56_INSTANCE_DuU7E3djxFwe/145608681/148186438\r\nhttps://www.uw.edu.pl/deklaracja-dostepnosci/ \r\nhttps://www.osw.waw.pl/pl/deklaracja-dostepnosci\r\nhttps://www.dwutygodnik.com/dostepnosc.html\r\nhttps://www.ebsco.com/pl-pl/technologia/dostepnosc \r\nhttps://support.proquest.com/s/article/Accessibility-Statement-Ebook-Central?language=en_US\r\nhttps://help.taylorfrancis.com/students_researchers/s/article/Accessibility-Statement\r\nArtykuły: 📄 Arellano, J. (2021). Not open for all: Accessibility of open textbooks (Nr 1). 34(1), Art. 1. https://doi.org/10.1629/uksg.557\r\n📄 Bowes III, F. (2018). An overview of content accessibility issues experienced by educational publishers. Learned Publishing, 31(1), 35–38. https://doi.org/10.1002/leap.1145\r\n📄 Eikebrokk, T., Dahl, T. A., Kessel, S., Oslo, & Hanssen, A. U. C. of A. S. with thanks to: E. (2014). EPUB as Publication Format in Open Access Journals: Tools and Workflow. The Code4Lib Journal, 24. https://journal.code4lib.org/articles/9462\r\n— krótkie omówienie artykułu po polsku: https://babin.bn.org.pl/?p=2812\r\n📄 Suzuki, M., & Yamaguchi, K. (2020). On Automatic Conversion from E-born PDF into Accessible EPUB3 and Audio-Embedded HTML5. W K. Miesenberger, R. Manduchi, M. Covarrubias Rodriguez, & P. Peňáz (Red.), Computers Helping People with Special Needs (s. 410–416). Springer International Publishing. https://doi.org/10.1007/978-3-030-58796-3_48 \r\n📄 Wang, L. L., Cachola, I., Bragg, J., Cheng, E. (Yu-Y., Haupt, C. H., Latzke, M., Kuehl, B., Zuylen, M. van, Wagner, L. M., & Weld, D. S. (2021). Improving the Accessibility of Scientific Documents: Current State, User Needs, and a System Solution to Enhance Scientific PDF Accessibility for Blind and Low Vision Users. ArXiv. https://www.semanticscholar.org/reader/56fbb4152b4535230827eb2bd23c618045855c26\r\n📄 Zhang, X., Tlili, A., Nascimbeni, F., Burgos, D., Huang, R., Chang, T.-W., Jemni, M., & Khribi, M. K. (2020). Accessibility within open educational resources and practices for disabled learners: A systematic literature review. Smart Learning Environments, 7. https://doi.org/10.1186/s40561-019-0113-2\r\n\r\n❖❖❖\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-06-narzeczepub/pe.png",
    "last_modified": "2023-03-23T21:23:31+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-12-bayes-factor/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część III: Czynnik Bayesowski",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2023-01-13",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWnioskowanie Statystyczne\r\nPorównywanie modeli\r\nCzynnik Bayesowski\r\nCzynnik Bayesowski a złożoność modelu\r\nBayesian Point Null Hypothesis Testing\r\nSavage-Dickey density ratio\r\nLikelihood sampling\r\n\r\nPorównywanie modeli?\r\nCzynnik Bayesa nie mierzy czy model jest prawdziwy\r\nCzynnik Bayesowski jest wrażliwy na rozkłady a priori parametrów modelu\r\nRozkłady posteriori parametrów nie muszą się zgadzać z czynnikiem Bayesa\r\nCzynnik Bayesa a prawdopodobieństwo braku efektu\r\nI co z tym wszystkim zrobić?\r\n\r\n\r\nZakończenie\r\n\r\n\r\n\r\n\r\nOsobiście nienawidzę czynników Bayesa…\r\n\r\n\r\n— Andrew Gelman, 2017\r\n\r\nWnioskowanie Statystyczne\r\nJak pamiętamy, wnioskowanie w statystyce częstościowej opiera się na testowaniu hipotezy zerowej. Hipotezą zerową zwykle jest model, który świadczy o braku efektu, na przykład, że różnica średnich wynosi lub współczynnik regresji wynosi 0. Im mniejsza p-value, tym bardziej otrzymane dane nie pasują do modelu zerowego. Jeśli jednak p-value jest większa niż 0.05 nie oznacza to, że model zerowy jest bardziej prawdopodobny niż inne. Innymi słowy, ten rodzaj testowania może nam jedynie powiedzieć o występowaniu różnicy średnich czy występowaniu korelacji, ale nie o jej braku.\r\nPrzypomnijmy sobie, że wielkość przedziału ufności dla testu zależy od wariancji i wielkości próby. Gdyby NHST pozwalało nam orzekać o równości średnich, udowodnienie takiej hipotezy byłoby niezwykle łatwe.\r\nNa przykład załóżmy, że chcemy sprawdzić czy nowy lek nie różni się skutecznością od leku będącego już na rynku. Wystarczyłoby manipulować wielkością próby, aby otrzymać wniosek o braku istotnej różnicy pomiędzy wpływem tych dwóch leków!\r\nDlatego chcielibyśmy móc orzec, który z modeli (hipotez) jest bardziej prawdopodobny. Podejście Bayesowskie daje nam taką możliwość (choć, nie do końca).\r\nPorównywanie modeli\r\nCzym jest model? Najprościej mówiąc model jest zbiorem ograniczeń, który nakładamy na proces generujący dane. Na przykład może interesować nas porównanie dwóch różnych modeli regresji z innymi zbiorami predyktorów. Przykładowy model \\(M_i\\) posiada parametry \\(\\theta\\) i funkcję wiarygodności \\(P(D \\mid \\theta, M)\\). Załóżmy, że chcielibyśmy obliczyć prawdopodobieństwo a posteriori modelu \\(P(M \\mid D)\\).\r\n\\[P(M_i \\mid D) = \\frac{ P(D \\mid M_i) \\ P(M_i) }{P(D) = \\int P(D \\mid M) \\ P(M) \\  \\text{d} M}\\,\\]\r\nWidzimy, że analogicznie jak w poprzednich częściach tutorialu za pomocą twierdzenia Bayesa, interesuje nas obliczenie prawdopodobieństwa modelu \\(M_i\\) pod warunkiem otrzymanych przez nas danych.\r\nPrawdopodobieństwo \\(P(M_1)\\) jest oczywiście prawdopodobieństwem a priori naszego modelu. Sami je ustalamy, więc nie ma z nim problemu.\r\nNasuwa się jednak pytanie jak obliczyć funkcję wiarygodności? \\(P(D|M_1)\\) oznacza marginalne prawdopodobieństwo (marginal likelihood) otrzymania danych pod warunkiem modelu dla wszystkich możliwych wartości parametrów:\r\n\\[P(D \\mid M_i) = \\int P(D \\mid \\theta, M_i) \\ P(\\theta \\mid M_i) \\ \\text{d}\\theta\\]\r\nCo to znaczy dla wszystkich możliwych parametrów? Załóżmy, że mamy dwa modele regresji liniowej, różniące się tym, że jeden ma dodatkowy predyktor. Tak jak pisałem wcześniej w statystyce Bayesowskiej nie otrzymamy punktowych estymat parametrów \\(\\theta\\), ale ich rozkłady \\(P(\\theta|y)\\). Ponieważ jednak chcemy otrzymać jedną wartość - prawdopodobieństwo otrzymania danych wygenerowanych przez model \\(M_1\\), a nie przez jakieś konkretne wartości parametrów tego modelu, musimy całkować po parametrach, by pozbyć się \\(\\theta\\) z równania.\r\nTeraz zerknijmy na mianownik naszego równania zawierający \\(P(D)\\). Tutaj sprawa się komplikuje, ponieważ by obliczyć \\(P(D)\\) musielibyśmy całkować po wszystkich (nieskończenie wielu) możliwych modelach. Jego obliczenie jest w zasadzie niemożliwe zarówno analitycznie (jak wyznaczyć rozkład \\(P(M)\\) dla wszystkich możliwych modeli), jak i numerycznie (ponieważ musielibyśmy wymyślić i policzyć te wszystkie modele).\r\nA co jeśli porównamy dwa modele ze sobą?\r\n\\[ \\underbrace{{\\frac{P(M_1|D)}{P(M_2|D)}}}_{\\text{stosunek a posteriori}} = \\frac{\\frac{P(D|M_1)P(M_1)}{P(D)}}{\\frac{P(D|M_2)P(M_2)}{P(D)}} = \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{{BF_{12}}} * \\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{stosunek a prior}}\\]\r\nRobiąc tak, pozbywamy się konieczności obliczania \\(P(D)\\)!\r\nCzynnik Bayesowski\r\nJednak oceniając modele nie używamy stosunku prawdopodobieństw posteriori, lecz stosunku marginalnych wiarygodności nazywanym czynnikiem Bayesowskim:\r\n\\[BF_{12} = \\frac{P(D|M_1)}{P(D|M_2)}\\]\r\nJest tak dlatego, że stosunek prawdopodobieństw a posteriori zależy od danych, ale także od prawdopodobieństw a priori modeli. Manipulując nimi, moglibyśmy zawsze uzyskać miarę faworyzującą nasz model. Użycie stosunku funkcji wiarygodności, mówi nam o ile bardziej/mniej prawdopodobne jest, że \\(M_1\\) wyprodukował obserwowane dane od \\(M_2\\). Ponadto jeśli przepiszemy wzór na stosunek a posteriori, uzyskamy\r\n\\[ \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{BF_{12}} = \\underbrace{\\frac{P(M_1|D)}{P(M_2|D)}}_{\\text{stosunek posteriori }}:\\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{stosunek a priori}}\\]\r\nCo daje nam dodatkową interpretację czynnika Bayesowskiego. Jest on stosunkiem prawdopodobieństw posteriori podzielony przez stosunek prawdopodobieństw a priori naszych dwóch modeli. Mówi nam o ile zmieniły się nasze przekonania a priori po zobaczeniu danych. Innymi słowy, czynnik Bayesowski mówi nam jak zmieniły się pod wpływem danych nasze początkowe przekonania co do tego, który model jest lepszy.\r\nZałóżmy, że przeprowadziliśmy analizę dwóch modeli i otrzymaliśmy \\(stosunek \\ posteriori = 4\\), faworyzujący pierwszy model. Jednak \\(BF_{12} = 0.5\\), mówiąc nam, że drugi model wyprodukował obserwowane dane z dwa razy większym prawdopodobieństwem. Skąd taka rozbieżność? Wynika ze tego, że \\(stosunek \\ a \\ priori = 8\\), od początku faworyzował pierwszy model.\r\nZwróćmy też uwagę, że jeśli przypiszemy modelom takie same prawdopodobieństwa a priori, stosunek a posteriori będzie się równał czynnikowi Bayesa.\r\nCzynnik Bayesowski a złożoność modelu\r\nCzynnik Bayesowski w naturalny sposób karze za złożoność modelu, choć ta właściwość nie jest widoczna na pierwszy rzut oka. Żeby ją zwizualizować, wyobraźmy sobie, że \\(P(D|M)\\) nie jest pojedyńczą wartością prawdopodobieństwa otrzymania zaobserwowanych danych, lecz rozkładem prawdopodobieństwa wygenerowania wszystkich możliwych zbiorów danych, przez model. Weźmy dwa takie rozkłady dla modeli: prostszego modelu - \\(M_1\\) i bardziej złożonego \\(M_2\\). Bardziej złożony model jest w stanie wygenerować więcej możliwych zbiorów danych niż prostszy, ponieważ jest bardziej elastyczny.\r\n\r\nCałka prawdopodobieństw (pole pod wykresem) każdego z rozkładów musi wynosić 1 (z definicji rozkładów prawdopodobieństwa). Oznacza to, że dla przedziału \\(C_1\\) model \\(M_1\\) będzie miał zawsze większe prawdopodobieństwo wygenerowania danych niż \\(M_2\\), w konsekwencji czynnik Bayesa będzie faworyzował \\(M_1\\). Logika stojąca za taka własnością \\(BF\\) jest iście Occamowska. Zadajmy sobie pytanie, który z modeli ma większe prawdopodobieństwo wygenerowania danych w przedziale \\(C_1\\) jeśli otrzymane przez nas dane mogą być wyjaśnione przez oba modele. Ten, który częściej będzie generował zbiory danych zawierające się w przedziale \\(C_1\\), czyli \\(M_1\\).\r\nBayesian Point Null Hypothesis Testing\r\nPrzypomnijmy sobie NHST (Null Hypothesis Significance Testing), które testuje hipotezy zerową i alternatywną. Hipoteza zerowa zwykle oznacza, że jakiś parametr \\(\\delta = 0\\), a hipoteza alternatywna, że \\(\\delta \\neq 0\\). czynnik Bayesa w naukach społecznych jest często wykorzystywany podobnym celu. Nazywa się to Bayesian Point Null Hypothesis Testing (BPNHT). Testujemy model, który zakłada, że \\(\\delta\\) może przyjąć dowolną wartość, względem takiego, w którym jego wartość jest zerowa. Takie podobieństwo pomiędzy tymi metodami nie jest przypadkowe i obie metody dzielą pewne problemy, o czym wkrótce.\r\nJako przykład posłużymy się czynnikiem Bayesa do przeprowadzenia Bayesowskiej wersji jednoczynnikowej analizy wariancji. Wygenerujmy sobie dane:\r\n\r\n\r\nset.seed(123)\r\nfactor_a = sample(c(rep(0,50),rep(1,50)))\r\ny = rnorm(100, 3 + 0.80*factor_a, 1)\r\ndata = data.frame(id = 1:50, y = y, factor_a = factor_a)\r\n\r\n\r\nCi z Was, którzy są zaznajomieni z klasyczną wersją analizy wariancji wiedzą, że Anova to specjalny przypadek regresji liniowej. Jest też idealnym przykładem do użycia czynnika Bayesowskiego w praktyce, ponieważ polega na porównaniu modeli. Najprostszym modelem, naszym modelem zerowym, będzie model, który zawiera tylko stałą. Modelem bardziej skomplikowanym, który chcemy przetestować jest model zawierający dodatkowo współczynnik regresji dla czynnika a:\r\n\\[\\begin{eqnarray*}\r\n&  &\r\nM_0: \\ \\ y = \\alpha\\\\\r\n&  &\r\nM_1: \\ \\ y = \\alpha + \\beta*X_a \\\\\r\n&  &\r\n\\end{eqnarray*}\\]\r\nJeśli \\(y\\) nie jest powiązany z czynnikiem \\(X_a\\), czynnik Bayesa powinien faworyzować model zerowy.\r\nSavage-Dickey density ratio\r\nDla pewnych modeli, jeśli wybierzemy odpowiednie rozkłady a priori, możemy obliczyć \\(BF\\) analitycznie. W przypadku bardziej skomplikowanych zdani jesteśmy na metody numeryczne. Niestety, dla wielu modeli estymacja \\(BF\\) jest trudna i niełatwa do zastosowania w praktyce.\r\nIstnieje jednak prosty sposób na obliczenie czynnika Bayesowskiego dla modeli zagnieżdżonych (nested models), czyli takich, w których jeden model możemy traktować jako rozszerzenie drugiego. Popatrzmy na \\(M_0\\) i \\(M_1\\). Możemy przeformułować \\(M_0\\) w następujący sposób:\r\n\\[M_0: \\ \\ y = \\alpha + \\beta*X_a, \\  \\ \\beta = 0\\] Teraz widzimy, że \\(M_0\\) jest zagnieżdżony w \\(M_1\\), to znaczy jest modelem \\(M_1\\), którego wartość parametru \\(\\beta\\) została ustawiona jako stała.\r\nGeneralnie, mając dwa modele z parametrami \\(\\varphi\\) i \\(\\delta\\), takie, że \\(M_0:\\delta=\\delta_0,\\varphi\\) i \\(M_1:\\delta,\\varphi\\), które spełniają warunek \\(p(\\varphi\\mid M_0) = p(\\varphi\\mid \\delta=\\delta_0,M_1)\\), możemy obliczyć \\(BF_{01}\\) w następujący sposób:\r\n\\[\\text{BF}_{01} = \\frac{p(\\delta=\\delta_0\\mid y,M_1)}{p(\\delta=\\delta_0\\mid M_1)}\\]\r\nDowód\r\nZ definicji:\r\n\\[\\begin{equation}\\label{eq:bf}\\text{BF}_{01}=\\frac{p(y \\mid M_0)}{p(y \\mid M_1)}.\\end{equation}\\]\r\nMożemy przekształcić \\(p(y \\mid M_0)\\) w: \\[\\begin{equation}\r\n\\begin{split}\r\n p(y \\mid M_0) &= \\int p(y \\mid \\varphi,M_0) \\, p(\\varphi\\mid M_0) \\, \\mathrm{d} \\varphi \\\\\r\n  &= \\int p(y \\mid \\varphi,\\delta=\\delta_0,M_1) \\, p(\\varphi\\mid \\delta=\\delta_0,M_1) \\, \\mathrm{d} \\varphi \\\\\r\n  &= p(y \\mid \\delta=\\delta_0,M_1).\\\\\r\n\\end{split}\r\n\\label{eq:ml-m0}\r\n\\end{equation}\\]\r\nStosując twierdzenie Bayesa do ostatniego wyniku:\r\n\\[\\begin{equation}\\label{eq:ml-bt}\r\np(y \\mid \\delta=\\delta_0,M_1) = \\frac{p(\\delta=\\delta_0\\mid y,M_1) \\, p(y \\mid M_1)}{p(\\delta=\\delta_0\\mid M_1)}\\end{equation}\\]\r\nWięc:\r\n\\[\\begin{equation}\r\n\\begin{split}\r\n  \\text{BF}_{01} &\\overset{\\eqref{eq:bf}}{=} \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}\\\\\r\n  &= p(y \\mid M_0) \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &\\overset{\\eqref{eq:ml-m0}}{=} p(y \\mid \\delta=\\delta_0,M_1) \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &\\overset{\\eqref{eq:ml-bt}}{=} \\frac{p(\\delta=\\delta_0\\mid y,M_1) \\, p(y \\mid M_1)}{p(\\delta=\\delta_0\\mid M_1)} \\cdot \\frac{1}{p(y \\mid M_1)}\\\\\r\n  &= \\frac{p(\\delta=\\delta_0 \\mid y,M_1)}{p(\\delta=\\delta_0\\mid M_1)},\r\n\\end{split}\r\n\\end{equation}\\]\r\nObliczmy sobie \\(BF_{10}\\) posługując się numerycznym przybliżeniem stosunku Savage-Dickey. By to zrobić musimy najpierw zdefiniować nasz \\(M_1\\). Przyjmijmy, że rozkłady a priori zarówno \\(\\alpha\\) jak i \\(\\beta\\) to:\r\n\\[ \\alpha, \\beta \\sim N(0,10)\\] Wobec tego mianownik stosunku Savage-Dickey, będzie gęstością prawdopodobieństwa wylosowania 0 z powyższego rozkładu.\r\n\r\n\r\nprior_beta_0 = dnorm(0,0,10)\r\n\r\n\r\nNasz model w JAGS (pisałem o nim tutaj) będzie wyglądał tak:\r\n\r\n\r\nm1 = '\r\nmodel{\r\nfor(i in 1:length(y)){\r\ny[i] ~ dnorm(mu[i], precision)\r\nmu[i] = alpha + beta*factor_a[i]\r\n}\r\nprecision ~ dunif(0.0000001,100)\r\nalpha ~ dnorm(0,10^-2)\r\nbeta ~ dnorm(0,10^-2)\r\n}'\r\n\r\n\r\nWyprodukujmy rozkłady posteriori parametru \\(\\beta\\).\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rjags)\r\nlibrary(ggmcmc)\r\nlibrary(polspline)\r\n\r\n# Parametry, które chcemy śledzić. \r\nparams = c(\"beta\")\r\n# Inicjacja modelu\r\njmod1 = jags.model(file = textConnection(m1), data = data, n.chains = 4, inits = NULL, n.adapt = 100)\r\n# Wypalanie\r\nupdate(jmod1, n.iter=100^2, by=1)\r\n# Losowanie próbek z rozkładu posteriori\r\npost = coda.samples(jmod1, params, n.iter = 10*100^2, thin = 1)\r\n# tidy format\r\nmodel1 = ggs(post)\r\n\r\n\r\nTeraz wyestymujemy prawdopodobieństwo wylosowania \\(\\beta = 0\\) z rozkładu posteriori.\r\n\r\n\r\npost_samples = filter(model1, Parameter == \"beta\")$value\r\nfit.posterior <- logspline(post_samples)\r\nposterior_beta_0 <- dlogspline(0, fit.posterior)\r\n\r\n\r\nMamy już wszystko by policzyć \\(BF_{01}\\). Jednak ponieważ \\(M_0\\) jest naszym modelem zerowym, popatrzmy na \\(BF_{10}\\) czyli \\(\\frac{1}{BF_{01}}\\).\r\nNasz \\(BF_{10}\\) wynosi\r\n\r\n\r\n1/(posterior_beta_0/prior_beta_0)\r\n\r\n[1] 3.913636\r\n\r\nOtrzymaliśmy wartość \\(BF_{10}\\) wskazującą to, że porównując te dwa modele, jest ca. 4 razy bardziej prawdopodobne, że \\(M_1\\) wyprodukował obserwowane dane niż \\(M_0\\). Dużo? Mało? Jedna z propozycji interpretacji \\(BF\\) jest następująca (Kass & Raftery, 1995):\r\n\r\nBF\r\nDowody\r\n< 1\r\nWspierające model zerowy\r\n1-3\r\nAnegdotyczne\r\n3-10\r\nZnaczne\r\n10-100\r\nSilne\r\n> 100\r\nDecyzyjne\r\n\r\nLikelihood sampling\r\nW przypadku gdy nasze modele nie są zagnieżdżone, musimy szukać innych metod. Jedną z nich jest naiwne symulowanie metodą Monte Carlo. Opiera się na pomyśle, że \\(P(Y|M_i)\\) możemy przybliżyć w następujący sposób (Gronau et al., 2017):\r\n\\[P(D \\mid M_i) = \\int P(D \\mid \\theta, M_i) \\ P(\\theta, M_i) \\ \\text{d}\\theta \\approx \\frac{1}{n} \\sum^{n}_{\\theta_i \\sim P(\\theta \\mid M_i)} P(D \\mid \\theta, M_i)\\] Popatrzmy na naszą całkę. Jeśli potraktujemy \\(P(D \\mid \\theta, M_i)\\) jako zmienną \\(x\\), a \\(P(\\theta, M_i)\\) jako związaną z \\(x\\) funkcję prawdopodobieństwa \\(P(X)\\) (którą w istocie jest), otrzymamy:\r\n\\[P(D \\mid M_i) = \\int x \\ P(x) \\ \\text{dx}\\]\r\nCzyli wzór na średnią. Możemy więc przybliżyć wartość \\(P(D|M_i)\\) losując najpierw \\(\\theta\\) z rozkładu a priori, następnie obliczając \\(P(D \\mid \\theta, M_i)\\) poprzez wstawienie uzyskanej \\(\\theta\\). Jeśli powtórzymy to wielokrotnie, a otrzymane prawdopodobieństwa uśredniamy, uzyskamy estymatę \\(P(D \\mid M_i)\\). Takie losowanie również możemy wykonać przy pomocy JAGS, jednak uzyskany \\(BF\\) nie jest zbyt stabilny. Dlatego stosuje się udoskonalone metody losowania takie jak np. Bridge Sampling. Nie będziemy jednak ich teraz dokładnie omawiać.\r\nPorównywanie modeli?\r\nCzynnik Bayesa wydaje się być niezłą metodą, którą można wykorzystać do wnioskowania statystycznego. Być może nawet pomoże nam on rozwiązać kryzys replikacyjny. Tak też pomyślało wielu, co skutkowało tym, że programy statystyczne takie jak np. JASP zwracają nam podsumowania Bayesowskich analiz defaultowo w postaci czynnika Bayesa. Również czasopisma naukowe często oczekują od autorów, jeśli Ci posługują się statystyką Bayesowską, wyników w postaci czynników Bayesa.\r\nNiektóre czasopisma wręcz wymagają czynników Bayesa jako metody przeciwdziałającej kryzysowi replikacyjnemu (pisałem o kryzysie replikacyjnym trochę tutaj). Skoro określono algorytm weryfikacji hipotez statystycznych w statystyce Bayesowskiej (rzekomo skuteczniejszy od statystyki częstosciowej), możemy wreszcie wrócić do pracy, nie martwiąc się o problem replikacji.\r\nNiestety, jak to się zwykle okazuje, rzeczywistość nie jest taka różowa. Czynnik Bayesa jest kontrowersyjny wśród statystyków. Różnice w podejściach możemy zauważyć na przykład w zaciętych dysputach Davida Mackaya z Adrew Gelmanem. Mackay napisał doktorat wykazujący właściwość czynnika Bayesa do karania skomplikowanych modeli, którą omówiliśmy wcześniej. Andrew Gelman i inni wykazali wiele niedociągnięć i problemów związanych z czynnikiem Bayesa zwłaszcza w kontekście Bayesian Point Null Hypothesis Testing.\r\nGrafika skradziona ze slajdów Richard E. TurneraRzućmy okiem na kilka najważniejszych zastrzeżeń. To, co ważne, to fakt nie wszystkie z nich są na pierwszy rzut oka oczywiste.\r\nCzynnik Bayesa nie mierzy czy model jest prawdziwy\r\nCzynnik Bayesowski jest stosunkiem marginalnej wiarygodności dwóch modeli, czyli o ile bardziej jest prawdopodobne, że jeden z modeli wyprodukował obserwowane dane niż drugi. Nie mówi nam on w żaden sposób o tym, jak dobre te modele w absolutnym sensie (Gelman & Rubin, 1995). Możemy być w sytuacji, w której czynnik Bayesa bardzo faworyzuje jeden z modeli, ale oba modele są fatalnie dopasowane do danych.\r\nW konsekwencji jeśli stosujemy sposób wnioskowania oparty na testowaniu hipotezy zerowej i alternatywnej (tak jak zrobiliśmy wyżej w przypadku Anovy), zasadniczo czynnik Bayesa nie mówi nam więcej niż p-value bez spojrzenia na wielkość efektu. By zbadać dopasowanie modelu potrzebujemy więc przeanalizować rozkłady posteriori parametrów.\r\nCzynnik Bayesowski jest wrażliwy na rozkłady a priori parametrów modelu\r\nRozkład posteriori parametrów \\(\\theta\\) jest niezależny od rozkładów a prior tychże, gdy liczba obserwacji dąży do nieskończoności.\r\n\\[\\lim_{n\\to\\infty}P(\\theta|D,M) \\propto P(D|\\theta, M)\\]\r\nTak jak pokazaliśmy w pierwszej części tutorialu, gdy liczba obserwacji wzrasta, wpływ rozkładów a priori maleje. Jak jest jednak w przypadku czynnika Bayesa? Przyjmując pewne upraszczające założenia (MacKay, 2003), możemy powiedzieć, że marginalna wiarygodność to:\r\n\\[P(D|M) \\approx P(D|\\theta_{map}, M)P(\\theta_{map}|M)\\sigma_{\\theta|D}\\] Gdzie \\(\\theta_{map} = \\arg \\max_{\\theta} P(\\theta|D,M)\\) to maximum posteriori approximation, czyli takie \\(\\theta\\), dla którego prawdopodobieństwo a posteriori jest największe. Z kolei \\(\\sigma_{\\theta|D}\\) to odchylenie standardowe \\(P(\\theta|D,M)\\).\r\nW konsekwencji czynnik Bayesowski zależy od a priori rozkładów parametru modelu \\(P(\\theta_{map}|M)\\). W przypadku gdy użyjemy nieinformatywnego rozkładu a priori dla \\(\\theta\\), rozkładu jednostajnego o szerokości \\(\\sigma_{\\theta}\\), wtedy wzór upraszcza się do\r\n\\[P(D|M) \\approx P(D|\\theta_{map}, M)\\frac{\\sigma_{\\theta|D}}{\\sigma_{\\theta}}\\]\r\nIm bardziej chcemy by dane przemówiły za siebie (im bardziej nieinformatywny rozkład a priori zastosujemy), tym bardziej faworyzowany będzie model zerowy w przypadku Bayesian Point Null Hypothesis Testing. Wrażliwość na rozkłady a priori nie jest tak istotnym problemem, gdy porównujemy ze sobą niezagnieżdżone modele to znaczy w przypadku, gdy testujemy dwa alternatywne wyjaśnienia zjawiska.\r\nRozkłady posteriori parametrów nie muszą się zgadzać z czynnikiem Bayesa\r\nW statystyce częstościowej p-value < 0.05 dla parametru oznacza odrzucenie hipotezy zerowej. Jednak w przypadku analizy Bayesowskiej możemy spotkać się z sytuacjami, w których czynnik Bayesa sugeruje przyjęcie modelu zerowego, jednak interesujący nas parametr w modelu alternatywnym nie zawiera w 95% przedziale wiarygodności, i vice versa (Kruschke & Liddell, 2018). Ta własność czynnika Bayesa jest często nieintuicyjna, ponieważ przenosimy nasze oczekiwania ze statystyki częstościowej.\r\nCzynnik Bayesa a prawdopodobieństwo braku efektu\r\nCzęsto się mówi, że by wnioskować o braku efektu musimy użyć statystyki Bayesowskiej, ponieważ w statystyce klasycznej możemy mówić co najwyżej o braku dowodów na istnienie efektu. Niestety, przypadku BPNHT, pojawia się pewien problem, ponieważ czynnik Bayesa jest wrażliwy na wielkość próby (Morey & Rouder, 2011).\r\nBy zobaczyć to na własne oczy posłużymy się przykładem. Załóżmy, że zebraliśmy dane, które mają rozkład normalny \\(y \\sim N(\\mu,\\sigma)\\), ze znanym odchyleniem standardowym \\(\\sigma = 5\\). Chcemy przetestować dwa modele w myśl BPNHT: \\(M_0: \\mu = 0\\) i \\(M_1: \\mu \\sim U(-\\infty,+\\infty)\\). Czyli model zakładający, że wartość \\(\\mu = 0\\) i model, który zakada, że każda wartość \\(\\mu\\) jest równie prawdopodobna.\r\nDokładny czynnik Bayesa dla tych dwóch modeli dany jest wzorem1:\r\n\\[BF_{10} = \\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}e^{(\\frac{\\overline{\\text{y}}^2n}{2\\sigma})}\\] Gdzie \\(\\overline{\\text{y}}\\) to średnia w próbie. Już we wzorze możemy zobaczyć, że ta wartość będzie coraz większa, gdy \\(n\\) rośnie. Zobaczmy na wykresie jak wyglądają wartości czynnika Bayesa dla ustalonych wartości \\(\\overline{\\text{y}}\\):\r\n\r\n\r\nBF = function(sample_size,sample_mean, sd){\r\n  (sqrt(sample_size)/(sqrt(2*pi)*sd))*exp(((sample_mean^2)*sample_size)/(2*sd^2))\r\n}\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x))+ \r\n  stat_function(fun = BF,args = list(sample_mean = 0.6,sd = 5), aes(color = \"0.6\")) + \r\n  stat_function(fun = BF,args = list(sample_mean = 0.75,sd = 5), aes(color = \"0.75\")) +\r\n  stat_function(fun = BF,args = list(sample_mean = 0.9,sd = 5), aes(color = \"0.9\")) +\r\n  xlim(0,100) +\r\n  xlab(\"N\") + \r\n  ylab(bquote(BF[10])) + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"Sample Mean\", values = c( \"blue\", \"green\", \"red\")) +\r\n  geom_hline(yintercept = 1) +\r\n  theme_light()\r\n\r\n\r\n\r\nOk, im większa próba, tym większe wsparcie dla hipotezy alternatywnej. Choć spojrzeliśmy tylko na dwa partykularne modele, ta własność utrzymuje się generalnie w całym BPNHT (Johnson & Rossell, 2010). Czy to źle? Niekoniecznie, ale to znaczy, że podobnie jak w przypadku p-value, tym mniejszy efekt jest potrzebny by odrzucić hipotezę zerową, im większa próba. Dlatego też ciężko jest wnioskować o braku efektu, ponieważ nie wiemy czy czynnik Bayesa faworyzuje model zerowy, ponieważ jest lepszy, czy dlatego, że nie ma wystarczających dowodów by go odrzucić.\r\nNie oznacza jednak, że statystyka Bayesowska nam na to nie pozwala. Jeśli rozkład posteriori parametru jest silnie skoncentrowany blisko zera, możemy mówić, że pod warunkiem danych mamy dużą pewność braku efektu lub marginalnego efektu.\r\nI co z tym wszystkim zrobić?\r\nDogłębne omówienie wyżej wymienionych problemów możecie znaleźć w artykule “A Review of Issues About Null Hypothesis Bayesian Testing” (Tendeiro & Kiers, 2019).\r\nAlternatywna rzeczywistość, w której, w miejsce statystyki częstościowej, statystyka Bayesowska zdominowała naukę. Grafika skradziona ze slajdów Richarda Moreya.Z jednej strony czynnik Bayesa pozwala na bezpośrednie porównanie prawdopodobieństwa wyprodukowania danych przez cały model, a nie tylko punktowe wartości parametrów. Jednocześnie w naturalny sposób karze złożoność modelu, co w innych miarach dopasowania modelu takich jak na przykład AIC, próbujemy robić estymując złożoność poprzez liczbę wolnych parametrów.\r\nJednakże, jak widzimy, czynnik Bayesa jest miarą, która również słabe strony, nie jest więc cudownym lekiem na statystyczne bolączki. Z tego miejsca mamy dwie możliwości. Możemy używać czynnika Bayesa rozważnie, czyli:\r\nOstrożnie dobierać rozkłady a priori parametrów modelu.\r\nPrzeprowadzać analizę wrażliwość (sensitivity analysis), tzn. sprawdzić jak bardzo wartość czynnika Bayesa ulega zmianie, gdy użyjemy innych rozkładów a priori parametrów. Niestety, konsekwencją tego, że czynnik Bayesa jest trudny do policzenia, jest to żmudny proces.\r\nNie raportować czynnika Bayesa samotnie, ale także miary rozkładu parametrów a posteriori.\r\nAlbo\r\nunikać porównywania modeli. Zamiast tego, na pierwszym miejscu skupić się na tworzeniu wiarygodnych modeli wspieranych przez teorię, przełożyć nacisk na estymację parametrów, tzn. zamiast porównywać interesujący nas model do modelu zerowego z wartością parametru równą 0, skupić się na ewaluacji rozkładu a posteriori parametrów. Wykorzystać Posterior Predictive Testsing, które może nam wskazać problemy z naszym modelem, bez konieczności porównywania go z innym.\r\nUnikać testowania przy pomocy BPNHT zasadności pojedynczego modelu. Prównywanie modeli jest natomiast jak najbardziej przydatne wtedy, gdy mamy dwa lub więcej (np. podyktowane teorią) konkurujące modele inaczej ujmujące mechanizm jakiegoś zajawiska np. pamięci. Co istotne, do tego możemy posłużyć się statystykami prostszymi do obliczenia, jak np. DIC albo WAIC.\r\nZakończenie\r\nDowiedzieliśmy się czym jest czynnik Bayesowski i przy okazji omówiliśmy kontrowersje wokół niego. Mamy już intuicję na temat statystyki Bayesowskiej, estymacji modeli i wnioskowania. Fajnie, by było wreszcie coś policzyć po Bayesowsku co nie jest prostym modelem liniowym. Dlatego w następnej części przyjrzymy się jednej z ciekawszych możliwości statystyki Bayesowskiej - modelom hierarchicznym.\r\nNa koniec dodam, że czynnik Bayesowski, p-value ani żadna inna statystyka nie jest rozwiązaniem kryzysu replikacyjnego. Nie dlatego, że te miary obarczone są wadami, tylko dlatego, że prawdziwe powody leżą gdzie indziej. Badacze czasami naiwnie stosują statystykę, czasami recenzenci wymagają od badaczy stosowania utartych, choć nieadekwatnych procedur. Dodatkowo system ewaluacji pracowników naukowych wymaga od nich by publikowali dużo, a czasopisma naukowe wymagają by publikowali badania świadczące o istnieniu efektu, raczej niż o jego braku. Bez zaadresowania tych problemów, nie sądzę by jakakolwiek statystyczny test rozwiązał problem replikacji w nauce.\r\n\r\n\r\n\r\nGelman, A., & Rubin, D. B. (1995). Avoiding model selection in bayesian social research. Sociological Methodology, 25, 165–173.\r\n\r\n\r\nGronau, Q. F., Sarafoglou, A., Matzke, D., Ly, A., Boehm, U., Marsman, M., Leslie, D. S., Forster, J. J., Wagenmakers, E.-J., & Steingroever, H. (2017). A tutorial on bridge sampling. Journal of Mathematical Psychology, 81, 80–97.\r\n\r\n\r\nJohnson, V. E., & Rossell, D. (2010). On the use of non-local prior densities in bayesian hypothesis tests. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(2), 143–170.\r\n\r\n\r\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430), 773–795.\r\n\r\n\r\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a bayesian perspective. Psychonomic Bulletin & Review, 25(1), 178–206.\r\n\r\n\r\nMacKay, D. J. C. (2003). Model comparison and occam’s razor. In Information theory, inference and learning algorithms. Cambridge university press.\r\n\r\n\r\nMorey, R. D., & Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. Psychological Methods, 16(4), 406.\r\n\r\n\r\nTendeiro, J. N., & Kiers, H. A. (2019). A review of issues about null hypothesis bayesian testing. Psychological Methods, 24(6), 774.\r\n\r\n\r\nDowód możecie znaleść w suplemencie B do tego artykułu (Tendeiro & Kiers, 2019)↩︎\r\n",
    "preview": "posts/2023-01-12-bayes-factor/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-02-01T20:03:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-26-poznajcie-jags/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część 2.5: Poznajcie JAGS",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-12-26",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nJAGS\r\nJAGS in JASP\r\nBonus - diagramy Bayesowskie\r\n\r\nDzisiejszy wpis będzie krótki. Pokażę wam jak zrobić to co robiliśmy w poprzedniej części przy użyciu oprogramowania, które pozwoli nam na tworzenie i estymowanie (przy użyciu MCMC) nawet bardzo skomplikowanych modeli w łatwy sposób. Możliwości jest kilka, ja przedstawie wam JAGS (Just Another Gibbs Sampler). Jest to samodzielne oprogramowanie, które można używać bezpośrednio lub za pomocą języków programowania takich jak R, a także graficznego programu statystycznego JASP. Pobrać możecie go tutaj.\r\nJAGS\r\nPowtórzmy naszą bayesowską regresję z tym razem przy użyciu JAGS. Stwórzmy zbior danych:\r\n\r\n\r\nlibrary(rjags)\r\nlibrary(coda)\r\nlibrary(MCMCvis)\r\n\r\nx = rnorm(100,0,10)\r\ny = 5 + x + rnorm(100,0,10)\r\ndata = data.frame(y,x)\r\n\r\n\r\nMusimy zdefiniować kod naszego modelu. JAGS używa dosyć intuicyjnego kodowania BUGS. Model regresji będzie wyglądał tak:\r\n\r\n\r\nmod = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(y)) {\r\n    y[i] ~ dnorm(a + b * x[i], sigma^-2)\r\n  }}\"\r\n\r\n\r\nOperator ~ oznacza, że parametr po lewej stronie dany jesr rozkładem po prawej. Ponieważ w naszym kodzie tylko zmienna Y występuje w danych, JAGS automatycznie rozpozna, że zdefiniowany rozkład Y jest funkcją wiarygodności. Zauważcie, że gdy definuje b jako wartość losowaną z rozkładu normalnego \\(N(0,100)\\), jako drugi argument podałem \\(100^{-2}\\). JAGS zamiast odchylenia standardowego dla rozkładu normalnego przyjmuje precyzję (precision), czyli odwrotność wariancji. Po więcej szczegółów polecam zajrzeć do dokumentacji.\r\n\r\n\r\n# Parametry, które chcemy śledzić. \r\nparams = c(\"a\",\"b\",\"sigma\")\r\n\r\n## Hiperparametry\r\nn.adapt = 100\r\n# Liczba iteracji adaptacji\r\nni = 3000\r\n# Liczba iteracji \"wypalania\"\r\nnb = 3000\r\n# Liczba próbek z rozkładu post priori\r\nnt = 1\r\n# Odchudzanie - 1 oznacza, bierzemy każdą próbkę z rozkładu post priori\r\nnc = 3\r\n# liczba łańcuchów\r\n\r\n# Inicjacja modelu\r\njmod = jags.model(file = textConnection(mod), data = data, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\n\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 100\r\n   Unobserved stochastic nodes: 3\r\n   Total graph size: 412\r\n\r\nInitializing model\r\n\r\n# Wypalanie\r\nupdate(jmod, n.iter=ni, by=1)\r\n\r\n# Losowanie próbek z rozkładu post priori\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\nZauważcie, że Jags miał tylko 3000 iteracji wypalania, podczas gdy mój kod z poprzedniej części potrzebował ponad 16 razy więcej. JAGS używa kombinacji różnych agorytmów MCMC, ponado dokonuje za nas tunningu hiperparametrów (takich jak na przykład step size w Metropolis-Hasting). Dlatego, oprócz wypalania i losowania, mamy jeszcze adaptację. Obejrzmy sobie rozkłady post priori:\r\n\r\n\r\nplot(post)\r\n\r\n\r\n\r\nPoliczmy statystyki:\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:6100\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n        Mean      SD Naive SE Time-series SE\r\na     4.5803 0.95620 0.010079       0.010650\r\nb     0.9113 0.08775 0.000925       0.000925\r\nsigma 9.3601 0.68294 0.007199       0.007507\r\n\r\n2. Quantiles for each variable:\r\n\r\n       2.5%    25%    50%    75%  97.5%\r\na     2.702 3.9454 4.5744 5.2252  6.471\r\nb     0.738 0.8524 0.9122 0.9695  1.085\r\nsigma 8.148 8.8782 9.3262 9.7982 10.810\r\n\r\nA także sprawdźmy czy łańcuchy się zbiegły:\r\n\r\n\r\ngelman.diag(post)\r\n\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\na              1          1\r\nb              1          1\r\nsigma          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\n\r\nStatystyka Gelmana-Rubina wskazuje, że łańcuchy się zbiegły. Sprawdźmy jeszcze czy aby na pewno stastystyka nie osiągneła takich wartości przez przypadek.\r\n\r\n\r\ngelman.plot(post)\r\n\r\n\r\n\r\nNa wykresie widzimy statystykę Gelmana Rubina liczną dla każdych 50 iteracji następujących po sobie. Dzięki temu możemy sprawdzić czy nasze próby wylosowane są z rozkładów, które naprawdę się zbiegły. Widzimy, że dla początkowych wartości łancuchów statystyka ma wyższe wartośći, co może sugerować, że powinniśmy zastosować dłuższy interwał wypalania (choć w naszym przypadku statystyka nie przekracza nigdzie wartości 1.15, więc bybyłby to raczej zabieg kosmetyczny).\r\nSprawdźmy autokorelację:\r\n\r\n\r\nautocorr.diag(post)\r\n\r\n                 a            b        sigma\r\nLag 0  1.000000000  1.000000000  1.000000000\r\nLag 1  0.029878333  0.021306509  0.050087619\r\nLag 5  0.005264560 -0.002882478  0.003968644\r\nLag 10 0.002108300 -0.009317490  0.006910744\r\nLag 50 0.006366223  0.019776605 -0.017047426\r\n\r\nAutokorelacja praktycznie nie występuje.\r\nJAGS poradził sobie dużo lepiej z regresją liniową, niż mój zabawkowy kod.Ponadto jego składnia jest relatywnie prosta. Dlatego z niego będziemy korzystać w następnych częściach tutorialu.\r\nPrzykładowe kody JAGS dla wielu modeli możecie znaleść tutaj.\r\nJAGS in JASP\r\nJeśli ktoś nie jest wielkim fanem robienia statystyk za pomocą języków programowania, JAGS jest kompatybilny z graficznym programem statystycznym JASP.\r\n\r\nBonus - diagramy Bayesowskie\r\nWarto wspomnieć, że do wizualizacji modeli bayesowskich często używa się grafów (ja do ich budowy używam biblioteki daft w Pythonie). W przypadku naszego modelu:\r\n\r\nimport daft\r\nimport matplotlib.pyplot as plt \r\n\r\npgm = daft.PGM(observed_style=\"inner\")\r\n\r\npgm.add_node(\"alpha\", r\"$\\alpha$\", 0.5, 2)\r\npgm.add_node(\"beta\", r\"$\\beta$\", 1.5, 2)\r\npgm.add_node(\"sigma\", r\"$\\sigma$\", 2.5, 2)\r\npgm.add_node(\"x\", r\"$x_i$\", 2, 1, observed=True)\r\npgm.add_node(\"y\", r\"$y_i$\", 1, 1, observed=True)\r\n\r\npgm.add_edge(\"alpha\", \"y\")\r\npgm.add_edge(\"beta\", \"y\")\r\npgm.add_edge(\"x\", \"y\")\r\npgm.add_edge(\"sigma\", \"y\")\r\n\r\npgm.add_plate([0.5, 0.5, 2, 1], label=r\"$i = 1, \\ldots, N$\", shift=-0.1)\r\n\r\npgm.render()\r\nplt.show() \r\n\r\n\r\n\r\n\r\nDiagram pokazuje, że rozkład zmiennej \\(y_i\\) definują 3 nieobserwowalne parametry (pojedyńcze okręgi) i jedna obserwowalna zmienna (pogrubiony okrąg).\r\nW przypadku modeli bayesowskich zapewno dosyć często będziecie się spotykać z takimi graficznymi opisami modeli. Do grafu dołączane są zwykle definicje parametrów. W naszym przypadku:\r\n\\[\\alpha \\sim U(-1000, 1000)\\] \\[ \\beta \\sim N(0, 100) \\] \\[ \\sigma \\sim U(0.000001,1000)\\] \\[ y_i \\sim N(a + b * x_i, \\sigma)\\]\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-26-poznajcie-jags/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-05-01T14:22:09+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-30-tutorial-bayes-ii/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część II: Estymacja modeli bayesowskich",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-10-30",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nWstęp\r\nMCMC\r\nPrzykład wizualny\r\nPrzykład praktyczny - Bayesowska Regresja Liniowa\r\n\r\nDiagnostyka\r\nGelman-Rubin Convergence Diagnostic\r\nAutokorelacja\r\nPosterior predictive check\r\n\r\nPodsumowanie\r\n\r\n\r\n\r\n\r\nPost będzie aktualizowany.\r\n\r\nWstęp\r\nW poprzedniej części dowiedzieliśmy się na czym polega wnioskowanie bayesowskie. W następnych częściach będziemy budować modele bayesowskie i będziemy używać do tego dedykowanych pakietów i oprogramowania. Dlatego dzisiaj dowiemy się jak od wewnątrz wygląda estymacja rozkładów post priori, gdy nie jest to możliwe metodą analityczną.\r\nZałóżmy, że zdefiniowaliśmy sobie jakąś funkcję wiarygodności i rozkłady a priori parametrów \\(\\theta\\). Chcemy numerycznie przybliżyć rozkład \\(P(\\theta|y)\\)\r\n\\[P(\\theta|y) \\propto P(y|\\theta)P(\\theta)\\]\r\nIstnieją dwie najpopularniejsze metody numerycznej estymacji modeli bayesowskich - MCMC i Variational Bayes.\r\nVariational Bayes to metoda, która zmniejsza wymagania obliczenowe przyjmując pewne upraszczające założenia, dzięki czemu można ją efektywnie stosować do dużych zbiorów danych.\r\nMCMC (Markov Chain Monte Carlo) jest metodą która jest dokładniejsza, ale bardziej czasochłonna i to ją dziś wam przedstawię.\r\nMCMC\r\nZastanówmy się przez chwilę nad problemem znalezienia rozkładów post priori. Gdybyśmy szukali najlepszych estymat punktowych parametrów, jak w częstościowym wnioskowaniu, moglibyśmy ewaluować naszą funkcję \\(P(\\theta|y)\\) i szukać takich \\(\\theta\\) dla których przyjmuje najwyższą wartość.\r\nMoglibyśmy to na przykład zrobić w ten sposób. Dla funkcji \\(f(x)\\) zaczynamy z losową wartością \\(x\\). W następnym kroku dodajemy małą losową wartość \\(\\Delta\\) do \\(x\\) i sprawdzamy czy \\(f(x + \\Delta)\\) jest większe. Jeśli tak, \\(x + \\Delta\\) staje się naszym \\(x\\). Jeśli nie, powtarzamy porzednie kroki. Zatrzymujemy się wtedy, gdy \\(x\\) przestanie się zmieniać.\r\nTen algorytm ma pewną wadę, ponieważ odszuka on najbliższe lokalne maksimum, które nie musi być globalnym. Do tego jeszcze wrócimy.\r\nW podejsciu bayesowskim szukamy rozkładu \\(P(\\theta|y)\\). Moglibyśmy go przybliżyć losując próbę z niego. Ale jak wylosować próbę z rozkładu, którego nie znamy? Możemy najpier poszukać środka rozkładu za pomocą wyżej opisanej metody. Gdy już go znajdziemy, możemy zasymulować losowanie w prosty sposób. Jeśli \\(P(\\theta|y)\\) ma wartość 0.2 w pewnym miejscu rozkładu, a w 0.1 w innym, to wiemy że pierwsza wartość \\(\\theta\\) musi występować średnio dwa razy częściej.\r\nMożemy więc przyjąć następującą strategię:\r\nZainicjować losowo pierwszą wartość \\(\\theta\\) i obliczyć \\(P(\\theta|y)\\).\r\nPrzesunąć się o losową wartość \\(\\Delta\\) (losowaną z rozkładu normalnego, którego SD jest wielkością przesunięcia - step size) i obliczyć \\(P(\\theta + \\Delta|y)\\).\r\nJeśli \\(P(\\theta + \\Delta|y)\\) jest większe od \\(P(\\theta|y)\\) przyjmujemy \\(\\theta + \\Delta\\) jako losową próbkę z rozkładu.\r\nJeśli \\(P(\\theta + \\Delta|y)\\) jest mniejsze od \\(P(\\theta|y)\\) losujemy wartość między z przedziału [0,1].\r\nPrzyjmujemy \\(\\theta + \\Delta\\) jako losową próbkę z rozkładu jeśli wylosowana wartość jest mniejsza od \\(\\frac{P(\\theta + \\Delta|y)}{P(\\theta|y)}\\).\r\nPowtarzamy procedurę od punktu 2, jeśli wylosowana wartość jest większa od \\(\\frac{P(\\theta + \\Delta|y)}{P(\\theta|y)}\\).\r\nPrzyjąć \\(\\theta+\\Delta\\) jako nową wartośc \\(\\theta\\) i powtórzyć procedurę od punktu 2.\r\nTa wersja MCMC to algorytm Metropolisa-Hastingsa. Żeby być rzetelnym, prawdopodobnie musiałbym zacząć od przedstawienia wam czym jest proces stochatyczny, łańcuchy Markova, rozkład stacjonarny, et cetera. Ale ponieważ chcę się skupić na pokazaniu jak wygląda estymacja MCMC w praktyce, pominę to. Dowód i opis matematyczny działania algorytmu Metropolisa-Hastingsa możecie znaleźć tutaj.\r\nSkąd wiemy, że losujemy z właściwej przestrzeni parametrów rozkładu? Żeby wyjaśnić ten termin pójrzmy na wykres poniżej.\r\n\r\n\r\n\r\nJak widzimy, ten rozkład prawdopodbieństwa ma dwa maksima. Chcielibyśmy, by rozkład post priori był odwzorowany jak najdokładniej. Jednak nasz algorytm może w utknąć makismum po lewej, dając nam błędne przybliżenie rozkładu.\r\nCzy nasz algorytm jest w stanie poradzić sobie z tym? Odowiedź brzmi: Wykonując nieskończoną liczbę iteracji, wartości zbiegną do właściwej przestrzeni rozkładu. Dzieje się tak dlatego, że nasz algorytm może poruszać się także po mniejszych wartościach \\(P(\\theta|y)\\), jest więc w stanie opuścić lokalne maksima.\r\nMy jednak nie dysponujemy nieskończoną liczbą iteracji. Dlatego algorytm powtarzamy \\(n\\) razy. Każdą instancję nazywamy łańcuchem, a ponieważ każdy łańcuch rozpoczyna w innym losowym miejscu przestrzeni parametrów, jeśli łańcuchy się zbiegną, możemy domniemywać, że odnaleźliśmy właściwą przestrzeń i zacząć losować próbki z naszego rozkładu.\r\nPrzykład wizualny\r\nZobaczmy to na przykładzie, który kiedyś podpatrzyłem i wydaje mi się idealny do zaprezentowania działania MCMC. Weźmy sobie pewną funkcję, która jako argumenty bierze \\(x\\) i \\(y\\).\r\n\r\n\r\nf = function (x,y) {\r\n  return(20*exp(-0.2*sqrt((x^2+y^2)/2))+exp(0.5*(cos(2*pi*x)+cos(2*pi*y))))\r\n}\r\n\r\nlibrary(lattice)\r\npoints = matrix(nrow = 61, ncol = 61, seq(-3,3,0.1)) \r\nfilled.contour(x = points[,1], y = points[,1], z = f(points, t(points)), nlevels=20)\r\n\r\n\r\n\r\nJak widzimy, nasza funkcja ma wiele lokalnych maksimów, ale tylko jedno globalne na środku. Sprawdźmy jak poradzi sobie nasz algorytm. Uwaga, funkcja, której używamy nie jest rozkładem prawdopodbieństwa, dlatego algorytm wygląda trochę inaczej niż w podpunktach przedstawionych wcześniej.\r\n\r\n\r\njmp = 0.2 # step size\r\niter = 10000 #liczba iteracji algorytmu\r\nchain = 3 # liczba łańcuchów\r\n\r\nsamples =  array(NA, c(10000,3,3)) # Pusty tensor na nasze próbki\r\n\r\n# Wartości początkowe łańcuchów\r\nsamples[1,1,1] = 2.3 \r\nsamples[1,2,1] = 1.98\r\nsamples[1,1,2] = -2.3  \r\nsamples[1,2,2] = -1.98\r\nsamples[1,1,3] = 1 \r\nsamples[1,2,3] = -2.5\r\n\r\nfor (j in 1:chain){\r\nsamples[1,3,j] = f(samples[1,1,j], samples[1,2,j]) # ewaluujemy naszą funkcję dla wartości\r\n# początkowych łańcuchów\r\nn = 1\r\n\r\nwhile(n <= (iter-1)) { #Iterujemy dopóki nie zbierzemy 'iter' próbek\r\n  new_x = rnorm(1, samples[n,1,j], jmp) # przemieszczamy się o losową wartość dla wartości x\r\n  new_y = rnorm(1, samples[n,2,j], jmp) # przemieszczamy się o losową wartość dla wartości y\r\n  \r\n  if (exp(-12*(f(samples[n,1,j], samples[n,2,j])-f(new_x, new_y))) > runif(1,0,1)) {\r\n  # Sprawdzamy czy nowe wartości zwracają wyzszą wartość naszej funkcji. Poniweważ funkcja nie\r\n  # jest rozkładem prawdopodobieństwa nie możemy policzyć stosunku prawdopodobieńst. Zamiast\r\n  # tego używamy funkcji eksponencjalnej różnicy pomnożonej przez ujemną stałą.  \r\n    \r\n    n = n + 1\r\n    samples[n,1,j] = new_x\r\n    samples[n,2,j] = new_y\r\n    samples[n,3,j] = f(new_x, new_y)\r\n\r\n  }   \r\n}\r\n}\r\n\r\n\r\nZobaczmy jak poradziły sobie nasze łańcuchy.\r\n\r\n\r\nfilled.contour(x=points[,1], y=points[,1], z =f(points,t(points)), nlevels=20, plot.axes = {\r\n  axis(1); axis(2); lines(samples[1:10000,1:2,1], col =\r\n  \"black\");lines(samples[1:10000,1:2,2], col = \"blue\"); lines(samples[1:10000,1:2,3], col = \r\n  \"green\") })\r\n\r\n\r\n\r\nŁańcuchy znajdowały na swojej drodze lokalne maksima, w których pozostawały na jakiś czas, po czym zmierzały do następnych. Ostatecznie wszystkie zbiegły się do lokalnego maksimum.\r\nDobrze. Wiemy, że nasz algorytm znalazł globalne maksimum, ponieważ znamy funkcję, której ekstremum szukamy. Jednak w przeciwieństwie do powyższego przykładu, zwykle nie znamy funkcji generującej rozkład post priori - dopiero staramy się go estymować. Jak więc sprawdzić czy znaleźliśmy własciwą przestrzeń losowania? Spójrzmy na pierwsze 200 interacji dla argumentu \\(x\\).\r\n\r\n\r\nx_samples = as.data.frame(samples[1:iter,1,1:3])\r\nx_samples$iteration = 1:dim(samples)[1]\r\ncolnames(x_samples) = c(1,2,3,\"Iteration\")\r\nx_samples_burn_in = x_samples[1:200,]\r\nx_samples_in_space = x_samples[2000:2500,]\r\nx_samples_burn_in = pivot_longer(x_samples_burn_in,cols =1:3, names_to = \"chain\", values_to = \"x\")\r\nx_samples_in_space = pivot_longer(x_samples_in_space,cols =1:3, names_to = \"chain\", values_to = \"x\")\r\n\r\nggplot(x_samples_burn_in, mapping = aes(x = Iteration, y = x, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWidzimy, że każdy łańcuch oscyluje wokół innej wartości. Teraz spójrzmy na iteracje między 2000 a 2500.\r\n\r\n\r\nggplot(x_samples_in_space, mapping = aes(x = Iteration, y = x, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWidzimy, że wszytkie łańcuchy się zbiegły. Co prawda, nie daje nam to całkowitej pewności, że łańcuchy odnalazły właściwą przestrzeń.\r\nDziałanie algorytmu MCMC dzieli się na dwie części: wypalanie (burn-in) i losowanie z rozkładu post priori. Wypalamy łańcuchy dopóki się nie zbiegną, wtedy możemy rozpocząć estymacje rozkładu post priori z próbek z łańcuchów. Im większej liczby łańcuchów użyjemy, tym silniejszą mamy przesłankę (gdy wszystkie się zbiegną), że odnaleźliśmy właściwą przestrzeń losowania.\r\nPrzykład praktyczny - Bayesowska Regresja Liniowa\r\nNabyliśmy już intuicję, jak działa MCMC. Zabierzmy się więc za prawdziwy statystyczny problem.\r\nPowiedzmy, że chcemy policzyć bayesowską regresję liniową zmiennej \\(y\\) ze względu na \\(x\\):\r\n\\[y_i = \\alpha + \\beta x_i + \\epsilon{_i}\\] gdzie \\(\\alpha\\) to stała, \\(\\beta\\) to współćzynnik regresji a \\(\\epsilon{_i}\\) to błąd.\r\nWytwórzmy sobie przykładowe dane:\r\n\r\n\r\nx = rnorm(100,0,10)\r\ny = 5 + x + rnorm(100,0,10)\r\n\r\n\r\nW ujęciu Bayesowskim będziemy szukali rozkładów post priori interesujących nas parametrów. Naszą zmienną zależną zamodelujemy w następujący sposób:\r\n\\[ y_i \\sim N(\\alpha + \\beta x_i, \\sigma)\\]\r\nCo oznacza, że wartość zmiennej zależnej u każdej obserwacji jest wylosowana z rozkładu normalnego o średniej \\(\\alpha + \\beta x_i\\) i odchyleniu standardowym \\(\\sigma\\). Jednocześnie, jak widzimy, jest to nasza funkcja wiarygodności \\(P(y|\\theta)\\) - prawdopodbieństwo otrzymania \\(y\\) pod warunkiem parametrów \\(\\alpha\\), \\(\\beta\\) i \\(\\sigma\\) zapisanych w skrócie jako wektor \\(\\theta\\). Musimy zdefiniować jeszcze rozkłady a priori dla parametrów \\(\\theta\\) i możemy przystąpić do obliczania rozkładów post priori interesującyh nas parametrów. W tym przykładzie użyjemy nieinformatywnych rozkładów a priori. Zamodelujemy \\(\\alpha\\) i \\(\\sigma\\) rozkładami jednostajnymi, a \\(\\beta\\) rozkładem normalnym o średniej 0.\r\nZaimplementujmy algorytm Metropolisa-Hastingsa by wyestymować rozkłady parametrów regresji.\r\n\r\n\r\n# Definujemy naszą funkcję wiarygodności, prior i posterior. Zwrócie uwagę, że funkcję\r\n# zwracają nam logarytm naturalny gęstości prawdopodobieństwa. Jest to użyteczna \r\n# transformacja, ponieważ komputery nie radzą sobie dobrze z wartościami bardzo bliskimi 0. \r\n\r\nlikelihood <- function(parameters){\r\n  a=parameters[1]; b=parameters[2]; sigma=parameters[3]\r\n  sum(dnorm(y,a +b*x, sigma, log = TRUE))\r\n}\r\n\r\nprior <- function(parameters){\r\n  a=parameters[1]; b=parameters[2]; sigma=parameters[3]\r\n  sum(dunif(a,-1000,1000,log = TRUE), dnorm(b,0,100,log = TRUE),   \r\n  dunif(sigma,0.000001,1000,log = TRUE))\r\n}\r\n\r\nposterior <- function(parameters) {likelihood(parameters) + prior(parameters)}\r\n\r\nchain = 3 # Liczba łańcuchów\r\nn.iter <- 300000 # Liczba iteracji\r\nresults <- array(NA, c(n.iter,3,3)) # Pusty tensor na nasze próbki\r\n\r\nn.burn = 50000 # Liczba iteracji, którą odrzucimy w ramach wypalania\r\n\r\nfor (j in 1:chain){\r\n\r\nparameters <- c(runif(1,-50,50), rnorm(1,0,10),runif(1,0.000001,100)) # Losujemy wartości początkowe dla łańcucha\r\nresults[1,,j] <- parameters\r\nnaccepted = 2\r\n\r\n  while(naccepted <= n.iter){\r\n    \r\n    candidate <-  rnorm(3, mean = parameters, sd = 0.1) # przemieszczamy się o losową wartość\r\n    # parametrów od wartości obecnych parametrów \r\n    \r\n    ratio <- exp(posterior(candidate) - posterior(parameters)) # Odejmujemy ponieważ log(a/b) \r\n  # = log(a) - log(b). Różnicę wkladamy do funckcji eksponencjalnej by z logarytmów otrzyamć\r\n  # stosunek prawdopodobieństw. \r\n    \r\n    if (runif(1) < ratio) {parameters <- candidate \r\n    results[naccepted, ,j] <- parameters\r\n    naccepted = naccepted + 1}\r\n  }}\r\n\r\nresults <- results[(n.burn+1):n.iter,,] # Usuwamy 'n.burn' pierwszych iteracji\r\n\r\nall_chains = data.frame()\r\n\r\nfor (i in 1:chain){\r\n  \r\n  all_chains = rbind(all_chains,data.frame(results[,,i], chain = as.character(i), Iteration = (n.burn+1):n.iter))\r\n  \r\n  }\r\n\r\ncolnames(all_chains) = c(\"a\", \"b\", \"sigma\",\"chain\", \"Iteration\")\r\n\r\n\r\nMamy to! Sprawdźmy czy nasze łańcuchy się zbiegły.\r\n\r\n\r\nlibrary(gridExtra)\r\np1 <- ggplot(all_chains, mapping = aes(x = Iteration, y = a, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\np2 <- ggplot(all_chains, mapping = aes(x = Iteration, y = b, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\np3 <- ggplot(all_chains, mapping = aes(x = Iteration, y = sigma, color = chain))+\r\n  geom_line() +\r\n  theme_minimal()\r\ngrid.arrange(p1, p2,p3, nrow = 3)\r\n\r\n\r\n\r\nWizualna inspekcja wskazuje, że łańcuchy zbiegły sie wystarczająco (póżniej poznamy ilościowe wskaźniki). Zobaczmy rozkłady post piori.\r\n\r\n\r\np1 <- ggplot(all_chains, mapping = aes(x = a))+\r\n  geom_histogram(fill = \"white\",color=\"black\",) +\r\n  theme_minimal()\r\np2 <- ggplot(all_chains, mapping = aes(x = b))+\r\n  geom_histogram(fill = \"white\",color=\"black\",) +\r\n  theme_minimal()\r\np3 <- ggplot(all_chains, mapping = aes(x = sigma))+\r\n  geom_histogram(fill = \"white\",color=\"black\") +\r\n  theme_minimal()\r\ngrid.arrange(p1, p2, p3, nrow = 1)\r\n\r\n\r\n\r\nPoliczmy statystyki rozkładów.\r\n\r\n\r\nsummary(all_chains[,1:3])\r\n\r\n       a                b              sigma       \r\n Min.   :0.6338   Min.   :0.4456   Min.   : 7.213  \r\n 1st Qu.:4.2037   1st Qu.:0.8558   1st Qu.: 9.315  \r\n Median :4.8798   Median :0.9194   Median : 9.779  \r\n Mean   :4.8822   Mean   :0.9195   Mean   : 9.819  \r\n 3rd Qu.:5.5547   3rd Qu.:0.9830   3rd Qu.:10.274  \r\n Max.   :9.3682   Max.   :1.3589   Max.   :13.426  \r\n\r\nPorównajmy z klasyczną regresją.\r\n\r\n\r\nm = lm(y~x)\r\ndata.frame(a = m$coefficients[1], b = m$coefficients[2], sigma = sd(m$residuals), row.names = \"Parameters\")\r\n\r\n                  a         b    sigma\r\nParameters 4.919473 0.9192618 9.606116\r\n\r\nDiagnostyka\r\nGdy już mamy nasz model, musimy sprawdzić czy został wyestymowany poprawnie oraz w jakim zakresie jest w stanie odtworzyć zaobserwowane dane.\r\nGelman-Rubin Convergence Diagnostic\r\nStatystyka Gelmana-Rubina sprawdza ilościowo czy łańcuchy się zbiegły. Opiera się na stosunku wariancji między łańcuchami do wariancji wewnątrz łańcuchów (Gelman & Rubin, 1992).\r\n\\[\\begin{eqnarray*}\r\n\\bar{x}_j\r\n& = &\r\n\\frac{1}{N}\\sum_{t=1}^N x_t^{(j)}\\hspace{2em}\\text{(Średnia łańcucha)}\\\\\r\n\\bar{x}_\\cdot\r\n& = &\r\n\\frac{1}{J}\\sum_{j=1}^J \\bar{x}_j\\hspace{2em}\\text{(Średnia łańcuchów)}\\\\\r\nB\r\n& = &\r\n\\frac{N}{J-1}\r\n\\sum_{j=1}^J (\\bar{x}_j-\\bar{x}_\\cdot)^2\\hspace{2em}\\text{(Wariancja pomiędzy łańcuchami)}\\\\\r\ns^2_j\r\n& = &\r\n\\frac{1}{N-1}\r\n\\sum_{t=1}^N (x_t^{(j)}-\\bar{x}_j)^2\\hspace{2em}\\text{(Wariancja wewnątrz łańcucha)}\\\\\r\nW\r\n& = &\r\n\\frac{1}{J}\\sum_{j=1}^J s_j^2\\hspace{2em}\\text{(Średnia wariancja wewnątrz łańcuchów)}\r\n\\end{eqnarray*}\\]\r\nGdzie \\(N\\) to liczba iteracji, a \\(J\\) to liczba łańcuchów. Statystyka Gelmana Rubina jest dana wzorem:\r\n\\[R = \\frac{\r\n\\frac{N-1}{N}W + \\frac{1}{N}B\r\n}{W}\\]\r\nZwykle uznaje się, że łańcuchy się zbiegły, jeśli R < 1.15. Statystykę liczy się dla każdego parametru. W przypadku naszej regresji:\r\n\r\n\r\nBW = pivot_longer(all_chains,cols = 1:3, names_to = \"parameter\", values_to = \"Value\")\r\nBW = BW %>% group_by(parameter,chain) %>% summarise(Mean = mean(Value), Var = var(Value), n =n())\r\nW = BW %>% group_by(parameter) %>% summarise(W = mean(Var)/3) \r\nB = BW %>% group_by(parameter) %>% summarise(B = var(Mean)) \r\n\r\nR = (W$W*((n.iter - n.burn - 1)/(n.iter - n.burn)) + B$B)/W$W\r\nnames(R) = c(\"a\",\"b\",\"sigma\")\r\nkable(R,col.names = \"R\")\r\n\r\n\r\nR\r\na\r\n1.001088\r\nb\r\n1.000062\r\nsigma\r\n1.003041\r\n\r\nKażdy parametr spełnia kryterium Gelmana-Rubina.\r\nAutokorelacja\r\nJak wspominałem już wielokrotnie wcześniej, MCMC ma symulować losowanie z rozkładu a posteriori. Wyobraźmy sobie taką sytuację - losujemy 1000 osób, by estymować rozkład wzrostu w populacji. Obserwacje są od siebie niezależne, wylosowanie osoby A nie ma wpływu na prawdopodobieństwo wylosowania osoby B. W przypadku łańcuchów nie jest to prawdą. Zauważmy, że wylosowanie obserwacji B, zależy od tego jaką wartośc ma obserwacja A.\r\nW MCMC poruszamy się od obserwacji do obserwacji różniących się o jakąś małą losową wartość. W następstwie w łańcuchach występuje autokorelacja. Jeśli jest duża, może zbiasować nasz rozkład a posteriori, ponieważ pewne wartości będą nadreprezentowane, a efektywna liczba prób z rozkładu post priori będzie mniejsza niż N.\r\nAutokowariancja dla opóźnienia (lag) \\(t\\) jest zdefiniowana tak:\r\n\\[aCov(t) = \\frac{1}{N - t} \\sum_{n=1}^{N-t} (\\theta_n - \\mu_f)\\,(\\theta_{n+t}-\\mu_\\theta)\\]\r\ngdzie\r\n\\[\\mu_\\theta = \\frac{1}{N}\\sum_{n=1}^N \\theta_n\\]\r\na \\(\\theta_n\\) to wartość wylosowana przez łańcuch w iteracji \\(n\\).\r\nAutokorelacja dana jest więc:\r\n\\[aCor(t) = aCov(t)/aCov(0)\\]\r\nSprawdźmy autokorelację dla pierwszego łańcucha i parametru \\(\\beta\\).\r\n\r\n\r\nacf(results[,2,1],lag.max = 1000)\r\n\r\n\r\n\r\nWyglada to dobrze. Slina autokorleacja występuje tylko dla bardzo małego opóźnienia. Sprawdźmy teraz autokorelacje parametru \\(\\sigma\\).\r\n\r\n\r\nacf(results[,3,1],lag.max = 1000)\r\n\r\n\r\n\r\nW przypadku \\(\\sigma\\) wygląda to gorzej. Możecie się spotkać z tym, że niektórzy badacze by zmniejszyć autokorelacje, stosują odchudzanie (thinning), polegające na tym, do naszej próby rozkładu post priori bierzyemy co \\(n\\)-tą wartość z łańcucha.\r\n\r\n\r\nthinned_results = results[seq(1, n.iter - n.burn, by = 10),,]\r\nacf(thinned_results[,3,1],lag.max = 2000)\r\n\r\n\r\n\r\nJednakże, odchudzanie nie jest najlepszą strategią. Historycznie odchudzanie było stosowane z chęci zaoszczędzenia pamięci komputerów. Choć zmniejsza autokorelację, odchudzanie zmniejsza precyzję naszych wyników (Link & Eaton, 2012).\r\nZastanówmy się dlaczego. Chcielibyśmy by średnia wartości naszego łańcucha była jak najbliższa prawdziwej średniej rozkładu.\r\nJak pamiętamy, wariancja dla rozkładu średnich niezależnych prób losowych wynosi (zauważcie, że to inna \\(\\sigma\\) niż wcześniej):\r\n\\[\\sigma^2 = \\frac{1}{N}\\mathrm{Var}(\\theta)\\]\r\nJednak, jeżeli obserwacje są skorelowane, wariancja wynosi:\r\n\\[\\sigma^2 = \\frac{\\tau}{N}\\mathrm{Var}(\\theta)\\]\r\nDlaczego tak jest, to na razie pominiemy (wymagało by to dłuższego zatrzymania się nad tym problemem, niemniej być może do tego wrócimy). \\(\\tau\\) to zintegrowana funkcja autokorelacji (integrated autocorrelation function) dana wzorem:\r\n\\[ \\tau =1 + 2\\sum^N_{t= 1} \\mathrm{aCor(t)}\\] Wobec tego \\(\\frac{N}{\\tau}\\) to efektywna liczba próbek (ESS), a \\(\\tau\\) to liczba iteracji jakie musi minąć łańcuch zanim “zapomni” gdzie sie zaczął.\r\nPopatrzmy na wykres autokorelacji dla \\(\\beta\\). Do pewnego \\(t\\) autokorelacja jest malejącą funkcją, po przekroczeniu \\(t\\) zaczyna oscylować wokół 0. Ponieważ autokorelacje mogą być tylko pozytywne gdy \\(N\\) dąży do nieskonczoności, obserwowane empirycznie negatywne korelacje na pewno wynikają z szumu (Geyer, 1992). Sumowanie ich by otrzymać zmniejszy nam estymatę \\(\\tau\\). Rownież te małe dodatnie korelacje są efektem szumu.\r\nZwróćmy uwagę na wzór na autokowariancję. Im większy lag \\(t\\) tym mniejsza liczebność próbki (wynosi \\(N-t\\)), z której liczymy autokowariancę. W związku z tym im większe \\(t\\) tym bardziej zaszumione estymaty autokowariancji otrzymujemy. Jednym ze sposóbw na obejście tego problemu jest estymowanie \\(\\tau\\) w następujący sposób:\r\n\\[ \\tau =1 + 2\\sum^M_{t= 1} \\mathrm{aCor(t)}\\] Gdzie \\(M\\) jest ostatnią największą wartością \\(t\\) dla której wszystkie \\(t < M\\) mają dodatnią autokowariancję.\r\nPoliczmy efektywną liczbę próbek dla parametru \\(\\sigma\\) i łańcucha 1.\r\n\r\n\r\nESS = function(data,parameter,chain){\r\n  \r\n  tmp = data[,parameter,chain]\r\n  ACF = acf(tmp,plot = F,lag.max = 10000, type = \"covariance\")$acf/var(tmp)\r\n  tau =  2*sum(ACF[1:(which(ACF<0)[1]-1)]) - 1\r\n  length(tmp)/tau\r\n}\r\n\r\nESS(results,3,1)\r\n\r\n[1] 1174.496\r\n\r\nTeraz policzmy ESS dla odchudzonego łańcucha.\r\n\r\n\r\nESS(thinned_results,3,1)\r\n\r\n[1] 1166.745\r\n\r\nEES zmniejszyło się względem nieodchudzonego łańcucha. Generalnie, znacznie lepszą metodą redukowania wpływu autokorelacji, jest użycie większej liczby dłuższych łańcuchów. Odchudzanie należy stosować tylko jeśli mierzymy się z bardzo dużą autokorelacją. Reguła kciuka została zaproponowana w tej książce (Christensen et al., 2010) - odchudzać jeśli duża autokorelacja występuje powyżej lag > 30. W naszym przypadku było to więc niejako uzasadnione. Nie należy jednak stosować odchudzania jako rutyny w MCMC.\r\nPosterior predictive check\r\nWreszcie, gdy upewnimy się, że losoujemy z właściwej przestrzeni parametrów, i że nasze próbki przynajmnie udają, że są od siebie niezależne, czas na sprawdzenie czy nasz model właściwie modeluje to, co powinien modelować (model fit). Posterior predictive check polega na losowaniu z posterior predictive distribution, który jest rozkładem prawdopodbieństwa uzyskania nowych danych, pod warunkiem zebranych przez nas danych:\r\n\\[P(y^*|y)=\\int P(y^*|\\theta)P(\\theta|y)d\\theta\\] gdzie \\(y^*\\), to nowe dane. W przypadku MCMC estymacja rozkładu \\(P(y^*|y)\\) składa się z dwóch kroków: Wylosować wartość \\(\\theta\\) z rozkładu post priori. Wstawić wylosowane \\(\\theta\\) do \\(P(y^*|\\theta)\\) i wylosować \\(y^*\\).\r\nJak wygląda \\(P(y^*|\\theta)\\)? Podobnie jak nasza funkcja wiarygodności, z tą różnicą, że tym razem nie obliczamy wiarygodności uzyskania \\(n\\) próbek z zadanego rozkadu, lecz po po prostu losujemy z owego rozkładu.\r\nMy sprawdzimy nasze dopasowanie modelu poprzez analizę wizualną. Wyplotujmy sobie histogram z naszych danych obok rozkładu generowanego przez model.\r\n\r\n\r\npredicted <- NULL\r\n\r\nfor (j in 1:length(x)){\r\nfor (i in 1:chain){\r\npredicted <- c(predicted,rnorm(dim(results)[1], results[dim(results)[1],1,i] +\r\nresults[dim(results)[1],2,i]*x[j], results[dim(results)[1],3,i]))\r\n}}\r\n\r\npredicted = as.data.frame(predicted)\r\ny = as.data.frame(y)\r\nggplot() + \r\n  geom_histogram(data = y,mapping = aes(x = y,y = ..density..),\r\n                 colour = 1, fill = \"white\") +\r\n  geom_density(data = predicted, aes(x =predicted, y = ..density..)) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nWszystko wydaje się być w porządku. Problemy z posterior predictive check może świadczyć o tym, że wybraliśmy złą funkcją wiarygodności lub/i rozkłady a priori.\r\nPodsumowanie\r\nUff, przebrneliśmy przez estymację bayesowską. Na szczęście od teraz będziemy stosować pakiety, które zrobią to za nas. Co ważne, zaprezentowany przeze mnie dziś kod raczej nie nadaje się do prawdziwej analizy. Dedykowane oprogramowanie przeprowadza losowanie MCMC w sposób bardziej optymalny (i skomplikowany). Warto jednak poznać na czym polega metoda estymacji, której często będziemy używać.\r\n\r\n\r\n\r\nChristensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). Bayesian ideas and data analysis: An introduction for scientists and statisticians (p. 146). CRC press.\r\n\r\n\r\nGelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science, 457–472.\r\n\r\n\r\nGeyer, C. J. (1992). Practical markov chain monte carlo. Statistical Science, 473–483.\r\n\r\n\r\nLink, W. A., & Eaton, M. J. (2012). On thinning of chains in MCMC. Methods in Ecology and Evolution, 3(1), 112–115.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-30-tutorial-bayes-ii/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-12-26T21:37:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-23-tutorial-bayes-iv/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Część IV: Wprowadzenie do modeli wielopoziomowych",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-10-30",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nMotywacja\r\nNiewykrycie prawdziego efektu\r\nWykrycie nieprawdziwego efektu\r\nZ czym się mierzymy?\r\nCzym jest więc model mieszany?\r\n\r\nModele wielopoziomowe\r\nModel z efektem klastra\r\nImplementacja\r\nNiezależność warunkowa\r\n\r\nBayesowskie dzielenie informacji\r\nModelujmy dalej\r\nModelowanie interceptu i wpspółczynnika regresji dla klastra\r\nPredyktory na wyższych poziomach\r\n\r\nPodsumowanie\r\n\r\n\r\n\r\n\r\nPost będzie aktualizowany.\r\n\r\nDzisiaj zajmiemy się bayesowskimi wielopopoziomowymi (mieszanymi/hierarchicznymi) modelami liniowymi. Na ten temat poświęca się całe książki, (choć z drugiej strony czasami mam wrażenie, że traktuje się ten temat jak czarną magię). Zobaczmy o co całe zmieszanie.\r\nMotywacja\r\nZamiast zaczynać od wprowadzenia definicji mieszanego modelu liniowego, przyjrzyjmy się poniższym sytuacjom.\r\nNiewykrycie prawdziego efektu\r\nZałóżmy, że wykonujemy przez pewien czas pomiary zadowolenia z obsługi i wysokości napiwków u stałych klientów pewnej restauracji.\r\n\r\n\r\n\r\nWidzimy tu brak zależności liniowej. Teraz zróbmy tak by na wykresie kolor kropki odpowiadał konkretnemu klientowi.\r\n\r\n\r\n\r\nWidzimy, że u każdego z klientów występuje związek liniowy pomiędzy napiwkiem i zadowoleniem. Jednak ponieważ, nasze pomiary zagnieżdone są w klientach, co objawia się tym, że każdy klient średnio daje inną wartość napiwku, przeoczylibyśmy ten związek, nie uwzględniając tego w modelu.\r\nWykrycie nieprawdziwego efektu\r\nCzy może nam się zdarzyć sytuacja odwrotna? Używając prostej analizy liniowej wykryjemy efekt, którego nie ma? Załóżmy, że chcemy ocenić wpływ picia napojów kofeinowych na wyniki ucznia w nauce. Zebraliśmy dane z trzech klas liceum pytając o średnie spożycie napojów kofeinowych dziennie i średnią ocen.\r\n\r\n\r\n\r\nZaznaczmy na wykresie kolorami uczniów przynależących do danej klasy.\r\n\r\n\r\n\r\nJak widzimy, w poszególnych klasach spożycie kofeiny nie ma wpływu na wyniki ucznia w nauce. Kiedy możemy zaobserwować taką zależność? Załóżmy na przykład, że każda z klas ma innego wychowawcę i ich wpływ przedkłada się na średnią ocen. Załóżmy też, że zupełnie przypadkowo w klasach z lepszym wychowawcą, średnie spożycie napojów kofeinowych jest większe. Otrzymamy wtedy sytuację jak na powyższych wykresach.\r\nOczywiście, może istnieć jakaś zależność pomiędzy średnim spożyciem kofeiny na klasę a jej średnią ocen lecz to nie odpowiada na pytanie, które zadaliśmy tzn. o wpływ kofeiny na osiągnięcia ucznia. Tutaj warto wprowadzić sobie rozróżnienie między efektem węwnątrzobiektowym i międzyobiektowym. Zrobiłem to w jednym z poprzednich wpisów, można przeczytać tutaj.\r\nZ czym się mierzymy?\r\nProblem możemy zdefiniować na kika sposobów. W pierwszej sytuacji chcemy zbadać związek pomiędzy napwkami a zadowoleniem z obsługi. Na przeszkodzie jednak staje nam efekt kienta - każdy klient ma inny bazowy średni napiwek. Klient jest tu zmienną zakłócającą. By poprawnie wyestymować wpływ zadowolenia na napiwki, moglibyśmy kontrolować wpływ poszczególnego klienta. W istocie, na tym z grubsza polega mechanizm działania mieszanych modeli liniowych.\r\nPrzyjrzyjmy się teraz problemowi z innej perspektywy. Gdy chcemy zbadać wpływ zmiennej \\(X\\) na Y, na przykład licząc współczynnik korelacji Pearsona, musimy wykonać następujące kroki (książkowo): Losujemy obserwacje do naszej próby. Następnie, ponieważ każda obserwacja miała takie samo prawdopobieństwo dostania się do próby, traktujemy te obserwacje jako niezależne zmienne losowe o identycznym rozkładzie (iid, independent and identically distributed random variables). Na chłopski rozum, oznacza to, że jeśli stworzymy sobie histogram zmiennej X, otrzymamy z grubsza estymatę rozkładu zmiennej \\(X\\) w populacji. W przypadku powyższych danych, nie jest to prawda. Przyjrzyjmy sie dokładniej tej definicji.\r\nDefinicja składa się z dwóch zasadniczych elementów. Niezależność oznacza, że obserwacje są niepowiązane (wylosowanie \\(x_1\\) nie mówi nam nic o prawdopobieństwie wylosowania \\(x_2\\)), co często nie jest spełnione w danych pogrupowanych w klastry, ponieważ obserwacje wewnątrz klastra są do siebie bardziej podobne. Innymi słowy obserwacje w klastrze są ze sobą skorelowane.\r\nDrugą częścią definicji jest fakt, że obserwacje zostały wylosowane z tego samego rozkładu. Przykładowo, jeśli wykonujemy pojedynczy pomiar zadowolenia u różnych osób odwiedzających restaurację, to możemy powiedzieć, że wszystkie te obserwacje zostały wylosowane z tego samego rozkładu zadowolenia (by być dokładnym, z tego samego rozkładu w danym klastrze, czyli w tej samej restauracji). Jednakże, w praktyce wielokrotnie wykonujemy pomiary zadowolenia u tych samych osób, co oznacza, że zmienna “zadowolenie” ma inny rozkład u klienta A, a inny u klienta B.\r\nBy więc poprawnie przeanalizować takie dane, musimy zastosować model mieszany.\r\nCzym jest więc model mieszany?\r\nWikipedia opisuje go tak: Mieszany model liniowy to model statystyczny, który uwzględnia zarówno efekty stałe (fixed effects), jak i losowe (random effects) w celu analizy zgrupowanych (clustered) danych.\r\nJest to definicja częstościowa, niemniej jednak pewnie każdy, kto miał styczność z modelami mieszanymi, słyszał o efektach stałych i losowych. Bez wchodzenia zbytnio w tę terminologię, efekty stałe są traktowane jako efekty, które mają określoną wartość, którą staramy się oszacować, na przykład wpływ zadowolenia na wysokość napiwku. Efekt losowy to efekt, który jest opisany jakimś rozkładem statystycznym, a my napotykamy na jakąś jego realizację, na przykład hojność danego klienta. Jednak gdy przeprowadzimy kolejne badanie na innych klientach, wciąż estymujemy ten sam związek między zadowoleniem a wysokością napiwku, ale nasi klienci będą mieli inną bazową wartość hojności.\r\nMy nie będziemy korzystać z tej terminologii. Według mnie czasami wporowadza ona zamieszanie w rozumieniu tej klasy modeli. Na szczęście mamy na to bardzo dobre uzasadnienie. Przypomnijcie sobie, że w statystyce Bayesowskiej wszystkie parametry traktujemy jako rozkłady statytyczne. Więc rozróżnienie na efekty stałe i losowe nie ma większego sensu, ponieważ w modelach Bayesowskich wszystkie efekty są losowe.\r\nModele wielopoziomowe\r\nModel z efektem klastra\r\nJak więc będzie wyglądał bayesowski model, którym poprawnie możemy przeanalizować dane z powyższych przykładów?\r\nW poprzednich częściach estymowaliśmy regresję liniową. Miała ona miała postać:\r\n\\[y_{i} \\sim N(\\mu = \\alpha + \\beta * x_i, \\sigma)\\]\r\nA wszystkie paramtery w tym równaniu miały rozkłady a priori ze zdefiniowanymymi przez nas hiperparametrami (czyli takimi parametrami, które są zdefiniowane arbitralnie, a nie estymowane z danych).\r\nTeraz musimy uwzględnić w modelu różnice pomiędzy obserwacjami wynikajace z przynależności do klastra:\r\n\\[y_{i} \\sim N(\\mu = \\beta * x_i + k_j , \\sigma)\\]\r\n\\[k_{j} \\sim N(\\alpha, \\tau)\\]\r\n\\[\\alpha \\sim U(-10^{3}, 10^{3})\\]\r\n\\[ \\tau \\sim N(10^{-6}, 10^{3}) \\]\r\n\\[ \\beta \\sim N(0, 10^{2}) \\]\r\n\\[ \\sigma \\sim N(10^{-6}, 10^{3}) \\]\r\nŻeby dokładnie zrozumieć co się dzieje, spójrzmy na diagram naszego nowego modelu.\r\n\r\nŻeby zrozumieć mechanizm tego modelu, warto zwizualizować sobie jak jest estymowany (mi to osobiscie pomagało zrozumieć jak działają bayesowskie modele). Przy użyciu MCMC, najpierw losujemy \\(\\alpha\\) i \\(\\tau\\) z ich rozkładów a priori, a następnie te wartości są używane do wylosowania wartości \\(k_j\\) dla każdego klastra, korzystając z rozkładu normalnego. Na koniec, mając wylosowane także wartości \\(\\beta\\) i \\(\\sigma\\), sprawdzamy, jak prawdopodobne jest, że model o takich parametrach wyprodukował zaobserwowane dane \\(y\\).\r\nParametr \\(k_j\\), który występuje w funckcji wiarygodności ma rozkład a priori opisany rozkładem normalnym o średniej \\(\\alpha\\) i wariancji \\(\\tau\\), który także jest estymowany z danych, a jego parametry mają własne rozkłady a priori. Reszta modelu pozostaje taka sama jak w przypadku standardowej regresji. Zamiast pojedynczego interceptu, model ma teraz \\(j\\) interceptów, gdzie \\(j\\) to liczba klastrów.\r\nRozkład a priori \\(P(k_j|\\alpha,\\tau)\\) określa prawdopodobieństwo wystąpienia intercepta o danej wartości, gdzie \\(\\alpha\\) oznacza średni intercept dla całego zbioru danych, a \\(\\tau\\) to wariancja interceptów.\r\nImplementacja\r\nPosłużymy się danymi (do pobrania tutaj) z drugiego przykładu i zaimplementujemy model użyciu JAGS.\r\n\r\n\r\nlibrary(rjags)\r\nlibrary(coda)\r\n\r\ncolnames(d) = c(\"Zadowolenie\", \"Klient\", \"Napiwek\")\r\n\r\nmod = \"model {\r\n  # Priors\r\n  a ~ dunif(-1000, 1000)\r\n  b ~ dnorm(0, 100^-2)\r\n  sigma ~ dunif(0.000001,1000)\r\n  tau ~ dunif(0.000001,1000)\r\n  \r\n  for (i in 1:5){\r\n  k[i] ~ dnorm(a, tau^-2)\r\n  }\r\n  \r\n  # Likelihood\r\n  for (i in 1:length(Napiwek)) {\r\n    mu[i] <- b * Zadowolenie[i] + k[Klient[i]]\r\n    Napiwek[i] ~ dnorm(mu[i], sigma^-2)\r\n  }}\"\r\n\r\nparams = c(\"a\",\"b\",\"k\",\"tau\",\"sigma\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 6000\r\nnt = 1\r\nnc = 6\r\njmod = jags.model(file = textConnection(mod), data = d, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(jmod, n.iter=ni, by=1)\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:9100\r\nThinning interval = 1 \r\nNumber of chains = 6 \r\nSample size per chain = 6000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean       SD  Naive SE Time-series SE\r\na     16.5100  7.87697 0.0415153      0.0765400\r\nb      0.2777  0.02614 0.0001378      0.0003779\r\nk[1]   6.2376  0.25082 0.0013220      0.0028611\r\nk[2]  11.4953  0.23383 0.0012324      0.0024677\r\nk[3]  16.2927  0.24670 0.0013002      0.0026946\r\nk[4]  21.6225  0.22811 0.0012023      0.0023383\r\nk[5]  26.5587  0.23112 0.0012181      0.0024175\r\nsigma  0.8541  0.06148 0.0003240      0.0003498\r\ntau   13.1714 11.67397 0.0615272      0.2492754\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%     50%     75%   97.5%\r\na      2.3227 13.2364 16.4631 19.6847 30.4752\r\nb      0.2266  0.2602  0.2776  0.2951  0.3298\r\nk[1]   5.7422  6.0706  6.2380  6.4056  6.7278\r\nk[2]  11.0347 11.3398 11.4964 11.6500 11.9617\r\nk[3]  15.8088 16.1289 16.2932 16.4570 16.7785\r\nk[4]  21.1711 21.4704 21.6255 21.7746 22.0685\r\nk[5]  26.1064 26.4034 26.5576 26.7139 27.0142\r\nsigma  0.7443  0.8110  0.8502  0.8936  0.9839\r\ntau    5.2418  8.0037 10.5506 14.7879 35.9838\r\n\r\nJeśli spojrzymy na kwantyle parametru \\(\\beta\\) zauważymy, że 0 znajduje się poniżej 2.5% percentyla. Innymi słowy, wykryliśmy efekt pomiędzy napiwkami a zadowoleniem z obsługi.\r\nZauważmy też, że model moglibysmy zaimplementować w następującej formie:\r\n\\[y_{i} \\sim N(\\mu =\\alpha +  \\beta * x_i + k_j , \\sigma)\\]\r\n\\[k_{j} \\sim N(0, \\tau)\\]\r\nOtrzymalibyśmy dokłokładnie to samo z tą różnicą, że teraz nie \\(k_j\\) nie byłoby interceptem dla klastra, a wartością o jaką różni się intercept dla klastra od \\(\\alpha\\) (czyli, \\(\\alpha\\) + \\(k_j\\) byłby intercepem dla klastra \\(j\\).)\r\nWspomniałem wyżej, że modele mieszane zasadniczo można interpretować w kategorii kontrolowania zmiennych zakłócających. Policzmy zwykłą regresję (częstościową, ale dotyczy to też bayesowskiego odpowiednika), w której uwzględnimy predyktory kategorialne wskazujące przynależność do klastra.\r\n\r\n\r\nlibrary(broom)\r\ntidy(lm(Napiwek ~ Zadowolenie + Klient, d))\r\n\r\n# A tibble: 6 × 5\r\n  term        estimate std.error statistic  p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)    6.23     0.246       25.3 4.42e-45\r\n2 Zadowolenie    0.278    0.0256      10.8 1.67e-18\r\n3 Klient2        5.26     0.261       20.1 9.36e-37\r\n4 Klient3       10.1      0.260       38.6 1.61e-61\r\n5 Klient4       15.4      0.263       58.6 1.23e-78\r\n6 Klient5       20.3      0.262       77.6 1.84e-90\r\n\r\nOtrzymaliśmy prawie taki sam współczynnik regresji! Różny sposób ujęcia problemu prowadzi do tego samego rozwiązania.\r\nOt, cała tajemnica modeli mieszanych. W identyczny sposób możemy rozszerzyć model, by uwzględniał różnice we współczynniku regresji pomiędzy klastrami. Bądź zmodyfikować go tak by uwzględniał więcej poziomów i.e. klastry wenątrz klastrów. I taki przykład sobie zrobimy gdy będziemy omawiać case studies, ale najpierw opowiedzmy sobie o właściwościach modeli wielopoziomowych.\r\nNiezależność warunkowa\r\nŁączny rozkład prawdopodobieństwa post priori naszych parametrów to:\r\n\\[P(\\beta,k_j,\\sigma,\\alpha,\\tau|y) \\propto P(y|\\beta,k_j,\\sigma) P(k_j|\\alpha,\\tau) P(\\beta)P(\\alpha)P(\\tau)P(\\sigma)\\]\r\nTo na co warto zwrócić uwagę, to relacje niezależności.\r\n\r\n\r\nWarunkowa niezależność\r\n\r\nNiech \\(A\\), \\(B\\) i \\(C\\) będą zdarzeniami. \\(A\\) i \\(B\\) są warunkowo niezależne jeśli:\r\n\\[P(A|B,C)= P(A|C) \\]\r\nEkwiwalentnie\r\n\\[P(A,B|C) = P(A|C)P(B|C)\\]\r\n\r\n\r\nCzyli, jak widzimy na diagramie, bezpośrednio zależne są ze sobą tylko elementy połączone strzałkami. Na przykład \\(P(y|\\beta,k_j,\\sigma)\\) nie zależy bezpośrednio od \\(P(\\alpha)\\) ponieważ:\r\n\\[P(y|\\beta,k_j,\\sigma,\\alpha) = P(y|\\beta,k_j,\\sigma)\\]\r\nInnymi słowy, cała informacja jaką \\(\\alpha\\) ma o prawdopodobieństwie \\(y\\), jest już zawarta w \\(k_j\\).\r\nMożemy zadać sobie pytanie, skoro ta klasa modeli nazywana jest (też) modelami wielopoziomowymi, to gdzie te poziomy. Jeśli mamy funkcję wiargodności i rozkłady a priori dla parametrów, które zarządzane sa przez wybrane przez badacza wartości hiperparametrów, to modelujemy dane na 1 poziomie. Jeśli jednak parametry rozkładów a priori same mają rozkłady a priori wtedy modelujemy dane na 2 (lub więcej) poziomach.\r\nBayesowskie dzielenie informacji\r\nRozważmy teraz następujący problem: zmierzyliśmy zmienną \\(X\\) w różnych podgrupach, a jedyne, co nas interesuje, to średnia wartość zmiennej \\(X\\) w danej podgrupie. Jednak podgrupy różnią się liczebnością. Mamy trzy możliwości postępowania:\r\nPoliczyć średnią dla każdej grupy, ignorując pozostałe grupy w tym procesie oraz fakt, że dokładność średnich będzie mniejsza dla mniej licznych grup (no pooling).\r\nPoliczyć średnią zmiennej \\(X\\) , uznając, że lepiej jest skorzystać z najpewniejszej estymaty i zignorować podział na grupy (full pooling).\r\nUżyć modelu mieszanego, który pozwoli nam wykorzystać wszystkie dostępne dane, poprzez wykorzystanie informacji z całej naszej próby (partial pooling, borrowing information).\r\nW jaki sposób model mieszany pozwoli nam na to? Dla uproszczenia zdefiniujmy model, w którym interesuje nas prawdopodobieństwo otrzymania średniej \\(y_{ij}\\) w próbie dla danej podgrupy \\(j\\). Jako funkcję wiarygodności wybierzmy rozkład normalny:\r\n\\[P(y|\\theta_j, \\sigma_j) \\sim N(\\theta_j , \\sigma_j)\\]\r\nParametry tego rozkładu to prawdziwa średnia w grupie \\(\\theta_j\\) oraz odchylenie standardowe w grupie \\(\\sigma_j\\).\r\nJednocześnie, rozkład prawdziwych średnich w grupach zamodelujmy następująco:\r\n\\[\\theta_{j} \\sim N(\\mu, \\tau)\\]\r\ngdzie \\(\\mu\\) to ogólna średnia zmiennej \\(X\\) , a \\(\\tau\\) to odchylenie standardowe rozkładu ogólnej średniej.\r\nDobrze, to co nas naprawdę interesuje, to policzenie najlepszej estymaty prawdziwej średniej w grupie \\(\\theta_j\\). Do tego potrzebujemy rozkładu \\(P(\\theta_j|\\bar{x_{i}})\\). Aby obliczyć szukany przez nas rozkład, możemy zastosować twierdzenie Bayesa:\r\n\\[P(\\theta_j|y) \\propto P(y|\\theta_j)P(\\theta_j)\\]\r\n\r\nRozkład Normalny\r\n\r\nRozkład normalny dany jest wzorem:\r\n\\[x \\sim N(\\mu,\\sigma); \\; \\; P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\r\n  \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\,\\right)\\]\r\n\r\nGdzie \\(\\mu\\) i \\(\\sigma\\) to kolejno średnia i wariancja \\(x\\). Ponieważ w statystyce bayesowskiej często wystarczą nam relacje proporcjonalności możemy usnąć wszystko, co tylko skaluje nasz rozkład:\r\n\\[P(x) \\propto\r\n  \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\,\\right)\\]\r\n\r\n\r\nFunkcja wiarygodności naszych danych to iloczyn prawdopdobieństwa otrzymania każdej obserwacji:\r\n\\[P(y|\\theta_j) = \\prod_{i=1}^{n} \\exp\\left( -\\frac{(y_{ij}-\\theta_j)^2}{\\sigma^2}\\,\\right) = \\exp\\left( -\\frac{\\sum_{i=1}^{n}(y_{ij}-\\theta_j)^2}{2\\sigma^2}\\,\\right)\\]\r\nA więc nasz rozkład post priori będzie proporcjonalny do:\r\n\\[P(\\theta_i|y) \\propto  \\exp\\left(-\\frac{\\sum_{i=1}^{n}(y_{ij} - \\theta_j)^2}{2\\sigma_j^2}\\right)\\exp\\left(-\\frac{(\\theta_j-\\mu)^2}{2\\tau^2}\\right)\\]\r\nGdy po wykonaniu kilku przekształceń otrzymamy:\r\n\\[P(\\theta_i|y) \\propto \\exp\\left(-\\frac{(\\theta_j - \\hat{\\theta_j})^2}{2S_i^2}\\right)\\]\r\nRozkład normalny jest rozkładem zgodym (conjugate prior) dla samego siebie. A więc rozkład a posteriori jest rozkładem normalnym o średniej \\(\\hat{\\theta_i}\\) i wariancji \\(S^2\\).\r\nGdzie\r\n\\[\\hat{\\theta_j} = \\frac{\\tau^2 \\bar{x_j} + \\frac{\\sigma_j^2}{n}\\mu}{\\tau^2 + \\frac{\\sigma_j^2}{n_j}} = \\left(\\frac{\\tau^2}{\\tau^2 + \\frac{\\sigma_j^2}{n_j}}\\right)\\bar{x_j} + \\left(\\frac{\\frac{\\sigma_j^2}{n_j}}{\\tau^2 + \\frac{\\sigma_j^2}{n}}\\right)\\mu\\]\r\n\\[\\bar{x_j} = \\sum_{i=1}^{n_j}\\frac{y_{ij}}{n_j}\\]\r\n\\[\\frac{1}{S_j^2} = \\frac{1}{\\tau^2} + \\frac{1}{\\frac{\\sigma_j^2}{n}}\\]\r\nCzyli nasz rozkład posteriori \\(\\theta_j\\) ma rozkład o średniej \\(\\hat{\\theta}_{j}\\) i wariancji \\(S^2\\). Zwykle, jeśli chcemy otrzymać punktową estymatę parametru z rozkładu posteriori, bierzemy jego średnią (w przypadku rozkładu normalnego jego średnia ma najwyższą wartość gęstości prawdopodobieństwa). W tym przypadku będzie to \\(\\hat{\\theta_i}\\). Możemy zauważyć, że wzór na \\(\\hat{\\theta_i}\\), który otrzymaliśmy, to średnia ważona średniej empirycznej w próbie w danej grupie \\(\\bar{x_j}\\) i ogólnej średniej w próbie \\(\\mu\\). Możemy zapisać to następująco:\r\n\\[\\hat{\\theta}_j = \\lambda_j \\bar{x_j} + (1-\\lambda_j)\\mu\\]\r\ngdzie:\r\n\\[\\lambda_j = \\frac{\\tau^2}{(\\tau^2 + \\frac{\\sigma_j^2}{n})}\\]\r\nWidzimy, że najlepsza estymata średniej w grupie będzie się różniła od średniej w próbie dla grupy \\(j\\) w zależności od tego, jak bardzo \\(\\lambda_i\\) będzie mniejsze od 1. Będzie się tak działo, gdy \\(\\frac{\\sigma_j^2}{n_j}\\) będzie duży. A kiedy będzie duży?\r\nIm mniejsza wariancja w danej grupie i im większa liczebność grupy, tym bardziej estymata średniej będzie się zbliżała do średniej empirycznej w grupie. Natomiast, jeśli wariancja w grupie jest duża i/lub grupa ma małą liczebność, estymata \\(\\hat{\\theta}_i\\) będzie bardziej przesunięta w stronę \\(\\mu\\), ponieważ prawdopodobieństwo, że średnia w grupie \\(j\\) znacznie odbiega od średniej ogólnej, jest mniejsze (przypomnijmy sobie przykład z księgarnią z pierwszej części tutorialu).\r\n\\(\\lambda_i\\) nazywamy współcznynnikiem ściągania (shrinkage coefficient). Choć jego formuła nie będzie zawsze wyglądała tak elegancko, jak w tym przypdaku (gdzie rozkład a priori i funkcja wiarygodności dane są rozkładami normalnymi), widzimy, że model mieszany pozwala nam na estymację średnich w grupach z większą pewnością, ponieważ wykorzystujemy do tego obserwacje ze wszystkich podgrup!\r\nTa własność pozwala nam nie tylko na uzyskanie większej pewności naszych estymat, ale także na nie martwienie się problemem wielokrotnych porównań. Model w naturalny sposób przybliża średnie w grupach do średniej ogólnej gdy wariancja grup \\(\\tau^2\\) zbliża sie do 0. Nie będziemy omawiać tego dokładniej, ale Andrew Gelman opisał to w tym artykule (Gelman et al., 2012), który wam polecam.\r\nModelujmy dalej\r\nZobaczmy teraz nieco bardziej skomplikowane przykłady modelu wielopoziomowego.\r\nRadon jest radioaktywnym gazem szlachetnym, drugą po paleniu tytoniu, przyczyną raka płuc. Zanieczyszcza on wnętrza budynków przedostając się do środka przez małe szczeliny i otwory, stwarzając zagrożenie dla ludzi.\r\nMamy dane z 919 domów z 85 hrabstw, pomiar radonu w mieszkaniach oraz infromacje czy dom ma piwnicę (0) czy nie (1). Posiadamy także pomiar średniego stężenia uranu w glebie hrabstwa (Dane możecie je pobrać tutaj). Zobaczmy je.\r\n\r\n\r\ndata = read.csv(\"Radon_Data.csv\")\r\n\r\nhead(data)\r\n\r\n  county floor     radon    uranium\r\n1      1     1 0.7884574 -0.6890476\r\n2      1     0 0.7884574 -0.6890476\r\n3      1     0 1.0647107 -0.6890476\r\n4      1     0 0.0000000 -0.6890476\r\n5      2     0 1.1314021 -0.8473129\r\n6      2     0 0.9162907 -0.8473129\r\n\r\nModelowanie interceptu i wpspółczynnika regresji dla klastra\r\nRadon uwalnia się z gleby, więc chcemy dowiedzieć się, jak posiadanie piwnicy wpływa na stężenie radonu. Nasz model będzie miał postać:\r\n\\[y_i = N(a_j + b_jx_i,\\sigma)\\]\r\nIndeks \\(i\\) oznacza numer obserwacji, a indeks \\(j\\) - numer hrabstwa. W naszym modelu uwzględniamy zarówno wyraz wolny (intercept), jak i współczynnik regresji dla danego hrabstwa. W ten sposób modelujemy korelacje pomiędzy obserwacjami w hrabstwach, które wynikają nie tylko z bazowego poziomu stężenia radonu w hrabstwie, ale także z różnic w poziomie stężenia radonu wynikających z posiadania piwnicy.\r\nWarto zwrócić uwagę na fakt, że teraz każde hrabstwo ma swój własny intercept \\(a_j\\) i współczynnik regresji \\(b_j\\). Prawdopodobnie istnieje pewna zależność między tymi parametrami. Na przykład w hrabstwach o wysokim bazowym poziomie radonu (\\(a_j\\)) posiadanie piwnicy może mieć niewielki wpływ na poziom stężenia (\\(b_j\\)), a w hrabstwach o niskim bazowym poziomie radonu posiadanie piwnicy może znacząco wpłynąć na poziom stężenia. Taka zależność również wpływa na korelacje obserwacji w grupach (hrabstwach), dlatego należy ją uwzględnić w modelu.\r\nJak to zrobić? Aby to zrozumieć, przyjrzyjmy się wielowymiarowemu rozkładowi normalnemu. Podobnie jak opisujemy rozkład zmiennej \\(x\\) za pomocą rozkładu normalnego, tak możemy opisać łączny rozkład zmiennych \\(x\\) i \\(y\\) za pomocą wielowymiarowego rozkładu normalnego. Wielowymiarowy rozkład normalny jest opisany dwoma parametrami: wektorem średnich zmiennych \\(\\mu\\) oraz macierzą kowariancji \\(\\boldsymbol{\\Sigma}\\), która opisuje związki między zmiennymi. Jeśli dwie zmienne są niezależne, to odpowiednie elementy macierzy kowariancji są równe zero.”\r\n\r\nWielowymiarowy Rozkład Normalny\r\n\r\nWielowymiarowy rozkład normalny opisuje sposób, w jaki zachowuje się \\(m\\) zmiennych losowych, podobnie jak jednowymiarowy rozkład normalny opisuje sposób, w jaki zachowuje się jedna zmienna losowa. Jest on opisany przez wektor średnich \\(\\boldsymbol{\\mu}\\) oraz macierz kowariancji \\(\\boldsymbol{\\Sigma}\\). Wielowymiarowy rozkład normalny oznaczamy symbolem \\(\\mathcal{N}_m(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), gdzie \\(m\\) określa wymiarowość przestrzeni, w której zmienne losowe mają swoje wartości.\r\n\r\nFunkcja gęstości prawdopodobieństwa dla \\(m\\)-wymiarowego rozkładu normalnego ma postać:\r\n\\[f(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{\\frac{m}{2}}\\sqrt{|\\boldsymbol{\\Sigma}|}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right)\\]\r\n\r\ngdzie \\(\\boldsymbol{x}\\) jest \\(m\\)-elementowym wektorem zmiennych losowych. Symbol \\(|\\boldsymbol{\\Sigma}|\\) oznacza wartość wyznacznika macierzy \\(\\boldsymbol{\\Sigma}\\).\r\n\r\n\r\nA więc rokład a priori dla \\(a_j\\), \\(b_j\\) będzie wyglądał tak:\r\n\\[ \\begin{aligned}\r\n\\begin{pmatrix}a_j \\\\ b_j \\end{pmatrix} &\\sim \\mathcal{N_m}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\\\\r\n\\end{aligned}\\]\r\nGdzie\r\n\\[\\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\alpha} \\\\ \\boldsymbol{\\beta} \\end{pmatrix}, \\qquad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathrm{Var}(a_j) & \\mathrm{Cov}(a_j,b_j) \\\\ \\mathrm{Cov}(a_j,b_j) & \\mathrm{Var}(b_j) \\end{pmatrix}\\]\r\nOczywiście wszystkie parametry z powyższego wzoru mają swoje rozkłady a priori (możecie zobaczyć je w kodzie poniżej).\r\nNasz model w JAGS będzie wyglądał następująco:\r\n\r\n\r\nmodel =  \"model {\r\n#Likehood\r\nfor (i in 1:length(radon)){\r\nradon[i] ~ dnorm(radon.hat[i], tau.y)\r\nradon.hat[i] <- a[county[i]]+ b[county[i]]*floor[i]\r\n}\r\n\r\n#Prior dla sigma\r\nsigma.y ~ dunif (0, 1000)\r\ntau.y <- pow(sigma.y, -2)\r\n\r\n#Prior dla alpha\r\nalpha ~ dnorm (0, .0001)\r\n\r\n#Prior dla beta\r\nbeta ~ dnorm (0, .0001)\r\n\r\n#Priory dla a_j i b_j\r\nfor (j in 1:max(county)){\r\nmu[j,1] <- alpha \r\nmu[j,2] <- beta \r\ng[j,1:2] ~ dmnorm(mu[j,1:2],tau.g[1:2,1:2])\r\na[j] <- g[j,1]\r\nb[j] <- g[j,2]\r\n}\r\n\r\n#Prior dla wariancji a_j\r\nsigma.a ~ dunif (0, 100)\r\nsigma.g[1,1] <- pow(sigma.a, 2)\r\n\r\n#Prior dla wariancji b_j\r\nsigma.g[2,2] <- pow(sigma.b, 2)\r\nsigma.b ~ dunif (0, 100)\r\n\r\n#Prior dla wspóczynnika korelacji a_j i b_j\r\nrho ~ dunif (-1, 1)\r\n\r\n#Kowariancja a_j i b_j\r\nsigma.g[1,2] <- rho*sigma.a*sigma.b\r\nsigma.g[2,1] <- sigma.g[1,2]\r\n\r\n#Zamiana na macierz precyzji\r\ntau.g[1:2,1:2] <- inverse(sigma.g[,])\r\n}\"\r\n\r\n\r\nO ile nie interesuje nas relacja między piętrem a radonem w konkretnym hrabstwie, możemy śledzić jedynie wektor \\(\\boldsymbol{\\mu}\\) zawierający ogólny intercept (\\(\\alpha\\)) i współczynnik regresji \\(\\beta\\). Wyestymujmy także \\(\\rho\\), czyli nasz współczynnik korelacji pomiędzy \\(a_j\\) i \\(b_j\\).\r\n\r\n\r\nparams = c(\"alpha\",\"beta\", \"rho\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 6000\r\nnt = 1\r\nnc = 4\r\njmod = jags.model(file = textConnection(model), data = data, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(jmod, n.iter=ni, by=1)\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:9100\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 6000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean      SD  Naive SE Time-series SE\r\nalpha  1.4630 0.05470 0.0003531      0.0008216\r\nbeta  -0.6866 0.08453 0.0005457      0.0024429\r\nrho   -0.2220 0.40279 0.0026000      0.0213931\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%     50%       75%   97.5%\r\nalpha  1.3563  1.4260  1.4625  1.499288  1.5720\r\nbeta  -0.8456 -0.7444 -0.6896 -0.630995 -0.5148\r\nrho   -0.8533 -0.5100 -0.2832 -0.009378  0.8514\r\n\r\nTak jak się spodziewaliśmy, im większy indywidualny intercept, tym mniejszy współczynnik regresji dla posiadania piwnicy. Taki model, który właśnie stworzyliśmy, jest konceptualnym odpowiednikiem modelu z random interceptem i random współczynnikiem w statystyce częstościowej.\r\nPredyktory na wyższych poziomach\r\nZauważmy, że w danych mamy jeszcze informacje o średnim stężeniu uranu w glebie w danym hrabstwie. Radon powstaje w wyniku rozpadu uranu, więc jest to istotny predyktor stężenia radonu w mieszkaniu. Jednakże, posiadamy tylko dane dotyczące hrabstwa, a nie mieszkania, co wymaga od nas zamodelowania związku na poziomie klastra. Jak będzie wyglądał nasz model? Funkcja wiarygodności pozostaje taka sama:\r\n\\[y_i = N(a_j + b_jx_i,\\sigma)\\]\r\nale nasz rozkład a priori \\(a_j\\) i \\(b_j\\) będzie wyglądał tak:\r\n\\[ \\begin{aligned}\r\n\\begin{pmatrix}a_j \\\\ b_j \\end{pmatrix} &\\sim \\mathcal{N_m}(\\begin{pmatrix} \\boldsymbol{\\alpha + \\lambda u_j} \\\\ \\boldsymbol{\\beta} \\end{pmatrix} \\begin{pmatrix} \\mathrm{Var}(a_j) & \\mathrm{Cov}(a_j,b_j) \\\\ \\mathrm{Cov}(a_j,b_j) & \\mathrm{Var}(b_j) \\end{pmatrix}) \\\\\r\n\\end{aligned}\\]\r\nJak możemy zauważyć, intercept dla danego hrabstwa jest teraz modelowany jako zmienna losowa z rozkładu o średniej \\(\\alpha + \\lambda u_j\\). Widzimy też dosyć wyraźnie, że modelujemy relację pomiędzy stężeniem radonu a średnim stężeniem uranu na innym poziomie niż relację zmienną oznaczającą posiadanie piwnicy. Jeśli nazwiemy predyktorem dla pierwszego poziomu zmienną floor, to znaczy zmienną na poziomie każdej obserwacji (mieszkania), to średnie stężenie uranu wpływa tak samo na każde mieszkanie w hrabstwie, a więc jest predyktorem drugiego poziomu, to znaczy predyktorem interceptu dla klastra.\r\nMożemy dopełnić skomplikowanie modelu w następujący sposób.\r\n\\[ \\begin{aligned}\r\n\\begin{pmatrix}a_j \\\\ b_j \\end{pmatrix} &\\sim \\mathcal{N_m}(\\begin{pmatrix} \\boldsymbol{\\alpha + \\lambda u_j} \\\\ \\boldsymbol{\\beta + \\gamma u_j} \\end{pmatrix} \\begin{pmatrix} \\mathrm{Var}(a_j) & \\mathrm{Cov}(a_j,b_j) \\\\ \\mathrm{Cov}(a_j,b_j) & \\mathrm{Var}(b_j) \\end{pmatrix}) \\\\\r\n\\end{aligned}\\]\r\nCzym w takim wypadku będzie \\(\\gamma\\)? Będzie nam mówił jak zmienia się współczynnik dla piętra w zależności od średniego poziomu uranu w hrabstwie. Innymi słowy będzie współczynnikiem dla interakcji dla predyktora pierwszego poziomu - piętra i predyktora drugiego poziomu - uranu.\r\nZaimplementujmy taki model.\r\n\r\n\r\nmodel =  \"model {\r\n#Likehood\r\nfor (i in 1:length(radon)){\r\nradon[i] ~ dnorm(radon.hat[i], tau.y)\r\nradon.hat[i] <- a[county[i]]+ b[county[i]]*floor[i]\r\n}\r\n\r\n#Prior dla sigma\r\nsigma.y ~ dunif (0, 1000)\r\ntau.y <- pow(sigma.y, -2)\r\n\r\n#Prior dla alpha\r\nalpha ~ dnorm (0, .0001)\r\n\r\n#Prior dla beta\r\nbeta ~ dnorm (0, .0001)\r\n\r\n#Prior dla lambda\r\nlambda ~ dnorm (0, .0001)\r\n\r\n#Prior dla gamma\r\ngamma ~ dnorm (0, .0001)\r\n\r\n#Priory dla a_j i b_j\r\nfor (j in 1:max(county)){\r\nmu[j,1] <- alpha + lambda*uranium[j]\r\nmu[j,2] <- beta + gamma*uranium[j]\r\ng[j,1:2] ~ dmnorm(mu[j,1:2],tau.g[1:2,1:2])\r\na[j] <- g[j,1]\r\nb[j] <- g[j,2]\r\n}\r\n\r\n#Prior dla wariancji a_j\r\nsigma.a ~ dunif (0, 100)\r\nsigma.g[1,1] <- pow(sigma.a, 2)\r\n\r\n#Prior dla wariancji b_j\r\nsigma.g[2,2] <- pow(sigma.b, 2)\r\nsigma.b ~ dunif (0, 100)\r\n\r\n#Prior dla wspóczynnika korelacji a_j i b_j\r\nrho ~ dunif (-1, 1)\r\n\r\n#Kowariancja a_j i b_j\r\nsigma.g[1,2] <- rho*sigma.a*sigma.b\r\nsigma.g[2,1] <- sigma.g[1,2]\r\n\r\n#Zamiana na macierz precyzji\r\ntau.g[1:2,1:2] <- inverse(sigma.g[,])\r\n}\"\r\n\r\n\r\nBy użyć tego modelu musimy wyciągnąć uranium z naszych danych i stworzyć z niego wektor tylu wartości, ile jest hrabstw.\r\n\r\n\r\nuranium = unique(data %>% select(county,uranium)) %>% arrange(county) %>% select(uranium)\r\ndata = as.list(data)\r\ndata[[4]] = uranium$uranium\r\n\r\n\r\nZaimplementujmy model\r\n\r\n\r\nparams = c(\"alpha\",\"beta\", \"rho\", \"lambda\",\"gamma\")\r\nn.adapt = 100\r\nni = 3000\r\nnb = 6000\r\nnt = 1\r\nnc = 4\r\njmod = jags.model(file = textConnection(model), data = data, n.chains = nc, inits = NULL, n.adapt = n.adapt)\r\nupdate(jmod, n.iter=ni, by=1)\r\npost = coda.samples(jmod, params, n.iter = nb, thin = nt)\r\n\r\n\r\n\r\n\r\nsummary(post)\r\n\r\n\r\nIterations = 3101:9100\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 6000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean      SD  Naive SE Time-series SE\r\nalpha   1.4662 0.03733 0.0002409       0.001365\r\nbeta   -0.6720 0.08772 0.0005662       0.003427\r\ngamma  -0.4356 0.22992 0.0014841       0.008282\r\nlambda  0.8169 0.09538 0.0006157       0.003355\r\nrho     0.3093 0.43181 0.0027873       0.030863\r\n\r\n2. Quantiles for each variable:\r\n\r\n          2.5%      25%     50%     75%    97.5%\r\nalpha   1.3962  1.44042  1.4654  1.4915  1.54106\r\nbeta   -0.8353 -0.73408 -0.6755 -0.6171 -0.48892\r\ngamma  -0.8884 -0.58505 -0.4392 -0.2794  0.02015\r\nlambda  0.6301  0.75404  0.8161  0.8799  1.00167\r\nrho    -0.5815  0.00579  0.3332  0.6672  0.96624\r\n\r\nPodsumujmy, co dowiedzieliśmy się z tego modelu. Średnia beta jest ujemna, co oznacza, że jeśli dom nie posiada piwnicy, to średnie stężenie radonu jest mniejsze. Średnia lambda jest dodatnia, co sugeruje, że im wyższy średni poziom radonu w danym hrabstwie, tym wyższe stężenie radonu w mieszkaniu w tym hrabstwie.\r\nJednak średnia gamma jest ujemna, co oznacza, że im wyższe stężenie uranu w danym hrabstwie, tym mniejsze stężenie radonu w domach bez piwnicy. To wydaje się być nieco dziwne, ale warto zwrócić uwagę na przedział wiarygodności. Dla gamma 95% przedział wiarygodności zawiera 0, co sugeruje, że ta estymata może nie być zbyt wiarygodna.\r\nPodsumowanie\r\nZobaczyliśmy, jak tworzy się modele hierarchiczne w podejściu bayesowskim. Niemniej jednak, ponieważ ten post skupiał się tylko na wybranych aspektach, możliwe, że pozostajecie z niedosytem. W takim przypadku polecam książkę Gelmana i Hill o modelach hierarchicznych, w której w sposób systematyczny pokazują, jak modelować dane wielopoziomowe (Gelman & Hill, 2006).\r\n\r\n\r\n\r\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.\r\n\r\n\r\nGelman, A., Hill, J., & Yajima, M. (2012). Why we (usually) don’t have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5(2), 189–211.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-23-tutorial-bayes-iv/Statystyczne_Dygresje2.jpg",
    "last_modified": "2023-05-31T10:04:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-31-tutorial-bayes/",
    "title": "Tutorial Statystyki Bayesowskiej",
    "description": "Częśc I: Nabywanie intuicji",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-08-14",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\n\r\nContents\r\nMały wstęp\r\nPrawdopodobieństwo\r\nTwierdzenie Bayesa\r\nDyskretny przykład\r\n\r\nPewność pomiaru\r\nCzęstościowcy\r\nBayesowcy\r\n\r\nPrzykład praktyczny\r\nPodsumowanie\r\n\r\n\r\n\r\nMały wstęp\r\nPomyślałem sobie, że napiszę taki mały tutorial statystyki Bayesowskiej.\r\nPróba wytłumaczenie czegoś komuś jest jedną z najlepszych metod nauki i weryfikacji swojej własnej wiedzy. W przeciwieństwie do rozważań we własnym umyśle, gdy tłumaczy się innym jakieś zagadnienie, nie można nie odpowiedzieć na pewne pytania, czy niektóre rzeczy zgrabnie pominąć :)\r\nTutorial nie będzie na pewno wyczerpującym podejściem do tematu. Jeśli ktoś potrzebuje metodycznego wprowadzenia do metod bayesowskich, możne znaleźć je świetnej książce “Bayesian Data Analysis” (Gelman et al., 2021). Tutorial jest raczej pewnego rodzaju przyśpieszonym kursem pozwalający na późniejsze samodzielne eksplorowanie i zabawę statystyką Bayesowską. Forma tutorialu wynika z podejścia w akademii, które dla rozpoczynających karierę naukowców można sprowadzić do zdania: najpierw badaj, potem zastanawiaj się jak zbadać.\r\nOsobiście preferuję używanie języków programowania i obliczenia w tym tekście będą wykonywane za pomocą R, ale wiele z pokazanych tu rzeczy można także wykonać w graficznym programie statystycznym JASP. W zrozumieniu zawartych tu treści pomoże, jeśli uczestniczyłeś/łaś wcześniej w jakimś kursie statystyki.\r\nI jeszcze kilka słów wstępu o samej statystyce bayesowskiej. Istnieje obiegowa opinia, że statystyka Bayesowska jest trudniejsza od klasycznej. Osobiście wydaje mi się, że “prostota” klasycznej statystyki wynika z jej dogmatyzmu. Istnieją instrukcje na zastosowanie odpowiednich testów do danych, utarły się schematy, do których należy się stosować, mimo, że czasami są arbitralne. Statystyka Bayesowska wymaga więcej namysłu od badacza, ponieważ nie istnieją tak ostre „reguły kciuka” jak w statystyce klasycznej.\r\nO statystyce bayesowskiej mówiło się ostatnio, jako rozwiązaniu kryzysu replikacyjnego. Mam pewną obawę, że statystyka bayesowska może tak samo popaść w dogmatyzm, jeśli potraktuje się ją mechanicznie jak klasyczną statystykę. Nie jestem twardym Bayesowcem, to znaczy nie uważam, że należy porzucić statystykę częstościową (tą zwykłą) na rzecz Bayesowskiej. Obie mają pewne zalety i wady. Podzielam zdanie Kevina Rosa, że dobry współczesny statystyk korzysta z każdego z podejść w zależności od potrzeb.\r\nPrawdopodobieństwo\r\nWszyscy intuicyjnie czujemy czym jest prawdopodobieństwo. Jeśli chcemy sprawdzić czy moneta nie jest obciążona i częściej wypada reszka niż orzeł, najprostszym sposobem jest dokonanie “eksperymentu losowego” i sprawdzenie ile razy wypadnie reszka spośród wszystkich prób. Im więcej razy rzucimy monetą, tym bardziej będziemy pewni otrzymanego wyniku. W statystyce klasycznej wychodzimy z podobnego założenia, formalnie zdefiniowanego tak:\r\n\\[ \\lim_{n\\to\\infty} P(A) = \\frac{k_n(A)}{n} \\]\r\ngdzie \\(k_n(A)\\) to liczba zdarzeń sprzyjających zdażeniu \\(A\\) a \\(n\\) to liczba prób. Słownie możemy przedstawić to założenie tak: Gdy liczba prób dąży do nieskończoności, częstość wystąpień zdarzeń \\(A\\) pośród wszystkich prób będzie się równało prawdopodobieństwu wystąpienia zdarzenia \\(A\\). Innymi słowy im więcej razy rzucimy monetą, tym większe mamy szanse na otrzymanie takiego stosunku reszek do orłów, który jest bliski prawdziwemu prawdopodobieństwu otrzymania reszki.\r\nW statystyce Bayesowkiej na prawdopodobieństwo patrzy się trochę inaczej. Zakłada się, że prawdopodobieństwo jest miarą pewności, którą możemy przypisać pewnym modelom (hipotezom). Wnioskowanie Bayesowskie można porównać do tego, jak wnioskują ludzie. Mamy jakieś oczekiwania, następnie dostajemy dane i rewidujemy nasze oczekiwania.\r\nWnioskowanie Bayesowskie opiera się na twierdzeniu Bayesa:\r\n\\[  P(Hipoteza|Dane) = \\frac{P(Dane|Hipoteza)P(Hipoteza)}{P(Dane)}\\]\r\n\\(P(Hipoteza)\\) nazywane prawdopodobieństwem a priori, oznacza uprzednią wiedzę, przed zebraniem danych, które hipotezy są bardziej prawdopodobne. Innymi słowy są to nasze oczekiwania, na przykład, że gdy odkręcimy kran, najprawdopodobniej poleci z niego woda. Również brak wiedzy/oczekiwań jest prawdopodobieństwem a priori. Wtedy wszystkie hipotezy można zamodelować jako równie prawdopodobne.\r\nCzłon \\(P(Dane|Hipoteza)\\) (wiarygodność, likehood) oznacza prawdopodobieństwo otrzymania danych, że pod warunkiem wygenerowania ich w zgodzie z daną hipotezą (wygenerowania ich przez określony model). Można powiedzieć, że jest to sposób w jaki aktualizujemy nasze przekonania - tzn. nasz model rzeczywistości (po odkręceniu kranu coś z niego leci).\r\nWreszcie człon \\(P(Hipoteza|Dane)\\) jest prawdopodobieństwem a posteriori, czyli prawdopodobieństwem hipotezy w zależności od posiadanych danych. Innymi słowy prawdopodobieństwo a posteriori mówi nam jak bardzo zmieniły się nasze przekonania a priori pod wpływem danych. Czyli jeśli z kranu popłynęło wino, to zrewidowaliśmy nasze oczekiwania względem tego kranu.\r\n\\(P(Dane)\\) jest parametrem skalującym (stałą), bez specjalnej interpretacji. O jego roli przekonamy się nieco dalej w tekście.\r\nWszystko to wygląda fajnie, ale już na tym etapie może budzić zamieszanie. Czym są owe prawdopodobieństwa? Żeby to dobrze zrozumieć, przypomnimy sobie czym jest prawdopodobieństwo warunkowe, a następnie wyprowadzimy wzór Bayesa.\r\nTwierdzenie Bayesa\r\nPrawdopodobieństwo zdarzenia \\(A\\) w zależności od zdarzenia B równa się prawdopodobieństwu wystąpienia zdarzeń \\(A\\) i \\(B\\) podzielonym na prawdopodobieństwo zdarzenia \\(B\\):\r\n\\[P(A|B) = \\frac{P(A \\land B)}{P(B)}\\] By obliczyć prawdopodobieństwo palenia papierosów pod warunkiem bycia mężczyzną musimy po prostu podzielić liczbę mężczyzn palących papierosy przez liczbę wszystkich mężczyzn.\r\nZauważmy, że powyższe równanie możemy przekształcić. Najpierw pomnóżmy obie strony równania przez \\(P(B)\\).\r\n\\[  P(A|B)P(B) = {P(A \\land B)}\\] Teraz podzielmy obie strony przez \\(P(A)\\).\r\n\\[  \\frac{P(A|B)P(B)}{P(A)} = \\frac{P(A \\land B)}{P(A)} = P(B|A)\\] To co otrzymaliśmy, jest twierdzeniem Bayesa:\r\n\\[  P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\]\r\nDyskretny przykład\r\nBy uświadomić sobie przydatność twierdzenia Bayesa wykonajmy małe ćwiczenie. Załóżmy, że opracowaliśmy test pozwalający na wykrycie pewnej choroby z 99% prawdopodobieństwem u osób faktycznie chorych. Możemy zapisać to w następujący sposób \\(P(Pozytywny|Choroba) = 0.99\\). Test jednak daje też wynik fałszywie pozytywny u 5% osób zdrowych \\(P(Pozytywny|\\neg Choroba) = 0.05\\). Wiemy także, że nosicielem choroby jest 1 procent populacji \\(P(Choroba) = 0.01\\). Chcielibyśmy się dowiedzieć jakie jest prawdopodobieństwo choroby, jeśli wynik jest pozytywny \\(P(Choroba|Pozytywny)\\). By je obliczyć potrzebujemy jeszcze prawdopodobieństwa otrzymania wyniku pozytywnego. Uzyskać je możemy za pomocą prawa sumy.\r\n\r\n\r\nPrawo całkowitego prawdopodobieństwa  Jeśli \\(B_1\\), \\(B_2\\) … \\(B_n\\) są parami rozłączne, a ich prawdopodobieństwo sumy zdarzeń wynosi 1, to dla dowolnego zdarzenia \\(A\\) zachodzi wzór:  Dla wartości dyskretnych. \\[P(A)=\\sum_{B}P(A \\land B) \\] Dla wartosci ciągłych. \\[ P(A) = \\int_{}P(A \\land B)dB\\]\r\n\r\n\r\nCzyli, by obliczyć intersujące nas prawdopodobieństwo musimy podstawić odpowiednie wartości do wzoru:\r\n\\[P(Choroba|Pozytywny) =\\frac{P(Pozytywny|Choroba)P(Choroba)}{P(Pozytywny) = P(Pozytywny|Choroba)P(Choroba) + P(Pozytywny|\\neg Choroba)P( \\neg Choroba)}\\]\r\nPo podstawieniu do wzoru\r\n\\[\\frac{0.99*0.01}{0.99*0.01 + 0.05*0.99} = 0.166 \\]\r\nTylko 16% procent osób z pozytywnym testem faktycznie będzie miało chorobę. Powyższy przykład obrazuje czemu czasami profilaktyczne badania mogą powodować więcej problemów niż ich brak. Jeśli osoby byłby wysyłane na badanie bez innych wskazań świadczących o chorobie, ponad 85% osób zdrowych, otrzymałoby fałszywy wynik świadczący o chorobie.\r\nPewność pomiaru\r\nRozważmy teraz następujący problem. Mierzymy IQ 15 osób i chcemy się dowiedzieć z jaką pewnością możemy mówić o średniej IQ z próby jako estymacie średniej IQ w populacji.\r\nCzęstościowcy\r\nW statystyce klasycznej policzylibyśmy błąd standardowy i przedział ufności zgodnie z paradygmatem Null Hypothesis Significance Testing (NHST). Dla uwidocznienia różnic pomiędzy podejściem Bayesowskim a częstościowym, przypominajmy sobie na jakich założeniach opiera się NHST.\r\nZgodnie z Centralnym Twierdzeniem Granicznym średnia niezależnych zmiennych losowych o takim samym rozkładzie dąży o rozkładu normalnego gdy liczba zmiennych dąży do nieskończoności. Średnia takiego rozkładu jest równa średniej w populacji, a jej wariancja wynosi \\(\\frac{D^2}{n}\\), gdzie \\(D^2\\) to wariancja w populacji. Zwizualizujmy taki rozkład dla średnich z wielkości próby równej 15. Wiemy, że średnia IQ wynosi 100, a odchylenie standardowe 15.\r\n\r\n\r\nlibrary(tidyverse)\r\nbreaks <- qnorm(c(0, .025, .2, .5, .8, .975, 1),100, 15/sqrt(15))\r\nggplot(data.frame(x = c(85, 115)), aes(x)) +\r\nscale_fill_brewer(\"x\") +\r\nstat_function(fun = dnorm,n = 1000, args = list(mean = 100, sd = 15/sqrt(15)),geom = \"area\",\r\n  colour = \"gray30\", alpha = 0.7, aes(fill = after_stat(x) |> cut(!!breaks),\r\n  group = after_scale(fill))) +\r\n  geom_vline(xintercept = c(100 - 15/sqrt(15)*qnorm(.975), 100 + 15/sqrt(15)*qnorm(.975))) +\r\n  labs(x = \"Mean IQ\", y = \"Density\") +\r\n  scale_fill_discrete(name = \"P(X < x)\", labels = c('0.025', '0.2', '0.5', '0.8', '0.975','1'))\r\n\r\n\r\n\r\nStworzyliśmy w ten sposób estymowany rozkład wszystkich możliwych średnich IQ z piętnastoosobowych prób. Ten rozkład mówi nam, że losując 15 osobową próbę mamy 95% prawdopodobieństwo na wylosowanie takiej próby, w której średnie IQ znajdzie się w przedziale ufności oznaczonym przez czarne pionowe kreski na wykresie. 95% jest tu wartością umowną, badacz może przyjąć dowolny poziom ufności (Poziom ufności to \\(1-\\alpha\\), gdzie \\(\\alpha\\) to prawdopodbieństwo błędu I rodzaju).\r\nZwykle gdy dokonujemy pomiaru, nie znamy średniej i wariancji i dopiero estymujemy je z naszej próby. Jeśli będziemy wielokrotnie wykonywać pomiar i wyznaczać przedziały ufności, możemy spodziewać się, że w 95% w przedziałach ufności z prób znajdzie się prawdziwa wartość średniej IQ.\r\n\r\n\r\nset.seed(999)\r\ndata <- data.frame()\r\n\r\nfor(i in 1:50) {\r\n  sample <- rnorm(15, 100, 15)\r\n  data <- rbind(data,data.frame(Mean = mean(sample), se = sd(sample)/sqrt(15)))\r\n}\r\n\r\ndata$Sample <-1:50\r\n\r\nggplot(data, aes(x = Sample, y = Mean)) +       \r\n  geom_errorbar(aes(ymin = Mean - se*qnorm(.975), ymax = Mean + se*qnorm(.975))) +\r\n  coord_flip() +\r\n  geom_hline(yintercept = 100, colour = \"red\" )\r\n\r\n\r\n\r\nNo dobrze, jaki z płynie wniosek? W idealnej sytuacji, gdy nie ma błędów systematycznych, wszystkie założenia są spełnione i próba była wylosowana w sposób reprezentatywny, kiedy wykonujemy wiele pomiarów średniej, średnio 95% przedziałów ufności dla próby powinno zawierać w sobie prawdziwą średnią.\r\nZałóżmy teraz, że wykonaliśmy pomiar IQ w 15 osobowej próbie i stwórzmy sobie takie dane:\r\n\r\n\r\niq <- c(87, 112, 83, 95, 106,105, 123, 107, 103, 98, 137, 114, 90, 90, 117)\r\ndata <- data.frame(IQ = \"Our sample\", Mean = mean(iq), se = sd(iq)/sqrt(length(iq)))\r\nggplot(data, aes(x = IQ, y = Mean)) +        \r\n  geom_point() +\r\n  geom_errorbar(aes(ymin = Mean - se*qnorm(.975), ymax = Mean + se*qnorm(.975))) +\r\n  coord_flip()\r\n\r\n\r\n\r\nJaką daje nam to pewność co do naszego pomiaru? Na ile możemy być pewni, że prawdziwa średnia leży w tym przedziale ufności? Tu zaczynają się trudności.\r\nNHST historycznie powstała do stosowania w warunkach przemysłowych, gdzie wykonujemy bardzo wiele identycznych badań, na przykład w kontroli jakości produktów. Na przykład, w Ikei produkowany jest pewien element do konstrukcji szafek. Każdy kolejny wyprodukowany egzemplarz tego elementu różni się odrobinę w długości od schematu, ale dobrze skalibrowana maszyna przycinająca elementy działa tak, że te różnice nie są na tyle duże by element nie pasował do szafki.\r\nOsoba odpowiedzialna za kontrolę jakości regularnie wykonuje pomiary długości \\(n\\) elementów każdego dnia, by sprawdzić czy średnia długość elementów nie wykracza założonego przedziału bezpiecznej różnicy w długości. Obliczył sobie ile elementów musi zmierzyć dziennie, by przedział ufności pokrywał się z tą bezpieczną różnicą przy ustalonym poziomie ufności 90%. Jeśli maszyna będzie działać poprawnie, kontroler otrzyma średnio 1 fałszywy alarm na 10 dni.\r\nZasadniczo, w idealnej sytuacji mamy \\(1-\\alpha\\) prawdopodobieństwo na wylosowanie takiej próby, której przedział ufności będzie zawierał prawdziwą średnią. Często spotykane, choć nie do końca zasadne, jest twierdzenie, w danej próbie istnieje prawdopodobieństwo równe \\(1-\\alpha\\), że w przedziale zawiera się średnia. Takie prawdopodbieństwo mamy przed wylosowaniem próby, gdy już ją mamy, nie wiemy co wylosowaliśmy.\r\nBayesowcy\r\nNo dobrze. Czas na Bayesowskie podejście do problemu. Chcemy obliczyć \\(P(Hipoteza|Dane)\\). Co jest jednak naszą hipotezą? O ile w przypadku zmiennej binarnej jak choroba i jej brak jest to proste, tutaj nasza hipoteza dotyczy wartości średniej - wartość średniej jest raczej zmienną ciągłą, może być dowolna. Dlatego \\(P(Hipoteza|Dane)\\) będzie rozkładem prawdopodobieństwa dla każdej możliwej wartości średniej.\r\nRozkład IQ jest, zgodnie z teorią, rozkładem normalnym. Dlatego \\(P(Dane|Hipoteza)\\) możemy zamodelować jako prawdopodobieństwo otrzymania danych pod warunkiem określonych wartości średniej (krtórą oznaczymy jako \\(\\mu\\)) i odchylenia standardowego (które oznaczymy jako \\(\\sigma\\)) rozkładu normalnego, które nazywamy parametrami. Gdy mamy dużo parametrów, zwykle dla wygody oznaczmy je literką \\(\\theta\\), która symbolizuje wektor parametrów.\r\nMy jednak mamy tylko dwa parametry, więc nasze \\(P(Dane|Hipoteza)\\) zapiszemy jako \\(P(y| \\mu ,\\sigma)\\), które jest rozkładem prawdopodobieństwa, mówi nam jak prawdopodobne jest wylosowanie wartości \\(y\\) z rozkładu normalnego \\(N(\\mu,\\sigma)\\). Na przykłd, gęstość prawdopodobieństwa otrzymania wartości 6 z rozkładu \\(N(4,2)\\) wynosi:\r\n\r\n\r\ndnorm(6,4,2)\r\n\r\n[1] 0.1209854\r\n\r\nZauważmy, że chcemy by nasza funkcja wiarygodności dawała prawdopodobieństwo wszystkich danych jakie mamy. Ponieważ dane (obserwacje), które mamy są losowe i od siebie niezależne, prawdopodobieństwo ich wylosowania równa się ich iloczynowi:\r\n\\[P(A \\land B) = P(A)P(B)\\] Czyli, prawdopodobieństwo wylosowania całego zbioru danych z rozkładu o określonych parametrach będzie iloczynem prawdopodobieństwa wylosowania każdej obserwacji z osobna:\r\n\\[P(y|\\mu,\\sigma) = \\prod_{i=1}^{n}P(y_i|\\mu,\\sigma) = P(y_1|\\mu,\\sigma)*P(y_2|\\mu,\\sigma)...P(y_n|\\mu,\\sigma)\\].\r\nSkoro mamy już naszą funkcję wiarygodności, potrzebujemy jeszcze naszego prawdopodobieństwa A priori \\(P(\\mu,\\sigma)\\). Zakładamy, że wartość średniej jest niezależna od wartości odchylenia standardowego, czyli \\(P(\\mu,\\sigma) = P(\\mu) P(\\sigma)\\). Tak jak już wspomniałem, prawdopodobieństwo a priori przedstawia stan wyjściowy (np. naszą wcześniejszą wiedzę).\r\nJaką wiedzę mamy na temat średniego IQ? Wiemy, że w populacji wynosi 100 (tak jest skonstruowana ta miara). Raczej nie podejrzewamy dużych odstępów od tej wartości więc możemy zamodelować \\(P(\\mu)\\) rozkładem normalnym o średniej 100 i odchyleniu 2 \\(N(100,3)\\). Wiemy też, że odchylenie standardowe wynosi 15, ale na potrzeby tego ćwiczenia udamy, że tego nie wiemy. A skoro nie mamy żadnej wcześniejszej wiedzy na temat SD, uznajemy że każda wartość SD jest tak samo prawdopodobna i zamodelujemy \\(P(\\sigma)\\) rozkładem jednostajnym (prawdopodobieństwo wylosowania każdej wartości z zadanego przedziału jest takie same).\r\nNasz wzór możemy zapisać:\r\n\\[P(\\mu ,\\sigma|y) = \\frac{P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)}{P(y)}\\]\r\nPotrzebujemy jeszcze prawdopodobieństwa \\(P(y)\\). Ponownie wykorzystujemy prawo sumy, tym razem dla zmiennych ciągłych:\r\n\\[P(y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)d\\mu d\\sigma\\]\r\nChoć ta podwójna całka może wyglądać dla niektórych przerażająco, można skonceptualizować ją sobie jako sumowanie członu \\(P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\) po wszystkich wartościach \\(\\mu\\) i \\(\\sigma\\).\r\nNa marginesie, zasadniczo nie musimy jej obliczać ponieważ jest to wartość stała, która skaluje otrzymany rozkład post priori by suma prawdopodobieństw równała się 1. Dlatego nasz wzór możemy zapisać:\r\n\\[P(\\mu ,\\sigma|y) \\propto P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\] Co oznacza, że nasz rozkład \\(P(\\mu ,\\sigma|y)\\) jest proporcjonalny do \\(P(y| \\mu ,\\sigma)P(\\mu)P(\\sigma)\\).\r\nTeraz możemy policzyć nasze szukany rozkład prawdopodobieństwa \\(P(\\mu ,\\sigma|y)\\). W statystyce bayesowskiej popularnością cieszą się rozwiązania numeryczne, ponieważ rozwiązania analityczne (czyli rozwiązania danego problemu czysto za pomocą wzorów) często nie istnieją.\r\nBy obliczyć nasze rozwiązanie, też posłużymy się metodą numeryczną. Będzie ona bardzo prosta, co pozwoli prześledzić dokładnie jak wykonywane są obliczenia, jednak w praktyce stosuje się trochę bardziej skomplikowane metody, które omówimy w następnych częściach tutorialu.\r\nRównania wyglądają czasami bardzo abstrakcyjnie, zwłaszcza jeśli nie można sobie dokładnie wyobrazić tego co reprezentują. Kod przydaje się w takich momentach. Po każdej wykonanej linijce można sprawdzić co nam wyszło i powiązać to z matematycznym zapisem.\r\n\r\n\r\n# Dane z naszej próby\r\niq <- c(87, 112, 83, 95, 106,105, 123, 107, 103, 98, 137, 114, 90, 90, 117)\r\n\r\n# To są nasze parametry czyli możliwe wartości parametrów mu i sigma. \r\n# Ponieważ jest to proste przybliżenie numeryczne, ograniczamy możliwe wartości średniej \r\n# między 50 a 150, dla odchylenia standardowego między 0.1 a 50.\r\npars <- expand.grid(mu = seq(50, 150, length.out = 300), \r\n                    sigma = seq(0.1, 50, length.out = 300))\r\n\r\n# Liczymy marginalne rozkłady a priori.\r\npars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 3)\r\npars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)\r\n\r\n# By uzyskać łączne prawdopodobieństwo a priori mnożymy je przez siebie.  \r\npars$prior <- pars$mu_prior * pars$sigma_prior\r\n\r\n# Obliczamy wiarygodność \r\nfor(i in 1:nrow(pars)) {\r\n  # Prawdopodobieństwo wylosowania danej obserwacji z rozkładu normalnego o parametrach mu i sigma.\r\n  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])\r\n  # Mnożymy by uzyskać wiarygodność\r\n  pars$likelihood[i] <- prod(likelihoods)\r\n}\r\n# Mnożymy wiarygodność przez prawdopdobieństwo a priori. \r\npars$probability <- pars$likelihood * pars$prior\r\n\r\n#Otrzymaną wartość dzielimy przez P(y). \r\npars$probability <- pars$probability/sum(pars$probability)\r\n\r\n# Voilà, mamy nasz rozkład prawdopodobieństwa P(mu, sigma| y)\r\nlibrary(lattice)\r\nlibrary(viridisLite)\r\ncoul <-  viridis(1000)\r\nlevelplot(probability ~ mu * sigma, data = pars, col.regions = coul)\r\n\r\n\r\n\r\nWidzimy tu prawdopodobieństwo, że dana kombinacja parametrów \\(\\mu\\) i \\(\\sigma\\) wygenerowała nasze dane. W odróżnieniu do statystyki częstościowej odpowiedzią na nasze pytanie jest nie punktowa wartość parametru, ale jego rozkład \\(P(\\theta|y)\\). Możemy skonstruować przedział, w którym zawrze się 95% prawdopodobieństwa. Nazywa się go przedziałem wiarygodności. W odróżnieniu do przedziału ufności, interpretacja przedziału wiarygodności mówi: Biorąc pod uwagę dane, które mamy, oraz prawdopodobieństwa a priori, mamy 95% prawdopodobieństwo, że parametr, który wygenerował dane jest w przedziale wiarygodności.\r\nWeźmy rozkład marginalny dla średniej za pomocą prawa sumy:\r\n\r\n\r\nmu = pars %>% group_by(mu) %>% summarise(probability = sum(probability))\r\ncs = cumsum(mu$probability)\r\nboundary_1 = mu$mu[which(cs < 0.025)[length(which(cs < 0.025))]]\r\nboundary_2 = mu$mu[which(cs >= 0.975)[1]]\r\nggplot(mu, aes(x = mu,y = probability))+\r\n  geom_line(colour = 'light blue') +\r\n  xlim(92,111) +\r\n  ylab(\"Density\") +\r\n  geom_area(alpha = 0.75, fill = 'light blue') +\r\n  geom_vline(xintercept = c(boundary_1, boundary_2))\r\n\r\n\r\n\r\nZastanówmy się co by się stało gdybyśmy ponowili nasze badanie. W przypadku statystyki częstościowej musielibyśmy powtórzyć całe postępowanie (tak, technicznie możemy zebrać dane z obu badań i policzyć z nich przedział ufności czy inną statystykę, ale nie byłoby to zgodne z założeniami testów statystycznych). W przypadku bayesowskim możemy potraktować nasz rozkład post priori \\(P(\\mu|y)\\) jako rozkład a priori \\(P(\\mu)\\) w następnym badaniu.\r\nŚrednia IQ w naszej bayesowskiej analizie wyszła nieco mniejsza niż w częstościowej, ponieważ zastosowaliśmy informatywny rozkład a priori. Tu właśnie mamy do czynienia z elementem “subiektywnym.” Możliwość arbitralnego doboru wyjściowego przekonania co do stanu rzeczy, była dla statystyków częstościowych czymś nie do przyjęcia. Jak się jednak przekonamy, dobór prawdopodobieństwa a prori nie jest aż taką arbitralną decyzją, a rozkłady a priori posiadają także niesubiektywistyczną interpretację.\r\nNo dobrze, wynikiem analizy częstościowej i bayesowskiej jest jakiś przedział. Może interpretacje się różnią, ale czy w praktyce nie wychodzi na to samo? Otóż nie. Przedziału wiarygodności używamy w różnych sytuacjach, ale statystyka bayesowska nie sprowadza się do bayesowskiego p-value.\r\nPrzykład praktyczny\r\nZałóżmy, że korzystamy z internetowej księgarni, która ma system oceny książek przez czytelników. Dla uproszczenia załóżmy, że internauci mogą jedynie polecić książkę lub ją nie polecić. Wybraliśmy sobie trzy książki, które nas interesują, ale stać nas na tylko jedną. Chcemy więc dokonać finalnego wyboru sugerując się ocenami innych. Książkę A poleca 8 z 10 czytających, książkę B poleca 35 z 50, a książkę C poleca 60 z 100.\r\nOdsetek poleceń książki w stosunku do wszystkich głosów, możemy potraktować jako ocenę jakości książki. Chcemy więc, by ta wartość była jak największa. Ale jednocześnie chcemy nasza miara jakości była jak najbardziej pewna. A jaka jest najbardziej pewna? Ta która ma najwięcej głosów (danych). Jak więc wybrać książkę, która ma największe prawdopodobieństwa bycia dobrą lekturą? Zauważmy, że prawdopodobieństwo wylosowania \\(k\\) ocen pozytywnych z \\(n\\) wszystkich ocen danej książki, jeśli książka ma prawdopodobieństwo \\(p\\) otrzymania recenzji pozytywnej możemy zamodelować rozkładem dwumianowym.\r\n\r\nRozkład dwumianowy  Rozkład dwumianowy modeluje prawdopodobieństwo uzyskania \\(k\\) sukcesów z \\(n\\) prób, gdy prawdopodobieństwo sukcesu wynosi \\(p\\). Dany jest wzorem: \\[ P(k)  =  {{n}\\choose{k}} \\cdot p^k(1-p)^{n-k}\\]\r\n\r\n W statystyce klasycznej najlepszym estymatorem \\(p\\) jest po prostu \\(\\frac{k}{n}\\). Nie uwzględnia to jednak niepewności pomiaru. My jednak chcielibyśmy podejść do problemu bayesowsko. Musimy więc zdefiniować wszystkie potrzebne zmienne.\r\nZmienną, której rozkład chcemy poznać jest \\(p\\), dlatego \\(p\\) będzie naszą hipotezą. Dane to recenzje pozytywne i negatywne. Mamy już dobrego kandydata na funkcję wiarygodności. Prawdopodobieństwo uzyskania \\(k\\) poleceń na \\(n\\) ocen pod warunkiem konkretnej wartości \\(p\\) zamodelujemy rozkładem dwumianowym.\r\n\\[P(k,n|p) = {{n}\\choose{k}} \\cdot p^k(1-p)^{n-k}\\]\r\nPotrzebujemy jeszcze prawdopodobieństwa a priori \\(P(p)\\). Zauważmy, że ponieważ szukane przez nas \\(p\\) jest prawdopodobieństwem, może przyjmować dowolne wartości z przedziału <0,1>. Do modelowania prawdopodobieństwa prawdopodobieństwa często wykorzystuje się rozkład Beta.\r\n\r\nRozkład Beta  Rozkład Beta to rozkład prawdopodobieństwa, którego nośnik (przedział dla którego funkcja zwraca wartości większe od 0) to <0,1>. Rozkład ma dwa parametry kształtu \\(\\alpha\\) i \\(\\beta\\). Dany jest wzorem: \\[ P(x)  =  \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\] W mianowniku mamy funkcję Beta. Dla wygody zapisuje się jako \\(B(\\alpha,\\beta)\\).  Wartość oczekiwana rozkładu Beta to: \\[E(x) = \\frac{\\alpha}{\\alpha+ \\beta}\\]  Na wykresie kształt rozkładu dla różnej wartości parametrów. \r\n\r\nA więc \\(P(p)\\) możemy zamodelować rozkładem Beta.\r\n\\[ P(p)  =  \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\\]\r\nCo wiemy parametrach tego rozkładu? Prawdopodobnie nic, ale możemy zastosować metodę zwaną empirycznym Bayesem. To znaczy, wyestymujemy rozkład a priori z danych. Nie jest to ortodoksyjne podejście Bayesowskie, które zakłada zdefiniowanie rozkładów a priori przed spojrzeniem na dane. Niemniej, człowiek musi sobie jakoś radzić.\r\nKsięgarnia internetowa posiada oceny wszystkich książek, jakie są w jej posiadaniu. Załóżmy, że mamy dostęp do tych danych (zasymulujmy je) i stwórzmy histogram wszystkich ocen.\r\n\r\n\r\nset.seed(123)\r\nx = rbeta(10000,10,10)\r\nhist(x)\r\n\r\n\r\n\r\nNa histogramie widzimy jak często pojawiają się dane oceny. Rozkład jest skupiony symetrycznie wokół wartości 0.5, które występują najczęściej. Im wartości bardziej oddalone od 0.5 tym rzadziej występują. Oznacza to, że książki bardzo dobre, albo bardzo złe są rzadsze od przeciętnych.\r\nTen rozkład jest sensownym wyborem rozkładu a priori. Mówi nam jak prawdopodobne jest, że wylosujemy książkę o danej ocenie, zanim zobaczymy ocenę konkretnej pozycji.\r\nWykorzystajmy ten rozkład by dobrać wartości \\(\\alpha\\) i \\(\\beta\\). Wyestymujmy parametry rozkładu z danych:\r\n\r\n\r\nfit <- function(pars,x) {-sum(log(dbeta(x,pars[1],pars[2])))}\r\nstart <- c(1,1)\r\nnames(start) <-c('a','b')\r\nrecov <- nlminb(start, fit, x = x, lower = -Inf, upper = Inf)\r\nround(recov$par, 2)\r\n\r\n   a    b \r\n9.95 9.96 \r\n\r\nFantastycznie. Mamy wszystko czego potrzebujemy by obliczyć nasz rozkład:\r\n\\[P(p|k,n) = \\frac{P(k,n|p)P(p)}{P(k,n)} \\] Zdradzę wam teraz, że rozwiązaniem jest takie, że \\(P(p|k,n)\\) jest rozkładem Beta o parametrach \\(\\alpha = \\alpha_0 + k\\) i \\(\\beta = \\beta_0 + n - k\\), gdzie \\(\\alpha_0\\) i \\(\\beta_0\\) to parametry rozkładu a priori.\r\nAnalityczną forma \\(P(p|k,n)\\) istnieje. Rozkład Beta jest zgodnym rozkładem a priori  (conjugate prior) dla rozkładu dwumianowego. To znaczy, że w tym wypadku nie musimy przybliżać rozwiązania metodami numerycznymi, ale możemy je wprost obliczyć.\r\nJeśli kogoś interesują obliczenia, które nie są trudne, a mogą pomóc zrozumieć jak rozkłady a priori, post priori i wiarygodność są powiązane, zapraszam do rozwinięcia tekstu poniżej.\r\nKliknij mnie\r\nPodstawmy wszystkie rozkłady pod nasze równanie:\r\n\\[P(p|k,n) =\\frac{{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}}{\\int{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}dp} \\]\r\nNie wygląda zbyt zachęcająco, prawda? Ale spokojnie, rozwiązanie tego równania jest bardzo proste. Weźmy wszystkie człony równania, które nie zależą od \\(p\\) i zapiszmy je jako \\(C\\).\r\n\\[P(p|k,n) = C*p^k(1-p)^{n-k}p^{\\alpha-1}(1-p)^{\\beta-1} = C* p^{\\alpha-1 +k}(1-p)^{\\beta-1 + n - k}\\] Gdzie:\r\n\\[ C = \\frac{{{n}\\choose{k}}\\frac{1}{B(\\alpha,\\beta)}}{\\int{{n}\\choose{k}} \\cdot p^k(1-p)^{n-k} \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}dp} \\]\r\nNo dobra, teraz mała sztuczka. Zauważmy teraz, że rozkład Beta jest proporcjonalny do \\(p^{\\alpha-1}(1-p)^{\\beta-1}\\):\r\n\\[\\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\\propto p^{\\alpha-1}(1-p)^{\\beta-1}\\]\r\nZauważmy też, że nasz w szukanym prze nas rozkładzie, \\(C\\) nie zależy od \\(p\\), jest więc jakąś wartością stałą, skalującą rozkład. \\[P(k,n|p) \\propto p^{\\alpha-1 + k}(1-p)^{\\beta-1 + k - n}\\]\r\nPrzyjmijmy, że \\(\\alpha = \\alpha_0 +k\\) i \\(\\beta = \\beta_0 + n - k\\), gdzie \\(\\alpha_0\\) i \\(\\beta_0\\) to parametry \\(\\alpha\\) i \\(\\beta\\) z powyższego wzoru. Teraz już powinniśmy zauważyć, że nasz rozkład jest proporcjonalny do rozkładu Beta. A ponieważ nasz \\(P(p|k,n)\\) jest rozkładem prawdopodobieństwa, to jego suma/całka musi równać się 1. Rozkładem proporcjonalnym do rozkładu Beta, którego suma/całka wynosi 1, jest właśnie rozkład Beta, a więc:\r\n\\[ C = \\frac{1}{B(\\alpha,\\beta)}\\]\r\nNasz rozkład a posteriori \\(P(p,|k,n)\\) jest rozkładem Beta o parametrach \\(\\alpha = \\alpha_0 + k\\) i \\(\\beta = \\beta_0 + n - k\\).\r\nWiemy już, że rozkłady \\(P(p|k,n)\\) dla każdej książki to rozkłady Beta o następujących parametrach:\r\n\\[A_p \\sim Beta(17.95, 11.96)\\] \\[B_p \\sim Beta(39.93, 19.93)\\] \\[C_p \\sim Beta(64.93, 44.93)\\]\r\nZobaczmy je na wykresie razem z wykresem a priori:\r\n\r\n\r\nlibrary(tidyverse)\r\nfun.1 = function(x){dbeta(x,17.95, 11.96)}\r\nfun.2 = function(x){dbeta(x,44.95, 24.96)}\r\nfun.3 = function(x){dbeta(x,69.95, 49.96)}\r\nfun.4 = function(x){dbeta(x,9.95, 9.96)}\r\ncolors <- c(\"A\" = \"blue\", \"B\" = \"green\", \"C\" = \"red\", \"a priori\" = \"black\")\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \r\n  stat_function(fun = fun.1, aes(color = \"A\")) + \r\n  stat_function(fun = fun.2, aes(color = \"B\")) +\r\n  stat_function(fun = fun.3, aes(color = \"C\")) +\r\n  stat_function(fun = fun.4, aes(color = \"A priori\")) +\r\n  xlab(\"p\") + ylab(\"Density\") + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"Rozkład\", values = colors) + \r\n  xlim(0,1) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nCzarna linia to wykres a priori. Symbolizuje naszą wiedzę na początku, jak prawdopodobne jest wybranie losowej książki o danej ocenie. Pozostałem linie pokazują jak zmieniły się nasze przekonania w stosunku do konkretnych książek po zobaczeniu ich oceny.\r\nMamy rozkłady, ale chcemy podjąć decyzję, a do tego potrzebujemy mieć punktowe estymaty \\(p\\). Możemy ją otrzymać na kilka sposobów. Możemy wziąć takie \\(p\\), które ma największe prawdopodobieństwo. Inną popularną estymatą jest średnia, ponieważ minimalizuje ona kwadratową funkcję błędu \\(l(p, \\hat{p}) = E((p - \\hat{p})^2)\\). Ponadto gdy nasz rozkład post priori jest symetryczny, obie te wartości są równe.\r\nPoliczmy sobie średnie rozkładów post priori.\r\n\\[E(A_p) = 0.6\\] \\[E(B_p) = 0.64\\] \\[E(C_p) = 0.58\\]\r\nJak widzimy, książka B ma największe prawdopodobieństwo bycia najlepszą książką. Co tu się właściwie zadziało? Wszystkie prawdopodobieństwa są mniejsze, niż gdybyśmy wzięli po prostu częstościową estymatę \\(\\frac{k}{n}\\).\r\nEstymata bayesowska oceny jest mniejsza od częstościowego odpowiednika dla każdej z książek. Dzieje się tak dlatego, że rozkład a priori wskazuje, że prawdopodobieństwa bliższe 0.5 - średniej rozkładu a priori są bardziej prawdopodobne i “ściąga” rozkłady post priori w swoją stronę. Jednocześnie bayesowska ocena książki C najmniej się różni od estymaty częstościowej, dlatego, że ocena książki C składa się z największej ilości głosów (obserwacji).\r\nŻeby to zwizualizować, załóżmy teraz, że wszystkie trzy książki mają 0.8 pozytywnych ocen, ale różnią się liczebnością jak w poprzednim przykładzie i popatrzmy na wykres:\r\n\r\n\r\nlibrary(tidyverse)\r\nfun.1 = function(x){dbeta(x,17.95, 11.96)}\r\nfun.2 = function(x){dbeta(x,49.95, 19.96)}\r\nfun.3 = function(x){dbeta(x,89.95, 29.96)}\r\nfun.4 = function(x){dbeta(x,9.95, 9.96)}\r\ncolors <- c(\"A\" = \"blue\", \"B\" = \"green\", \"C\" = \"red\", \"a priori\" = \"black\")\r\n\r\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \r\n  stat_function(fun = fun.1, aes(color = \"A\")) + \r\n  stat_function(fun = fun.2, aes(color = \"B\")) +\r\n  stat_function(fun = fun.3, aes(color = \"C\")) +\r\n  stat_function(fun = fun.4, aes(color = \"A priori\")) +\r\n  xlab(\"p\") + ylab(\"Density\") + labs(color = \"Legend\") + \r\n  scale_colour_manual(\"Rozkład\", values = colors) + \r\n  xlim(0,1) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nIm więcej mamy danych (obserwacji), tym mniej istotne staje się prawdopodobieństwo a priori. Jednocześnie im bardziej informatywny rozkład a priori (im węższy, im więcej gęstości prawdopodobieństwa jest skupione wokół danej wartości) tym więcej danych trzeba by rozkład post priori różnił się od niego.\r\nPodsumowanie\r\nW statystyce częstościowej dane są modelowane jako zmienne losowe, a parametry traktowane są jako stałe. W analizie staramy się oszacować najlepszą możliwą punktową wartość tego parametru. Jak zauważyliśmy, analiza Bayesowska zwraca nam rozkład prawdopodobieństwa parametru, którego szukamy. Rozkład reprezentuje nasz poziom pewności co do wartości parametru. W podejściu Bayesowskim dane są stałe, a parmetry są zmiennymi losowymi. To znaczy, że szukane przez nas parametry reprezentujemy jako rozkład statystyczny, pod warunkiem zebranych przez nas danych. Gdy otrzymujemy nowe dane, możemy aktualizować naszą pewność co do prawdopodobieństwa konkretnych wartości parametrów.\r\nMam nadzieję, że pokazanymi tu przykładami udało mi się pokazać na czym polega wnioskowanie Bayesowskie. W następnej części zajmiemy się budowaniem i estymowaniem modeli Bayesowskich.\r\n\r\n\r\n\r\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2021). Bayesian data analysis. Chapman; Hall/CRC.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-31-tutorial-bayes/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-12-24T13:48:41+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-13-mixedmotivation/",
    "title": "Dlaczego liniowe modele mieszane?",
    "description": "Mała zachęta do stosowania mieszanych modeli liniowych.",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-05-14",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nDzisiejszy wpis będzie taką mini motywacją do zainteresowania się mieszanymi modelami liniowymi (aka hierarchicznymi/wielopoziomowymi modelami liniowymi).\r\nRozważmy hipotetyczny eksperyment. Badane osoby oglądają różne zdjęcia, a po po obejrzeniu oceniają jak bardzo zdjęcie było pobudzające. W trakcie badania uczesnicy mają na palcu wskazującym i środkowym elektrody mierzące aktywność skórno-galwaniczną (mikropocenie).\r\nNastępnie możemy zadać pytanie czy większa amplituda sygnału z elektrod jest związana z większym pobudzeniem. Jeśli pytamy o to, czy osoby z średnio wyższą amplitudą sygnału mają wyższą średnią ocenę (efekt międzyobiektowy), możemy po prostu uśrednić pomiary na osobę i skorelować je ze sobą.\r\nJeśli jednak zapytamy czy średnio im większy sygnał w pojedynczym pomiarze, tym wyższa ocena (efekt wewnątrzobiektowy), takie podejście może nie zadziałać. Czemu? Zasymulujmy sobie takie dane.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(lme4)\r\nlibrary(parameters)\r\nlibrary(datawizard)\r\nlibrary(cowplot)\r\n\r\n\r\nbetween_effect = 0\r\nwithin_effect = 0.5\r\nbetween_sd = 2\r\nwithin_sd = 0.1\r\n\r\nsignal_intercepts = rnorm(100,0,1) \r\nresponse_intercepts = rnorm(100,0, between_sd) + between_effect * signal_intercepts\r\n\r\ndata =  data.frame()\r\nfor (participant in 1:100){\r\n  \r\n  signal = rnorm(100,4,0.1)  \r\n  response =  within_effect*signal + rnorm(100,0,within_sd)\r\n  response = response + response_intercepts[participant]\r\n  signal = signal + signal_intercepts[participant]\r\n  data =  rbind(data, data.frame(signal,response,participant))\r\n  \r\n}\r\ndata_averaged = data %>% group_by(participant) %>% summarise(mean_signal = mean(signal), mean_response = mean(response))\r\n\r\ncat(\"Korelacja pojedynczych pomiarów\",cor(data$signal, data$response))\r\n\r\n\r\nKorelacja pojedynczych pomiarów -0.0215814\r\n\r\ncat(\"Korelacja średnich pomiarów na osobę\",cor(data_averaged$mean_signal, data_averaged$mean_response))\r\n\r\n\r\nKorelacja średnich pomiarów na osobę -0.02477697\r\n\r\nNie obserwujemy znaczących korelacji w żadnym wypadku. Zerknięcie na wykres wyjaśni tajemnicę:\r\n\r\n\r\nplot1 <- ggplot(data, aes(x = signal, y = response, colour =\r\n  as.character(participant))) + geom_point() + theme_bw() + \r\n  theme(legend.position=\"none\")\r\n\r\nplot2 <- ggplot(data_averaged, aes(x = mean_signal, y = mean_response)) +\r\n  geom_point() + theme_bw()\r\n\r\nplot_grid(plot1, plot2, labels = \"AUTO\")\r\n\r\n\r\n\r\n\r\nJeśli spojrzymy na wykres po lewej, zobaczymy różnokolorowe zgrupowane punkty. Te kolory to poszczególni badani. Jeśli się im przyjrzymy zauważymy, że są lekko przechylone w prawo, co sugeruje związek liniowy. Jednak badani są rozrzuceni po całym wykresie (duża wariancja międzyobiektowa), ponieważ występuje duże zróżnicowanie pomiędzy ich bazowymi amplitudami i ocenami. Wykres ich średnich pokazuje brak związku, ponieważ te bazowe wartości nie są ze sobą związane. Jak więc wykryć związek?\r\nA gdybyśmy tak od wartości naszych zmiennych odjęli średnią dla danego badanego?\r\n\r\n\r\ndata = data %>% group_by(participant) %>% mutate(signal_demeaned = signal - mean(signal), \r\nresponse_demeaned = response - mean(response))\r\n\r\nggplot(data, aes(x = signal_demeaned, y = response_demeaned, colour = as.character(participant))) +\r\ngeom_point() + theme_bw() +  theme(legend.position=\"none\")\r\n\r\n\r\n\r\n\r\nLepiej, co nie? Jak więc uwzględnić to w modelu liniowym?\r\nRegresja liniowa ma postać:\r\n\\[ Y_i = \\alpha + \\beta X_i\\] gdzie \\(a\\) to stała, a \\(\\beta\\) to współczynnik regresji.\r\nNasz model musimy zmodyfikować tak by brał pod uwagę średnie amplitudy badanych.\r\n\\[ Y_{ij} = \\alpha + \\beta X_i + b_j z_i\\]\r\ngdzie \\(z_j\\) to zmienna binarna oznaczająca czy dany pomiar należy do badanego \\(j\\), a \\(b_j\\) to współczynnik regresji dla \\(z_j\\). Ponieważ \\(z_i\\) przyjmuje dla badanego \\(j\\) wartość 1, a dla wszystkich innych 0, równanie możemy przepisać:\r\n\\[ Y_{ij} = (\\alpha +b_j) + \\beta X_i\\] Jak widzimy teraz nasz model uwzględnia odchylenia od stałej dla każdego badanego.\r\nZerknijmy na model mieszany:\r\n\r\n\r\nmodel = lmer(response ~ signal + (1| participant), data = data)\r\nmodel_parameters(model)\r\n\r\n\r\n# Fixed Effects\r\n\r\nParameter   | Coefficient |   SE |        95% CI | t(9996) |      p\r\n-------------------------------------------------------------------\r\n(Intercept) |       -0.02 | 0.19 | [-0.40, 0.36] |   -0.09 | 0.926 \r\nsignal      |        0.51 | 0.01 | [ 0.49, 0.52] |   50.43 | < .001\r\n\r\n# Random Effects\r\n\r\nParameter                   | Coefficient\r\n-----------------------------------------\r\nSD (Intercept: participant) |        1.89\r\nSD (Residual)               |        0.10\r\n\r\nW tabeli Fixed Effects mamy estymaty stałej \\(a\\) i współczynnika regresji \\(\\beta\\). Niżej w tabeli Random Effects widzimy estymaty odchylenia standardowego stałych \\(b\\) (wariancji międzyobiektowej) i błędu (w tym wypadku wariancji wewnątrzobiektowej). Jak widzimy współczynnik dla sygnału jest pozytywny i wynosi około 0.5. Dzięki dodaniu dodatkowych stałych \\(b_j\\) nasz model liczy teraz efekt wewnątrzobiektowy.\r\nAle czasami chcielibyśmy by liczył także efekt międzyobiektowy. Spójrzmy na taki przykład:\r\n\r\n\r\n\r\nJak widzimy na wykresie, występuje zarówno efekt międzyobiektowy, jak i wewnątrzobiektowy. Ponadto, te efekty mają przeciwny znak.\r\nPrzykładem z życia takiej sytuacji jest szybkość pisania na klawiaturze. Im szybciej średnio dana osoba pisze na klawiaturze tym rzadziej popełnia błędy. Jednak każda osoba, im relatywnie szybciej (względem swojej średniej) pisze, tym więcej błędów popełnia.\r\nModel mieszany wykryje tylko efekt wewnątrzobiektowy. Jeśli chcielibyśmy wykryć oba efekty, musimy wykonać pewną sztuczkę i rozbić zmienną niezależną na dwie (Bell et al., 2019): \\(X_{between}\\) = średnia dla danego badanego i \\(X_{within} = X - X_{between}\\).\r\n\r\n\r\ndata <- cbind(\r\n  data,\r\n  demean(data, select = c(\"signal\"), group = \"participant\")\r\n)\r\n\r\nmodel = lmer(response ~ signal_within + signal_between + (1| participant), data = data)\r\nmodel_parameters(model)\r\n\r\n\r\n# Fixed Effects\r\n\r\nParameter      | Coefficient |   SE |         95% CI | t(395) |      p\r\n----------------------------------------------------------------------\r\n(Intercept)    |       11.71 | 3.19 | [ 5.44, 17.99] |   3.67 | < .001\r\nsignal within  |        0.93 | 0.05 | [ 0.83,  1.03] |  17.86 | < .001\r\nsignal between |       -1.88 | 0.39 | [-2.65, -1.11] |  -4.81 | < .001\r\n\r\n# Random Effects\r\n\r\nParameter                   | Coefficient\r\n-----------------------------------------\r\nSD (Intercept: participant) |        5.27\r\nSD (Residual)               |        1.00\r\n\r\nDzięki temu nasz model estymuje zarówno efekt wewnątrzobiektowy, jak i międzyobiektowy.\r\nPoniżej możecie zobaczyć jak zmiana poszczególnych parametrów w symulacji (kod z początku wpisu) wpływa na to jak wyglądają dane i jak radzi sobie model mieszany.\r\n\r\n\r\n\r\n\r\nMożliwość estymacji efektów wewnątrzobiektowych i międzyobiektowych to jedna z zalet mieszanych modeli liniowych. Posiadają one jeszcze inne, ciekawe właściwości. Niemniej, tu zakończymy tą małą zachętę do stosowania mieszanych modeli liniowych.\r\nJeśli ktoś jest zainteresowany szczegółami, zarówno matematycznymi jak i bardziej praktycznymi, zachęcam do skorzystania z bazy wiedzy, gdzie znajdują się odnośniki do dobrych tutoriali i kursów omawiających tę klasę modeli.\r\n\r\n\r\n\r\nBell, A., Fairbrother, M., & Jones, K. (2019). Fixed and random effects models: Making an informed choice. Quality & Quantity, 53(2), 1051–1074.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-13-mixedmotivation/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-05-15T08:57:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-05-tranistive/",
    "title": "Większe czy równe?",
    "description": "O nieprzechodności testów statystycznych.",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": {
          "https://revan-tech.github.io/kontakt.html": {}
        }
      }
    ],
    "date": "2022-05-05",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nWspomnę jeszcze nie raz, że nie jestem fanem p-value. Nie dlatego, że jest ona złym sposobem oceny wniosków jakie wyciągamy z danych, ale z powodu tego, jak jest używana i nadużywana. Poruszyłem ten temat tutaj.\r\nDzisiejszy wpis będzie o sytuacji jaką możemy napotkać gdy obcujemy z danymi. Mamy taki zbiór danych z trzema zmiennymi: wartości jakiejś cechy, grupa do której obserwacja należy i unikalny identyfikator. Przeprowadzamy analizę wariancji.\r\n\r\n\r\nlibrary(ez)\r\nlibrary(knitr)\r\nlibrary(broom)\r\n\r\ndata = data.frame(value = c(1,2,3,3,4,5,5,6,7), group = c(\"a\",\"a\",\"a\",\"b\",\"b\",\"b\",\"c\",\"c\",\"c\"), id = 1:9)\r\n\r\nkable(ezANOVA(data = data, dv = value, between = group, wid = id)$ANOVA)\r\n\r\n\r\nEffect\r\nDFn\r\nDFd\r\nF\r\np\r\np<.05\r\nges\r\ngroup\r\n2\r\n6\r\n12\r\n0.008\r\n*\r\n0.8\r\n\r\nIstotny efekt! Wykonujemy więc porównania posthoc.\r\n\r\n\r\nkable(tidy(pairwise.t.test(data$value, data$group), p.adjust.method = 'holm'))\r\n\r\n\r\ngroup1\r\ngroup2\r\np.value\r\nb\r\na\r\n0.0996505\r\nc\r\na\r\n0.0081410\r\nc\r\nb\r\n0.0996505\r\n\r\nZwizualizujmy także efekt grupy.\r\n\r\n\r\nezPlot(data = data, wid = id, between = group, dv = value, x = group)\r\n\r\n\r\n\r\n\r\nMamy do czynienia z sytuacją, w której \\(c\\) jest istotnie większe od \\(a\\), natomiast \\(b\\) nie jest istotnie różne od \\(a\\) i od … \\(c\\). I co teraz? Popularna wykładnia mówi: p-value < 0.05 - efekt jest (średnie się różnią), p-value > 0.05 - efektu nie ma (średnie się nie różnią). Jednak patrząc na wszystkie 3 testy moglibyśmy dojść do wniosku, że efekt zarówno jest jak i go nie ma.\r\nOczywiście nie wydarzyło się tu nic niezwykłego. Różnica średnich pomiędzy \\(c\\) i \\(b\\) oraz \\(a\\) i \\(c\\) jest za mała, by przy tej wariancji wykazać istotny efekt. Wnioski otrzymane za pomocą testów statystycznych są nieprzechodne. Przechodność to taka własność, która mówi nam, że jeśli \\(a<b\\) i \\(b<c\\) to \\(a<c\\).\r\nJak zachować się w takiej sytuacji? Prawdopodobnie najrozsądniejszym wyjściem jest przedstawienie wyniku \\(a\\) i \\(c\\), oraz stwiedzenie, że nie mamy wiedzy, by orzekać o \\(b\\) (patrząc na przedział ufności na wykresie, widzimy, że może być zarówno bliżej \\(a\\) jak i \\(c\\)).\r\nGdybyśmy mieli do czynienia z sytuacją w której \\(a,b,c\\) się nie różnią statystycznie albo \\(a,b\\) się nie różnią statystycznie, a \\(c\\) jest od nich istotnie większe moglibyśmy szybko przedstawić nasze wnioski, bez zbędnej konsternacji.\r\nStwierdzenie, że coś jest istotnie różne jest użyteczną heurystyką tego, że coś jest naprawdę różne, jednak prosty przykład przedstawiony powyżej pokazuje nam, że nie zawsze otrzymane wyniki będą spójne. Warto pamiętać, że p-value jest tylko miarą wskazującą jak bardzo dane nie pasują do danego modelu (w tym przypadku modelu, w którym dana para średnich jest równa).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-05-tranistive/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-05-05T20:36:26+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-28-behrens/",
    "title": "Jak mózg monitoruje zmienność środowiska?",
    "description": "Modelowanie niepewności w procesie uczenia się wartości bodźców.",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": "https://revan-tech.github.io/kontakt.html"
      }
    ],
    "date": "2022-04-28",
    "categories": [
      "Statystyczne Dygresje"
    ],
    "contents": "\r\nModelowanie procesów poznawczych to taka dziedzina, dzięki której możemy sięgnąć do czarnej skrzynki, jaką jest umysł. Jednym procesów, który możemy zamodelować, jest uczenie się. Choć my, ludzie, potrafimy nauczyć się czynności poprzez obserwację, czy nawet werbalny komunikat, dzielimy ze wszystkimi zwierzętami starszy (ale nie mniej przydatny) mechanizm. Mowa o warunkowaniu.\r\nPodstawowa zasada warunkowania jest prosta: Jeśli po wykonaniu jakiejś czynności otrzymujemy rezulatat pozytywny, zwiększa się prawdopodobieństwo powtórzenia tej czynności, jeśli negatywny, zmniejsza się.\r\nJednym z modeli warunkowania jest model Rescola-Wagner (Sutton & Barto, 2018), który wygląda tak:\r\n\r\n\r\n\r\nModel jest bardzo prosty. Przewidywaną wartością bodźca \\(V\\) w czasie \\(i\\) jest wartość bodzica w czasie \\(i-1\\) plus błąd predykcji, czyli różnica pomiędzy rezultatem (np. wartością nagrody), a przewidywaną wartością bodźca w czasie \\(i-1\\) , pomnożoną przez stałą \\(a\\). Od wielkości współczynnika uczenia (learning rate) \\(a\\) zależy jak szybko agent aktualizuje wartość bodźca.\r\nModel Rescola-Wagner jest dość prosty i nie odda wszystkich zjawisk zachodzących u ludzi i zwierząt podczas warunkowania. Jednak jego prostota pozwala szybko zrozumieć ideę stojącą za tą klasą modeli - uczenia ze wzmocnieniem (reinforcement learning models).\r\nTakie modele wykorzystywane są w kognitywistyce i ekonomii (uczenie się i podejmowanie decyzji), a także w biologii (uczenie ze wzmocnieniem to jedna z podstawowych zasad działania neuronów dopaminergicznych) i robotyce. Ja chciałym się skupić na badaniu przeprowadzonym przez zespół Timothego Behrensa, które wyjątkowo mi się spodobało (Behrens et al., 2007).\r\nBadani wykonywali proste zadanie, w którym mieli wybierać wiekokrotnie jedną z dwóch opcji. W pierwszej fazie (120 prób) każda z możliwości była związana ze stałym prawdopodbieństwem uzyskania nagrody. Następnie, w drugiej fazie prawdopodobieństwo uzyskania nagrody zmieniało się co 30-40 prób.\r\nCałą procedurę badani wykonywali w funkcjonalnym rezonansie magnetycznym (fMRI). Dzięki użyciu neurobrazowania, można nie tylko sprawdzić jak dobrze dany model uczenia ze wzmocniem wyjaśnia nam obserwowalne dane (w tym wypadku decyzje badanych), ale możemy sprawdzić czy mózg oblicza wartość bodźca i błąd predykcji w podobny sposób jak model (i jakie ośrodki w mózgu w tym współuczestniczą).\r\nBehrensa i jego zespół zainteresował problem zmienności środowiska. Jeśli środowisko jest stabilne (prawdopodobieństwa przypisane wyborom/opcjom nie zmieniają się), współczynnik uczenia się powinien być niższy, ponieważ nawet jeśli preferowana przez nas opcja akurat nie dostarczy nagrody, jest to raczej wynikiem przypadku. Natomiast jeśli prawdopodobieństwa związane z wyborami zmieniają się (a co za tym idzie zmienia się wartość bodźców), musimy szybko aktualizować wartości bodźców, inaczej z uporem wybieralibyśmy te, których wybranie w przeszłosci dawało często nagrodę.\r\nMoglibyśmy tak zmodyfikować model Rescola-Wagner, by współczynnik uczenia nie był stały, ale zmieniał się w czasie. Uzyskalibyśmy wtedy informację jak dana osoba dostosowuje swój współczynik uczenia. Nie wiedzielibyśmy jednak dlaczego to robi. Wielkość współczynnika uczenia będzie także zależała od innych czynników.\r\nNa przykład, gdy nie mamy wcześniejszych informacji na temat wartości bodźców, współczynnik musi być wysoki, byśmy mogli szybko nabyć jakieś przekonania, na podstawie których podejmiemy decyzję. Natomiast, wraz z postępem czasu powinien maleć (wynika to z chartakterystyki uczenia się - wraz z postępem czasu efekty uczenia maleją).\r\nJeśli chcielibyśmy wiedzieć jak bardzo współczynnik zmienia się w zależności od niestabilności (volatility), a także czy jej monitorowanie jest odzwierciedlone w sygnale fMRI (co interesowało Behrensa), musimy mieć jej jakąś estymatę.\r\nBehrens stworzył więc algorytm - Bayesowskiego Obserwatora, którego zadaniem było przewidywanie prawdopodobieństw i monitorowanie niestabilności. Uzyskał wartość niestabilności w czasie \\(i\\). Dzięki temu odkrył, że w monitorowaniu zmienności środowiska uczestniczy pewien obszar mózgu - przedni zakręt obręczy.\r\nPonieważ lubię się pobawić takimi zabawkami, zaimplementowałem go (kod można znaleźć tutaj). Wygenerowałem zmienną \\(y\\) przyjmującą wartości 0 (brak nagrody) i 1 (nagroda). Przez pierwsze 120 obserwacji prawdopodobieństwo nagrody wynosiło 90%, następnie przez 220 prób prawdopodobieństwo zmieniało się między 80% a 20% co 30 lub 40 prób. W ostatnich 120 obserwacjach prawdopodbieństwo wynosiło 10%.\r\nModel radzi sobie tak:\r\n\r\n\r\n\r\nPrzy pomocy naszego idealnego obserwatora możemy policzyć teraz niestabilność środowiska. Przy odrobinie wysiłku można zaadaptować model do innych sytuacji, bądź rozwinąć go, by używać niestabilności środowiska jako zmiennej w modelach podejmowania decyzji czy uczenia się.\r\nEstymowaną niestabilność środowiska moglibyśmy nazwać też niepewnością pierwszego poziomu. Warto wspomnieć, że istnieje model uczenia ze wzmocnieniem - Hierarchical Gaussian Filter, który uwzględnia niepewność decyzyjną \\(n\\) poziomów (właściwie tyle ile chcemy), czyli modeluje też niestabiność niestabilności - i tak dalej (Mathys et al., 2014).\r\nJak działa model?\r\nTwierdzenie Bayesa\r\nBayesowskie modele opierają się na twierdzeniu Bayesa (lub inaczej mówiąc, wzorze na prawdopodobieństwo warunkowe):\r\n\\[ P(parameters|data) = \\frac{P(data|parameters)P(parameters)}{P(data)}\\] Człon \\(P(data|parameters)\\) (wiarygodność, likehood) oznacza prawdopodobieństwo, że dany parametr wygenerował zaobserwowane dane (czyli na przykład prawdopodbieństwo tego, że jeśli prawdopodobieństwo nagrody wynosi 80%, jakie jest prawdopodobieństwo, że zaobserwujemy 1 - czyli nagrodę).\r\n\\(P(parameters)\\) nazywane prawdopodobieństem a priori, oznacza jakie jest prawdopodobieństwo danego parametru. Często opisuje się go jako wcześniejsze doświadczenie, ponieważ możemy wiedzieć, że wystąpienie jakiejś wartości parametru jest mniej prawdopodobne, niż innej (bądź ma pewnien określony rozkład statystyczny). Jeśli tego nie wiemy, \\(P(parameters)\\) przyjmują taką samą wartość dla wszystkich \\(P(data|parameters)\\).\r\n\\(P(data)\\) jest właściwie tylko parametrem skalującym (stałą), bez specjalnej interpretacji. Dlatego powyższy wzór możemy zapisać:\r\n\\[ P(parameters|data) \\propto P(data|parameters)P(parameters)\\] gdzie \\(\\propto\\) oznacza “jest proporcjonalny do”\r\nWreszcie człon \\(P(parameters|data)\\) jest prawdopodbieństwem a posteriori, czyli prawdopodbieństwem wartości parametru modelu przy zaobserowanych danych (na przykład, jeśli zaobserwowaliśmy 1, jakie jest prawdopodbieństwo, że prawdopodobieństwo otrzymania nagrody wynosi 66%).\r\nProces Markowa\r\nProces Markowa to proces stochastyczny, w którym prawdopodobieństwo zdarzenia w czasie \\(i\\) zależy tylko od stanu systemu w czasie \\(i-1\\). Czyli:\r\n\\[P(X_i = x|X_{i-1}, X_{i-2}  ... X_1) = P(X_i = x|X_{i-1})\\]\r\nObserwator Bayesowski\r\nModel Behrensa można zwizualizować następujący sposób:\r\n\r\n\r\n\\(y_{i+1}\\) czyli nagroda lub jej brak zależy tylko od parametru \\(r_{i+1}\\) - prawdopodobieństwa jej uzyskania. Z kolei parametr \\(r_{i+1}\\) zależy od siebie krok wcześniej \\(r_{i}\\) oraz od niestabilności \\(v_i\\), która sama zależy od \\(v_{i-1}\\) i stałego parametru \\(k\\), który moglibyśmy nazwać niestabilnością niestabilności. Jak widzimy, model Behrensa zakłada proces Markowa (wartości parametrów w czasie \\(i\\) zależą tylko od wartości parametrów w czasie \\(i-1\\)).\r\nSzukamy takich parametrów, które maksymalizują prawdopodobieństwo a posteriori \\(P(r_i,v_i,k|y_i)\\). Dzięki temu będziemy poznamy najbardziej prawdopodobne wartości parametrów \\(r\\), \\(v\\) i \\(k\\).\r\nUżywiając twierdzenia Bayesa, wzór na prawdopodobieństwo a posteriori wygląda tak:\r\n\\[P(r_i,v_i,k|y_i) \\propto \\int \\int P(y_i|r_i)P(r_{i}|r_{i-1},v_i) P(v_{i}|v_{i-1},k) P(r_{i-1},v_{i-1},k|y_{i-1})dv_{i-1} dr_{i-1}\\]\r\nPotrzebujemy do pełni szczęścia jeszcze poszczególnych prawdopodobieństw wystepujących we wzorze.\r\nPrawdopodobieństwo \\(P(y_i|r_i)\\), czyli otrzymanie nagrody, pod warunkiem prawdopodobieństwa \\(r_i\\) wynosi po prostu \\(r_i\\). Możemy więc zamodelować je rozkładem dwumianowym:\r\n\\[P(y_i|r_i) \\sim Binomial(r_i)\\]\r\nPrawdpodobieństwo \\(P(r_{i+1}|r_i,v_i)\\) Behrens zamodelował rozkładem Beta, jednak ja, kierowany lenistwem, zamodelowałem je rozkładem normalnym ograniczonym na przedziale (0,1): \\[P(r_{i+1}|r_{i},v_{i}) \\sim N^{(0,1)}(r_i,e^{v_{i}})\\]\r\nJak widzimy, prawdopodobieństwo \\(r_{i+1}\\) dane jest rozkładem normalnym o średniej \\(r_i\\) i wariancji \\(e^{v_{i+1}}\\). To jak bardzo prawdopodobny jest większy przeskok pomiędzy \\(r_i\\) do \\(r_{i+1}\\) zależne będzie od tego jak duża jest wariancja \\(e^{v_{i+1}}\\). Właśnie ona jest naszą ukrytą zmienną reprezentującą niestabilność.\r\nPrawdopodobieństwo \\(P(v_{i+1}|v_i,k)\\) również zamodelowane jest rozkładem normalnym:\r\n\\[P(v_{i+1}|v_i,k) \\sim N(v_i,e^{k})\\]\r\nCzyli wielkość przeskoków pomiędzy \\(v_{i+1}\\) i \\(v_{i}\\) zależna jest od parametru k - stałego dla wszystkich \\(i\\), który, a jakże, też zamodelowany jest rozkładem normalnym:\r\n\\[P(k) \\sim N(0,10^{10})\\]\r\nDo uzyskania rozkładów a posteriori użyłem metod ABC (Approximate Bayesian Computation), a konkretniej próbkowania Gibbsa (przy użyciu JAGS).\r\nPo więcej detali polecam zajrzeć do orginalnego artykułu.\r\n\r\n\r\n\r\nBehrens, T. E., Woolrich, M. W., Walton, M. E., & Rushworth, M. F. (2007). Learning the value of information in an uncertain world. Nature Neuroscience, 10(9), 1214–1221.\r\n\r\n\r\nMathys, C. D., Lomakina, E. I., Daunizeau, J., Iglesias, S., Brodersen, K. H., Friston, K. J., & Stephan, K. E. (2014). Uncertainty in perception and the hierarchical gaussian filter. Frontiers in Human Neuroscience, 8, 825.\r\n\r\n\r\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (pp. 346–349). MIT press.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-28-behrens/Statystyczne_Dygresje2.jpg",
    "last_modified": "2022-05-04T21:39:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-15-seria-jak-rozumie-nauk-metodologia-bada-i-statystyka/",
    "title": "Metodologia badań i Statystyka",
    "description": "Seria: Jak rozumieć naukę?",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "Refleksja"
    ],
    "contents": "\r\n\r\nContents\r\nStatystycznie rzecz biorąc\r\nJak zbadać wszystich Polaków?\r\nRóżnią się czy nie?\r\nKrysys Replikacyjny\r\n\r\nZwykle kursy z metodologii w szkołach wyższych zaczyna się od filozofii nauki. Wprowadza ona należyty kontekst, historię refleksji naukowej. Przedstawia się rolę teorii i modeli w procesie wyjaśniania rzeczywistości. Następnie wyjaśnia się założenia i metody stosowane w dziedzinie nauki, którą ktoś akurat studiuje. My te i inne tematy odłożymy sobie na potem, a serię jak rozumieć naukę zaczniemy od przyjrzenia się statystyce i metodologii badań.\r\nStatystycznie rzecz biorąc\r\nStatystyka to dziedzina nauki pozwalająca na formalne (to znaczy w języku matematyki) przedstawienie interesujących nas zjawisk. Często jest przedmiotem znienawidzonym przez studentów. Uznawana za nudną i trudną, często uczona przez wykładowców, którzy sami ją za taką uważają, co zdecydowanie nie pomaga w jej przyswajaniu. Zdecydowaliśmy się zacząć od statystyki, ponieważ jest ona wykorzystywana właściwie we wszystkich naukach empirycznych (choć niektórzy fizycy zapewne z pogardą pokręciliby głową). Dobrze podsumował to matematyk John Tukey: „Najlepszą rzeczą w byciu statystykiem jest to, że możesz grzebać w podwórku u wszystkich innych” (Lin et al., 2014).\r\nZacznijmy więc zagłębiać się w meandry statystyki. Najpierw musimy wykonać pomiar. Pomiar oznacza przypisanie cesze pewnego obiektu wartości liczbowej. Np. ile waży człowiek w kilogramach, na kogo zagłosuje w najbliższych wyborach lub jaki jest poziom jego otwartości na nowe doświadczenia. Następnie możemy sprawdzić czy pomiędzy mierzonymi zmiennymi zachodzi jakiś związek. Na przykład, gdybyśmy przyjrzeli się danym dotyczącym wieku i wzrostu ludzi do 21 roku, zauważylibyśmy, że im więcej ktoś ma lat, tym jest wyższy. Związek, w którym wartość jednej zmiennej wzrasta i wartość drugiej zmiennej także wzrasta (bądź maleje) nazywamy korelacją liniową.\r\nIstnieją oczywiście inne, bardziej subtelne związki pomiędzy zmiennymi, ale na razie zostańmy przy relacji liniowej. Pewną mantrą, którą słyszy się na zajęciach ze statystyki jest: „korelacja nie oznacza przyczynowości”. Weźmy taki przykład: okazuje się, że im więcej sprzedaje się lodów tym częściej ludzi atakują rekiny. Czy spożycie lodów w jakiś sposób zachęca rekiny do ataku? Oczywiście nie, pozorny związek pomiędzy tymi dwoma zjawiskami wyjaśnia fakt, że ludzie najczęściej kupują lody w upalne dni, a w upalne dni więcej ludzi odpoczywa na plaży, co przekłada się na częstsze ataki rekinów.\r\nGdy mamy więc dane obserwacyjne (nasze wyniki pomiarów) nie możemy mówić o przyczynowości, gdyż widzimy tylko współzmienność cech obiektów, które badamy. By wnioskować o przyczynowości musimy posłużyć się eksperymentem. W warunkach kontrolowanych dzielimy losowo badane przez nas obiekty na dwie grupy (bądź więcej), zapewniamy im możliwie jak najbardziej zbliżone warunki, a następnie w jednej z nich (zwanej grupą docelową) wykonujemy jakąś interwencję X (np. podajemy lek), a w drugiej grupie (nazywanej kontrolną) nie. Następnie w obu grupach wykonujemy pomiar. Jeśli pomiary w grupach się różnią, zakładamy, że różnicę mogła spowodować tylko interwencja X.\r\nCzęsto jednak nie możemy posłużyć się eksperymentem, bo na przykłąd: jego koszt jest zbyt wysoki, wykonanie byłoby nieetyczne, lub jest to po prostu niemożliwe. Wtedy, by odnaleźć przyczynowość, posługujemy się dodatkowymi metodami, takimi jak wykorzystanie istniejących teorii, zgodność z modelem, wyeliminowanie innych potencjalnie wpływających zmiennych, modelami zwierzęcymi czy obserwacją następstw czasowych w badaniach podłużnych. Żadna z tych metod nie daje jednak takiej pewności we wnioskowaniu o przyczynowości jak eksperyment. Pewną nadzieję na rozwiązanie tego problemu dają rozwijane od kilkunastu lat metody matematyczne pozwalające na wnioskowanie o przyczynowości z danych obserwacyjnych. Zainteresowanym tym tematem polecam książkę „Przyczyny i Skutki” Jude’a Pearl’a i Dany Mackenzie (Pearl & Mackenzie, 2021).\r\nJak zbadać wszystich Polaków?\r\nZajmijmy się teraz kolejnym istotnym problemem, mianowicie, jak możemy uogólniać (generalizować) wyniki badań. Załóżmy, że chcemy powiedzieć coś o wadze w populacji Polaków, więc przeprowadzamy badanie. Mierzymy wagę dziesięciu ludzi napotkanych na ulicy, ale od razu napotykamy problem. Otrzymaliśmy dokładne informacje o wadze tylko tych 10 ludzi. Skąd mamy pewność, że ich średnia waga jest jakkolwiek zbliżona do średniej w populacji? Może należałoby zwiększyć próbę? Sto, może tysiąc osób wystarczyłoby aby uzyskać odpowiednie przybliżenie. Taką logiką kierował się magazyn „Literary Digest”, który przeprowadził sondaż w wyborach prezydenckich w Stanach Zjednoczonych w 1936 roku. Próba wynosiła ponad dwa miliony ludzi. W sondażu zwyciężył Alf Landon z 57% poparciem. Jednak w wyborach zwyciężył Franklin Delano Roosevelt z 61% poparciem, gdy Alf Landon otrzymał jedynie 8% głosów. Jak to możliwe? „Literary Digest” losował respondentów z książek telefonicznych. Jednak w 1936 roku telefon posiadały raczej osoby zamożne, dlatego wyniki sondażu były bardzo skrzywione. Większość ludzi w dobie największego kryzysu ekonomicznego w Stanach, głosowało na prosocjalne reformy Roosevelta (Babbie, 2008).\r\nA jednak dzisiejsze sondaże są zadziwiająco dokładne. Jak się to dzieje? Otóż zachowanie matematycznych obiektów, a takimi są nasze pomiary, można policzyć. Jeśli każdy obiekt w populacji (np. Polaków) ma niezerowe, takie samo prawdopodobieństwo wylosowania do próby, możemy policzyć ile osób musimy wylosować, by z prawdopodobieństwem X wylosować próbę, w której średnia wartość cechy nie będzie różna więcej niż Y od średniej wartości cechy w populacji.\r\n\r\nUmożliwia nam to jedno z najważniejszych twierdzeń w statystyce: Centralne Twierdzenie Graniczne.\r\nW praktyce wygląda to tak, że gdybyśmy chcieli mieć 95% szansę na wylosowanie próby, w której procent respondentów głosujących na daną partię, nie różni się więcej niż o 3% od realnego poparcia w społeczeństwie, powinniśmy wylosować 1067 osób do próby. Ponieważ każdy Polak musi mieć szansę na wylosowanie, próbę powinniśmy losować ze zbioru, który zawiera wszystkich Polaków, na przykład zbioru numerów PESEL.\r\nPróbę, do której obiekty zostały przydzielone w sposób losowy z całej populacji nazywamy próbą reprezentatywną. Próbę w której wszystkie obiekty zostały wylosowane z takim samym prawdopodobieństwem nazywamy doborem prostym losowym. Istnieją jeszcze inne metody doboru próby reprezentatywnej. Wszystkie łączy to, że możemy dokładnie policzyć margines błędu dla wyników.\r\nSpotkałem się kiedyś z opinią, że jeśli ankieter stanie w centrum miasta i będzie podchodził do niektórych ludzi i prosił ich o wypełnienie ankiety, mamy do czynienia z losowym doborem do próby. Ankieter przecież nie zna tych ludzi, nie może więc dobierać ich sobie według ich poglądów. Niemniej, ludzie Ci znaleźli się w tym konkretnym miejscu, o tej konkretnej godzinie w sposób nieprzypadkowy. Przy uczelni będzie więcej studentów, w godzinach szczytu będzie więcej osób zmierzających do pracy, i tak dalej.\r\nA jednak wśród badań naukowych próby reprezentatywne to znaczna mniejszość. Jest to spowodowane wieloma czynnikami. Przede wszystkim, przeprowadzenie badania reprezentatywnego jest kosztowne i trudne. Uzyskanie dostępu do listy zawierającej dane wszystkich obywateli wymaga przejścia wielu czasochłonnych procedur. Po drugie, o ile w wypadku ludzi istnieje jakaś lista, w przypadku badań nad zwierzętami nic takiego nie ma. Biolodzy nie mają skąd wylosować reprezentatywnej próby szczurów. Chemicy i fizycy mają w tym względzie nieco łatwiej. Atom wodoru na wsi będzie identyczny z atomem wodoru w mieście.\r\nBy poradzić sobie z tym problemem stosuje się różne metody. W dniu wyborów, po zakończeniu zbierania głosów, zwykle o godzinie 21 ogłasza się sondażowe wyniki wyborów, metodą exit poll. Badanie to nie jest przeprowadzone na próbie reprezentatywnej, ponieważ ankieterzy muszą zadać pytanie o oddany głos zaraz po wyjściu wyborcy z lokalu wyborczego. A jednak znowu mamy do czynienia z niezwykłą dokładnością. Badacze wyszczególniają zmienne silnie skorelowane z preferencjami wyborczymi, na podstawie wcześniejszych badań reprezentatywnych. Takie zmienne to zwykle płeć, wielkość miejscowości zamieszkania, poziom wykształcenia czy wielkość dochodów. Respondenci są dobierani tak, by procentowo liczba osób wykształconych odpowiadała tej w populacji itd. Przy poprawnie dobranych założeniach pozwala to na dokładne oszacowanie wyników wyborów. Biolodzy współcześnie w badaniach nad zwierzętami podobnie starają się brać pod uwagę wewnątrzgatunkowe zróżnicowanie genetyczne, środowisko zwierzęcia i jego historię, by stworzyć próbę, której wyniki będą dały się generalizować (Farrar et al., 2021).\r\nJednocześnie w niereprezentatywnych próbach należy uznać, że wpływ pewnych potencjalnych źródeł zmienności jest zaniedbywalny, to znaczy nie ma znaczenia dla wyniku badań. Na przykład, że mechanizm molekularny skurczu mięśnia poprzecznie prążkowanego jest niezależny od tego czy ktoś mieszka w Warszawie czy San Francisco. Jednak czasami takie założenia są błędne. Psychologowie długo twierdzili, że badają uniwersalne mechanizmy zachowania i myślenia ludzkiego, co miało uzasadniać mało zróżnicowane próby składające się głównie ze studentów psychologii (obecnie w ramach psychologii międzykulturowej zwraca się uwagę na różnice wynikające z kultury) (Hanel & Vione, 2016). Problemy z doborem próby miały także inne dziedziny nauki, jak biologia i medycyna, co jest jednym z powodów kryzysu replikacyjnego, który omówimy w dalszej części tego tekstu.\r\nRóżnią się czy nie?\r\nNo dobrze. Jak zauważyliśmy, dobór próby do badania to nietrywialna sprawa. Załóżmy jednak, że dobraliśmy naszą próbę odpowiednio i teraz chcemy powiedzieć czy średnia waga mężczyzn i kobiet się różni. Nie wystarczy jednak tylko spojrzeć na średnią mężczyzn i kobiet z naszej próby i zobaczyć, która średnia jest większa. Ponieważ losowaliśmy naszych badanych, nawet jeśli średnia waga w populacji jest taka sama dla kobiet i mężczyzn, prawie na pewno nie otrzymamy takich samych średnich w naszej próbie (zawsze będzie między nimi jakaś różnica). Jak więc określić czy otrzymane wyniki w próbie istotnie się różnią w populacji, czy różnica jest wynikiem błędu losowego.\r\nI tu statystycy opracowali kolejną ciekawą sztuczkę. Na podstawie wyników otrzymanych w próbie jesteśmy w stanie oszacować prawdopodobieństwo tego, że jeśli w populacji mężczyźni i kobiety nie różnią się wagą, to na ile jest prawdopodobne wylosowanie próbek różniących się o określoną wartość. Jeśli jest to wysoce nieprawdopodobne (zwykle przyjmuje się, że to prawdopodobieństwo wynosi mniej niż 5%) przyjmuje się to jako argument, że różnica rzeczywiście istnieje w populacji. Prawdopodobieństwo to nazywa się wartością p (p-value), a jeśli jest mniejsze od przyjętego progu (dopuszczalnego prawdopodobieństwa otrzymania wyniku fałszywie pozytywnego), wynik nazywamy istotnym statystycznie.\r\nP-value możemy obliczyć dla większości hipotez, nie tylko o różnicy średnich, np. czy zmienne są skorelowane liniowo, szacując prawdopodobieństwo otrzymania wyniku w próbie, który w populacji nie istnieje. Nie jest to jedyny sposób weryfikacji hipotez statystycznych, niemniej z powodów historycznych oraz faktu, że obliczenie p-value jest stosunkowo proste, jest to najczęstsza metoda stosowana w praktyce.\r\nNiemniej wokół użycia p-value narosło wiele kontrowersji. Wynika to między innymi z automatycznego korzystania z tej miary przez naukowców traktujących statystykę jako mechaniczny sposób na wskazanie czy badanie potwierdza daną tezę. Z jednej z możliwości weryfikacji hipotez statystycznych p-value stała się obowiązjuącym standardem w badaniach, choć nie zawsze jej użycie jest najbardziej adekwatne (Gigerenzer, 2004).\r\nPrzeszliśmy od doboru próby do prostej analizy statystycznej otrzymanych wyników. Czy to jednak zawsze wystarczy by otrzymać rzetelną informację na temat interesującego nas efektu? Niestety sprawa jest bardziej skomplikowana. Rozpatrzmy następujący przykład, przeprowadzamy badanie na podstawie danych obserwacyjnych w którym chcemy określić wpływ aktywności fizycznej na gęstość kości. Liczymy wskaźnik korelacji liniowej, jednak ku naszemu zdziwieniu analiza pokazuje brak związku, choć z innych badań wiemy, że wraz ze wzrostem częstotliwości uprawiania sportu, gęstość kości powinna rosnąć.\r\nUżywamy więc innej metody statystycznej – regresji liniowej, która pozwala na uwzględnienie wpływu więcej niż jednej zmiennej na zmienną, która nas interesuje (gęstość kości). Do analizy używamy teraz nie tylko częstotliwości aktywności fizycznej, ale także wagę w kilogramach. Okazuje się, że obydwie zmienne są pozytywnie skorelowane z gęstością kości (im większa częstotliwość aktywności fizycznej bądź waga, tym większa gęstość kości). Jednak jeśli analizować zmienne z osobna, nie wykryjemy związku. Dzieje się tak dlatego, że częstotliwość aktywności fizycznej i waga są ze sobą negatywnie skorelowane (im częstsza aktywność, tym mniejsza waga). Oznacza to, że większa gęstość kości u osób aktywnych jest równoważona przez większą gęstość kości u osób ważących więcej, przez co wydaje się, na pierwszy rzut oka, że aktywność fizyczna nie ma związku z gęstością kości.\r\nZauważmy, że gdybyśmy przeprowadzili eksperyment, nie spotkalibyśmy się z podobnym problemem, ponieważ tylko jedna grupa uprawiałaby wzmożoną aktywność fizyczną, co zwiększyłoby średnią gęstość kości. Choć dane obserwacyjne nie pozwalają nam mówić o przyczynowości, za pomocą metod statystycznych jesteśmy w stanie odseparować wpływ poszczególnych zmiennych.\r\nNa koniec tej części warto wspomnieć o interpretacji wyników analizy statystycznej. Ponownie załóżmy, że przeprowadzamy eksperyment, w którym interesuje nas wpływ alkoholu na zachowania agresywne. Dobieramy próbę, grupie docelowej podajemy alkohol, a następnie mierzymy częstotliwość zachowań agresywnych. Podczas analizy naszych wyników widzimy, że średnia częstotliwość zachowań agresywnych jest istotnie wyższa w grupie, której podaliśmy alkohol. Z tego punktu wiedzie prosta droga do wniosku, że alkohol powoduje agresję.\r\nNie jest to jednak wniosek poprawny. Jeśli dołączymy do analizy zmienną wskazującą na tendencję do zachowań agresywnych, okaże się, że częstotliwość agresji wzrasta po alkoholu, ale proporcjonalnie do wcześniej istniejącej skłonności do agresji (Chiavegatto et al., 2010). Otrzymaliśmy zgoła inny wniosek, mówiący nam, że alkohol działa jako wyzwalacz agresji, jednak jej nie powoduje. Podczas przeprowadzania badań należy pamiętać o tym, że wniosek statystyczny (średnia ilość zachowań agresywnych wzrasta), jest czymś innym od wniosku interpretacyjnego (alkohol powoduje agresję).\r\nKrysys Replikacyjny\r\nZwróćmy uwagę teraz na większy obraz wynikający z naszych rozważań nad metodologią statystyczną. Załóżmy, że w idealnym świecie, gdzie badacze idealnie przygotowują swoje eksperymenty na doskonałych próbach, średnio 5% badań, w których badany efekt nie istnieje, będzie zawierało fałszywe wyniki. Czyli jeśli na temat jakiegoś zjawiska pojawiło się wystarczająco dużo badań, to zawsze znajdziemy artykuł przedstawiający dane za, jak i przeciw danej tezie. Dlatego jeśli ktoś przedstawia nam badanie popierające jakąś tezę, niekoniecznie oznacza to, że ma rację. W nauce ważne jest gromadzenie (akumulacja) dowodów i powtarzanie (replikacja) badań. Duże role odgrywają w tym metaanalizy czyli wtórna analiza statystyczna wyników wielu badań w celu określenia istotności dowodów przemawiających za danym zjawiskiem.\r\n\r\nUpraszczam tu sprawę, ponieważ mówię tylko o prawdopodobieństwie wyników fałszywie pozytywnych, a należałoby uwzględnić także sytuacje, w których nie wykryto efektu faktycznie istniejącego, czyli fałszywych wynikach negatywnych. Realnie prawdopodobieństwo otrzymania wyników nieprawdziwych jest większe.\r\nNo właśnie. Tak by było w idealnym świecie. Jednak realnie mamy do czynienia z efektem złudzenia publikacyjnego, czyli skłonności wydawców do publikacji artykułów zawierających wyniki świadczące o istnieniu efektu. Brak efektu nie jest sexy, bo w badaniu nie wyszło nic interesującego. Doprowadziło to sytuacji, w której badania przedstawiające jakiś efekt były znacznie częściej publikowane niż te, które go nie pokazywały. A tak jak zauważyliśmy wcześniej zawsze pewien odsetek badań fałszywie pokaże nam istnienie efektu, którego nie ma. Może prowadzić to do sytuacji, w której dobrze udokumentowany efekt faktycznie nie istnieje. Za przykład może posłużyć zjawisko zagrożenia stereotypem z psychologii społecznej polegające na obniżeniu sprawności wykonywania zadania przez osoby należące do grupy objętej negatywnym stereotypem, gdy wcześniej „przypomni” im się o istnieniu stereotypu. Na przykład jeśli kobietom (niekoniecznie wprost) powie się, że kobiety są gorsze z matematyki, a następnie przeprowadzi się test, okaże się, że wypadły gorzej niż mężczyźni.\r\nMimo licznych badań potwierdzających ten efekt, metaanalizy przeprowadzane od 2015 roku wykazały istnienie złudzenia publikacyjnego (Flore & Wicherts, 2015). Wykrycie złudzenia publikacyjnego jest możliwe dzięki temu, że wyniki badań (w formie liczb) powinny zachowywać się (statystycznie) w określony sposób. Od kilkunastu lat rozwijają się metody statystyczne pozwalające na wykrywanie tendencyjności w publikowaniu artykułów. Złudzenia publikacyjne stanowią jeden z problemów składających się na wspomniany wcześniej kryzys replikacyjny.\r\nW 2015 roku w prestiżowym czasopiśmie naukowym „Science” ukazały się wyniki badania, w którym autorzy powtórzyli 100 nigdy wcześniej niereplikowanych badań psychologicznych. Tylko w 39% z nich udało się otrzymać wynik taki jak w oryginalnych publikacjach (Open Science Collaboration, 2015). To i podobne badania, wraz z rosnącą świadomością złudzeń publikacyjnych, zapoczątkowało debatę na temat kryzysu replikacyjności i rzetelności współczesnych praktyk naukowych. Szybko okazało się, że problem nie dotyczy tylko psychologii, lecz również biologii, ekonomii i medycyny.\r\nKryzys replikacyjny ma wiele źródeł, jednak najczęściej wymienia się zjawisko “publikuj lub zgiń” (publish or perish). Współczesna polityka ewaluacji pracownika naukowego i jego dorobku opiera się na publikacjach. Im więcej jest publikacji, im częściej są cytowane przez innych badaczy, w im lepszych czasopismach są publikowane, tym lepiej. Od wyniku ewaluacji zależy zatrudnienie i finansowanie badań naukowca. Presja publikacyjna wywierana na badaczy w połączeniu z tendencyjnością publikowania tylko “interesujących” wyników sprawiła, że niekiedy popełniają oni pewne metodologiczne nadużycia. Jakie to nadużycia? Pomoże nam to wyjaśnić martwy łosoś w rezonansie magnetycznym.\r\nZespół neuronaukowców pod kierownictwem Craiga Bennetta przygotowywał się do przeprowadzenia badania dotyczącego przetwarzania w mózgu emocjonalnych zdjęć przy użyciu funkcjonalnego rezonansu magnetycznego (fMRI). Urządzenie to działa jak trójwymiarowa kamera pozwalająca na rejestrowanie zmian w natężeniu pola magnetycznego w voxelach (trójwymiarowych odpowiednikach pikseli). W zależności od tego czy krew jest utlenowana czy odtlenowana ma inne właściwości magnetyczne, pozwala to sprawdzać poziom utlenowania krwi w różnych obszarach mózgu. Im większą aktywność wykonuje dany region mózgu, tym więcej tlenu zużywa. Pozwala to na sprawdzenie jakie obszary mózgu są bardziej aktywne w różnych warunkach eksperymentalnych.\r\nBennett przed wykonaniem właściwych badań zdecydował się na przetestowanie procedury wkładając do skanera trzykilogramowego martwego łososia. W trakcie pomiaru wyświetlano na specjalnym ekranie zdjęcia, tak jak miałoby to miejsce podczas prawdziwego badania, po czym zadawano łososiowi pytania, jakie emocje prezentują osoby na zdjęciach. Po przeanalizowaniu danych okazało się, że mózg martwego łososia wykazuje zwiększoną aktywność pewnych regionów podczas oglądania zdjęć, niż w spoczynku (Bennett et al., 2009).\r\nWynikało to ze zjawiska znanego w statystyce jako problem wielokrotnych porównań. W badaniach przy użyciu fMRI porównuje się aktywność w każdym voxelu pomiędzy warunkami. Voxeli w skanie fMRI jest kilkaset tysięcy. Wykonując tyle porównań prawdopodobieństwo, że wyjdzie nam choć jeden wynik fałszywie pozytywny wynosi niemal 100%. Prawdopodobieństwo, że w każdym pojedynczym porównaniu wystąpi wynik fałszywie pozytywny nadal wynosi 5%, ale jeśli średnio 5% wokseli pokaże nam różnice pomiędzy warunkami, to wykryjemy aktywność nawet u martwego łososia (przez szum wynikający z fluktuacji pola magnetycznego). By poradzić sobie z tym problemem stosuje się współcześnie odpowiednie poprawki, które utrzymują niższe prawdopodobieństwo otrzymania wyniku fałszywie pozytywnego.\r\nCo ma więc martwy łosoś do zjawiska “publikuj lub giń?” Naukowcy w swoich badaniach zwykle zbierają więcej danych, niż te które są potrzebne do weryfikacji a priori postawionych hipotez (pytań badawczych, które stanowiły motywację do przeprowadzenia badania). Dodatkowe dane zbiera się w celu kontrolowania wpływu zmiennych potencjalnie wpływających na efekt, jak wspominaliśmy powyżej. Ponadto, z uwagi, że zwykle przeprowadzenie badania jest kosztowne i czasochłonne zbiera się jak najwięcej danych w celach eksploracyjnych (dodatkowych analiz nie wynikających bezpośrednio z hipotez). Presja publikacyjna, razem z tendencją wydawnictw do publikowania artykułów potwierdzających efekt niż mu zaprzeczających, sprawia, że jeśli badacze nie potwierdzą swoich założonych hipotez, szukają w danych jakiegoś istotnego efektu, by zwiększyć szansę na publikację.\r\nW tym momencie powracamy do problemu wielokrotnych porównań. Istnieją oczywiście odpowiednie statystyczne środki zaradcze, niestety czasami badacze by zwiększyć sexapill artykułów przedstawiają uzyskane istotne wyniki tak, jakby odpowiadały na ich pytanie badawcze postawione przed jego przeprowadzeniem badania. Pomijają także w tekście analizy, które nie wykrywały efektu. Nieodpowiedni dobór próby, tendencyjność publikacyjna i presja kładziona na badaczy składa się na kryzys replikacyjny. Od kilku lat stosuje się coraz częściej środki zaradcze w postaci częstszej publikacji replikacji, czasopism, które publikują tylko artykuły mówiące o braku efektu czy prerejestracji - przedstawieniu do wiadomości publicznej w formie krótkiego artykułu, hipotez badawczych i metodologii przed zaczęciem badania.\r\nCzy kryzys replikacyjny oznacza, że nie możemy ufać nauce? Nie. Nauka nadal pozostaje najlepszym narzędziem do poznawania otaczającego nas świata i jest procesem samokrytycznym, to znaczy metody i założenia stosowane w nauce są poddawane ciągłej krytyce przez środowisko naukowe. Należy tu dodać, że kryzys replikacyjny może być związany z umasowieniem nauki. Liczba osób otrzymujących doktoraty stale się zwiększa, i to w państwach, w których liczba ludności spada. Standaryzacja metod oceny pracowników naukowych jest potrzebna, jednak w obecnej formie może nasilać omawiany problem.\r\nNa tym kończymy pierwszy wpis serii “Jak rozumieć naukę.” Dotknęliśmy tylko wierzchołka góry lodowej jeśli chodzi o zagadnienia statystyczne i metodologiczne w nauce, jednak mam nadzieję, że ten tekst będzie stanowił interesujące wprowadzenie do zagadnień metodologicznych. Osobom zainteresowanym poszerzeniem swojej wiedzy polecamy przejrzenie zasobów w bazie wiedzy.\r\n\r\n\r\n\r\nBabbie, E. (2008). Podstawy badań społecznych (pp. 209–210). PWN.\r\n\r\n\r\nBennett, C. M., Miller, M. B., & Wolford, G. L. (2009). Neural correlates of interspecies perspective taking in the post-mortem atlantic salmon: An argument for multiple comparisons correction. Neuroimage, 47(Suppl 1), S125.\r\n\r\n\r\nChiavegatto, S., Quadros, I., Ambar, G., & Miczek, K. (2010). Individual vulnerability to escalated aggressive behavior by a low dose of alcohol: Decreased serotonin receptor mRNA in the prefrontal cortex of male mice. Genes, Brain and Behavior, 9(1), 110–119.\r\n\r\n\r\nFarrar, B. G., Voudouris, K., & Clayton, N. S. (2021). Replications, comparisons, sampling and the problem of representativeness in animal cognition research. Animal Behavior and Cognition, 8(2), 273.\r\n\r\n\r\nFlore, P. C., & Wicherts, J. M. (2015). Does stereotype threat influence performance of girls in stereotyped domains? A meta-analysis. Journal of School Psychology, 53(1), 25–44.\r\n\r\n\r\nGigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587–606.\r\n\r\n\r\nHanel, P. H., & Vione, K. C. (2016). Do student samples provide an accurate estimate of the general public? PloS One, 11(12), e0168354.\r\n\r\n\r\nLin, X., Genest, C., Banks, D. L., Molenberghs, G., Scott, D. W., & Wang, J.-L. (2014). Past, present, and future of statistical science (p. 44). CRC Press.\r\n\r\n\r\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251).\r\n\r\n\r\nPearl, J., & Mackenzie, D. (2021). Przyczyny i skutki rewolucyjna nauka wnioskowania przyczynowego. Copernicus Center Press.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-seria-jak-rozumie-nauk-metodologia-bada-i-statystyka/naser-tamimi-yG9pCqSOrAg-unsplash.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-13-o-odpowiedzialnoci-za-czyny-w-dobie-lotw-na-marsa/",
    "title": "O odpowiedzialności za czyny w dobie lotów na Marsa",
    "description": "Opowieść o wolnej woli",
    "author": [
      {
        "name": "Szymon Mąka",
        "url": {}
      }
    ],
    "date": "2021-09-13",
    "categories": [
      "Refleksja"
    ],
    "contents": "\r\n\r\nContents\r\nBezwolne maszyny\r\nZłudzenie konieczne\r\nNie ma odpowiedzialności, nie ma kary\r\nEpilog\r\n\r\nW Polsce (i na świecie) nie brakuje przykładów postaw społecznych sprzecznych współczesnej wiedzy naukowej. Za przykład mogą posłużyć postawy anty-GMO, anty-szczepieniowe, katastrofa klimatyczna, które są prawdopodobnie najbardziej znane. Jednak poza nimi wiele aspektów państwa i społeczeństwa funkcjonuje “obok” refleksji naukowej. Przyjrzyjmy się mniej popularnemu zagadnieniu - problematyce wolnej woli.\r\nBezwolne maszyny\r\nW XIX wieku fizyka święciła triumfy, zwłaszcza w dziedzinach elektryczności i magnetyzmu. Émilie du Châtelet i James Joule niezależnie sformułowali zasadę zachowania energii. Michael Faraday odkrył zjawisko indukcji elektromagnetycznej, Georg Ohm opisał związek pomiędzy natężeniem a napięciem prądu elektrycznego. James Maxwell połączył elektryczność i magnetyzm w jedną dziedzinę - elektromagnetyzm. Emil du Bois-Reymond odkrył, że informacja w układzie nerwowym przekazywana jest przez sygnały elektryczne. XIX wiek obfitował również w nowe wynalazki powstałe dzięki rozwijającej się wiedzy, m.in.: oświetlenie elektryczne, kolej parowa, telegram, telefon czy pierwsze aparaty fotograficzne.\r\nNiezwykłe dokonania naukowe oraz zdolność fizyki do dokładnego przewidywania zjawisk przyrodniczych zmusiła współczesnych do refleksji nad naturą rzeczywistości. Według ówczesnej wiedzy, rzeczywistość fizyczna rządzona była przez stałe i uniwersalne prawa fizyki. Znając je i dysponując odpowiednią aparaturą, można było przewidzieć z absolutną dokładnością dowolne zjawisko fizyczne, pomniejszoną tylko o błąd pomiaru.\r\nPopularną metodą rozwiązywania sporów jest rzut monetą. To prosty, a przede wszystkim losowy sposób na decyzję pomiędzy dwoma możliwościami. Jednak zgodnie z prawami fizyki klasycznej, losowość rzutu monetą nie wynika z jakiejś rzeczywistej losowości. Przypadek jest tylko niedostatkiem informacji. Biorąc pod uwagę wszystkie parametry fizyczne: siłę wyrzutu, prędkość kątową, wiatr, grawitację etc, można by obliczyć czy moneta upadnie awersem czy rewersem do dołu. Fizycy twierdzili, że wszystkie zdarzenia w świecie fizycznym, a więc i działanie człowieka, są powiązane związkiem przyczynowo-skutkowym - to znaczy, obecne zdarzenia miały być zdeterminowane przez zdarzenia przeszłe.\r\nPierre-Simon Laplace w roku 1814 dokonał pewnego eksperymentu myślowego: jeśli wyobrazimy sobie istotę - demona, który zna wszystkie obecne parametry fizyczne każdego atomu we wszechświecie, będzie on w stanie przewidzieć wszystko co się zdarzy w przyszłości. Oznaczałoby to, że wszechświat jest całkowicie zdeterminowany, istnieje tylko jedna przyszłość, do której wszyscy zmierzamy, a nasze działania są tylko skutkami zdarzeń z dalekiej przeszłości.\r\nTaki obraz świata nie był przychylny koncepcji wolnej woli - idei ciężkiej do zdefiniowania, a w uproszczeniu oznaczającej, że człowiek, pomijając ograniczenia fizyczne, jest w stanie całkowicie niezależnie decydować o swoich czynach. Jedyną przyczyną jego zachowania jest świadoma decyzja,która sama nie ma przyczyny - przynajmniej w świecie fizycznym. Termin ten ma silne ugruntowanie zarówno w tradycji chrześcijańskiej, jak i filozoficznej. Kartezjańska opozycja umysłu i materii na lata zakorzenił się tak w myśli powszechnej jak i naukowej. Obie tradycje odwołują się do niematerialnego bytu - duszy bądź umysłu, który nieograniczony zasadami obejmującymi materię jest zdolny do ‘’wolnej’’ decyzji. Choć współcześnie rzadziej widać odwołania do niematerialnego umysłu, wciąż widać wpływ dualizmu kartezjańskiego np. w opozycji myślenia racjonalnego i emocji (Damasio, 2011). Idea wolnej woli jest dla nas naturalna i większość z nas doświadcza poczucia sprawczości objawiającego się przekonaniem, że to my jesteśmy przyczyną działania np. ruchu ręki.\r\nKoncept wolnej woli krytykowany był również od strony filozoficznej. Artur Schopenhauer zauważył, że ludzie w swojej naiwności dowodzą wolności swej woli twierdząc: “mogę robić co chcę.” Schopenhauer zapytał wtedy: skoro można robić co się chce, czy można chcieć co się chce? Nawet jeśli przyjęlibyśmy odpowiedź twierdzącą, pytanie można rozszerzyć do: „czy można chcieć tego, co chce się chcieć?”. W ten sposób Schopenhauer pokazał, że nie mamy wpływu na nasze ‘’chcenia’’, które są przyczyną naszych działań (Schopenhauer, 1991).\r\nOdkrycie fizyki kwantowej, że rzeczywistość na poziomie subatomowym jest probabilistyczna, zachwiała wiarą w determinizm fizyczny. Była to idea tak niepojęta dla wielu fizyków, że, jak stale powtarza fizyk Andrzej Dragan w swoich wykładach, wielu odkrywców efektów kwantowych nie wierzyło w swoje odkrycia aż do śmierci. Sam problem losowości i determinizmu jest niezwykle ciekawy. Badania wykorzystujące twierdzenie Bella wykazują, że stan kwantowych cząstek splątanych nie wynika z ich historii, tzn. stanu jaki miały wcześniej, lecz w sposób probabilistyczny objawia się podczas pomiaru. Eksperymenty nie dają całkowitej pewności (jak to zwykle bywa w nauce) o istnieniu losowości na kwantowym poziomie. Wynika to z problemów metodologicznych, takich jak: możliwe wady eksperymentu czy niepewności pomiarowe. Niemniej, wyniki tych badań zdają się mocno sugerować prawdziwość postulatów fizyki kwantowej1.\r\nNiektórzy myśliciele dopatrywali się w zjawiskach kwantowych mechanizmów umożliwiających istnienie wolnej woli oraz świadomości. Roger Penrose uważa, że zjawiska kwantowe zachodzące w mózgu generują świadomość. Z kolei Johnjoe McFadden uważa, że za wolną wolę i świadomość odpowiada pole elektromagnetyczne wytwarzane przez pracujący mózg. Należy tu poczynić pewną uwagę. Mechanika kwantowa jest najbardziej podstawową teorią naukową jaką posiadamy, dotyczy zachowania mikrocząstek, z których zbudowane są wszystkie większe obiekty fizyczne i podlegają jej prawom. W takim ujęciu wszystko jest “kwantowe.” Gdy jednak mowa o kwantowych efektach w mózgu, zwykle chodzi o nietrywialne efekty kwantowe takie jak tunelowanie, splątanie czy superpozycja. Efekty te potrzebują odpowiednich warunków do zaistnienia. Obliczenia pokazują, że te efekty kwantowe w mózgu ulegają dekoherencji zbyt szybko, by wpływać lub zarządzać pracą neuronów (Seife, 2000)2. Nawet jeśli dopuścimy pewną losowość w pracy mózgu, problem z wolną wolą pozostał zasadniczo ten sam. Nawet jeśli istnieje pewna losowość w działaniu umysłu, to nie umysł ją generuje. Jeśli decyzja jest dziełem przypadku, nie może być wolna, ponieważ jest przypadkowa. Ponadto pojawia się też pytanie - jak taka losowość miałaby się manifestować. Na poziomie neuronalnym, gdyby losowe neurony losowo generowałyby potencjały czynnościowe, zakłóciłoby to pracę mózgu. Losowość musiałaby być niezwykle precyzyjna. Nie znajdziemy jej jednak też na poziomie świadomym, gdyż ludzie nie są w stanie np. wygenerować losowych sekwencji liczb (Figurska et al., 2008; Schulz et al., 2012).\r\nPoszukiwania kwantowej świadomości przypominają pod pewnymi względami problem homunkulusa. Hipotetyczny homunkulus to mały zarządca w naszym mózgu, który “ogląda” informacje dostarczane przez zmysły, integruje je i podejmuje decyzje. W toku historii wielokrotnie poszukiwano owego homunkulusa czy to w postaci niematerialnej duszy czy jakiegoś konkretnego obszaru w mózgu, który odpowiada za świadomość. Nie odnaleziono jednak niczego takiego, a świadomość wydaje się wyłaniać ze zsynchronizowanej pracy całego mózgu. W naukowym obrazie świata, zawieszeni pomiędzy deterministycznymi i stochastycznymi procesami, nie znajdziemy miejsca dla wolności przez duże “W.”\r\nHomunkulus\r\nProblemem organu zarządczego jest to, że jego działeniem również powinno coś zarządzać, i tak ad infinitum.\r\nZłudzenie konieczne\r\nMimo to, wielu ludzi - w tym naukowców, nie godzi się z takim stanem rzeczy. Doświadczenie sprawczości jest tak potężne, że stanowi samoistny argument za istnieniem wolnej woli. Hannah Arendt pisała “uznaję wewnętrzne świadectwo »ja chcę« za dostateczny dowód realności fenomenu woli” (Arendt, 1996), jakoby samo istnienie subiektywnego poczucia woli byłoby wystarczające, by udowodnić istnienie wolnych decyzji. Mgliste stwierdzenie, że struktura rzeczywistości, w której się znajdujemy, zaprzecza możliwości istnienia takiego fenomenu, może być dla wielu nieprzekonująca, gdy tak żywo czujemy naszą własną sprawczość. Niektórzy myśliciele, by znaleźć empiryczne dowody za lub przeciw wolnej woli, zdecydowali poszukać ich w prężnie rozwijającej się dziedzinie zajmującej się badaniem umysłu - szeroko pojętej neuronauki.\r\nNajsłynniejszymi eksperymentami dotyczącymi wolnej woli są eksperymenty Libeta i później Haynesa, którzy wykazali, że w przypadku prostych decyzji, takich jak naciśnięcie jednego z dwóch guzików, badacz jest w stanie przewidzieć na podstawie aktywności mózgu co wybierze badany, zanim uświadomi on sobie swoją decyzję (Soon et al., 2013). Choć te eksperymenty są najbardziej znane, ponieważ explicite poruszały tematykę wolnej woli, nie znajdziemy wielu badań studiujących wolną wolę. Wydaje się wręcz (pozornie), że na temat wolnej woli neuronauka ma niewiele do powiedzenia. Jest to podyktowane faktem, że na pytanie o wolną wolę (choć nie zostało zadane) została już udzielona odpowiedź. Założenia metodologiczne neuronauk i uzyskana dzięki nim wiedza, spójna z resztą naszej wiedzy o świecie, wykluczają istnienie fenomenu, który by jej jawnie przeczył. Mózg jest tylko (i aż) biologicznym komputerem. Laptop, na którym piszę ten tekst też potrafi podejmować decyzje, niemniej nie posiada z tego tytułu wolnej woli. Wydaje się, że wolna wola jest pojęciem, który ciężko wpisać we współczesną naukę.\r\n\r\nCelowo nie rozwijam problemu świadomości, który, moim zdaniem, jest bardziej skomplikowany niż problematyka wolnej woli. Na szczęście ta ostatnia nie wymaga prób odpowiedzenia na pytanie “czym jest świadomość?”\r\nMózg determinuje nasze decyzje poprzez obliczenia, a owe obliczenia determinowane są przez interakcje genów i środowiska. Bez odwołania do metafizycznych efektów, nie ma miejsca na ów wolny element. Niemniej, filozof Phillipe Meyer w książce “Złudzenie konieczne” zauważa, że chociaż neuronaukowcy udowodnili materialność funkcjonowania mózgu w każdym aspekcie, wielu naukowców, w tym niektórzy neuronaukowcy, odrzucają czysto materialny charakter pracy mózgu i to niekoniecznie w oderwaniu od swojej pracy naukowej (Meyer, 1998). John Eccles, laureat nagrody Nobla z fizjologii i medycyny (za badania nad synapsami), napisał artykuł w którym twierdził, że „[mózg odznacza się] wrażliwością innego rodzaju niż jakikolwiek instrument fizyczny” oraz że „umysł osiąga połączenie z mózgiem za pośrednictwem przestrzennoczasowych pól oddziaływania, które stają się aktywne dzięki tej wyjątkowej […] funkcji pobudzonej kory mózgowej”. Znany neuronaukowiec Michael Gazzaniga tak skomentował poglądy Eccelsa: “No, no! Przecież to czyste wudu, opisane wyszukanym językiem. Eccles zastąpił Kartezjuszową szyszynkę tajemniczą wrażliwością pobudzonej kory mózgowej. Dwieście lat po Kartezjuszu kontynuował kartezjańską tradycję dualizmu, mimo że spędzał sześćdziesiąt godzin tygodniowo na badaniu i rejestrowaniu aktywności neuronów i choć we wszystkich innych sprawach był gorliwym wyznawcą determinizmu. To po prostu niepojęte.” (Gazzaniga, 2020).\r\nWydaje się, że mamy do czynienia tutaj przykładem desperackiej próby ocalenia tego, w co dość długo wierzyliśmy, że nas ludzi wyróżnia. Wspomniany już Kartezjusz wierzył, że zwierzęta są maszynami napędzanymi przez skomplikowane mechanizmy (Descartes, 1980). Nie chciał jednak dopuścić, że podobnie może być w wypadku ludzi. Mimo oporów wobec materialistycznych i mechanicystycznych wyjaśnień działania ludzkiego umysłu, neuronauki zdają się sukcesywnie pokazywać, że nasza świadomość, poczucie sprawczości i ciągłości “ja,” są sprytnymi złudzeniami.\r\nBy zobaczyć jakie problemy nastręcza pojęcie wolnej woli, przyjrzyjmy się kilku badaniom naukowym. Michael Gazzaniga badał pacjentów po komisurotomi - chirurgicznym rozszczepieniu półkul mózgu wykonanym w celu złagodzenia ciężkich ataków epilepsji. W wyniku tego obie półkule dostawały te same dane wejściowe z obszarów podkorowych, ale działają niezależnie od siebie. Półkule nie dzielą informacji wzrokowej, prawe pole widzenia dochodzi tylko do lewej półkuli, a lewe do prawej. Jednocześnie u większości ludzi obszary związane z mową mieszczą się w lewej półkuli, słowa produkowane są bez udziału prawej półkuli. Pozwala to na dostarczanie różnych informacji obu półkulom: “Pokazaliśmy pacjentowi dwa obrazki: w prawej połowie jego pola widzenia umieściliśmy wizerunek kurzej łapy, tak aby lewa półkula mózgu widziała tylko ten obrazek, a w lewej połowie – obraz zaśnieżonego podwórka, tak aby półkula prawa nie widziała niczego poza nim. Następnie położyliśmy przed pacjentem kilkanaście rysunków, widocznych dla obu półkul mózgu, i poprosiliśmy, żeby wybrał spośród nich obrazki kojarzące mu się z tym, co przed chwilą zobaczył. Jego lewa ręka wskazała szuflę (która stanowiła najlepszą odpowiedź na widok zaśnieżonego podwórka), a prawa – kurę (była to najbardziej trafna reakcja na rysunek kurzej łapy). Kiedy zapytaliśmy, dlaczego wybrał właśnie te obrazki, jego ośrodek mowy zlokalizowany w lewej półkuli odparł: „Och, to bardzo proste. Kurza łapa kojarzy mi się z kurą,” z łatwością wyjaśniając to, co wiedział. Zobaczył kurzą łapę. Następnie, spojrzawszy na swoją lewą dłoń wskazującą szuflę, mężczyzna dodał bez wahania: „A szufla jest potrzebna do posprzątania kurnika”[…] Interesujący wydawał się fakt, że lewa półkula nie odpowiedziała: „Nie wiem”, co byłoby zgodne z prawdą. Zamiast tego wymyśliła odpowiedź pasującą do sytuacji. Konfabulowała, składając informacje, którymi dysponowała, w sensowną odpowiedź. Nazwaliśmy ten lewopółkulowy proces interpretatorem.\" (Gazzaniga, 2013)\r\nGazzaniga kontynuował badania nad interpretatorem. Jest to moduł wyspecjalizowany w tworzeniu spójnej narracji, umożliwia również tworzenie abstrakcyjnych relacji przyczynowo-skutkowych. Pozwala, na podstawie różnych przesłanek, wyciągać logiczne wnioski, wychodzące poza czystą percepcję rzeczywistości. Lewopółkulowy interpretator nieustannie szuka potencjalnych przyczyn różnych zdarzeń, jednak struktura, znajdująca się prawdopodobnie w prawym płacie ciemieniowym, hamuje działanie interpretatora, kiedy ten zaczyna tworzyć historie zbytnio odstające od rzeczywistości. Gdy półkule są rozdzielone, sygnał hamujący nie dochodzi do lewej półkuli, a interpretator może nieskrępowanie snuć swoje historie. Prowadzi to do systematycznych konfabulacji pacjentów z rozdzielonymi półkulami, ponieważ nawet najmniej prawdopodobne wyjaśnienia dostają się do świadomości jako wiarygodne przyczyny. Przykładowo, gdy Gazzaniga zaprezentował prawej półkuli pacjentki przerażający film, ta zaaktywizowała układ współczulny, wprowadzając fizjologiczną reakcję strachu - przyśpieszone bicie serca, potliwość i aktywność mięśniową. Ponieważ obszary podkorowe przekazują informacje o aktywności układu współczulnego prawej i lewej półkuli, lewa półkula “wiedziała,” że organizm jest przestraszony, nie wiedziała jednak dlaczego. Interpretator na podstawie dostępnych informacji - obecności Gazzanigi w pomieszczeniu - natychmiast wytworzył wyjaśnienie tego stanu: to doktor Gazzaniga jest przerażający.\r\nDalsza praca zespołu Gazzanigi pokazała role lewopółkulowego interpretatora u ludzi zdrowych. Sprawia on, że mamy poczucie spójnej narracji w naszych działaniach. Interpretator jest jednak tak dobry jak informacje, które do niego docierają, a wyniki eksperymentów pokazują, że docierają do niego rezultaty, czy też wyniki działań innych modułów mózgu, nie ma on jednak dostępu do ich obliczeń. Przykładem jest coś, co nazywamy pamięcią procesualną. W odróżnieniu do pamięci deklaratywnej, czyli takiej, do której mamy świadomy dostęp, pewne czynności potrafimy wykonywać doskonale, ale nie jesteśmy właściwie w stanie wytłumaczyć jak to robimy. Przykładem może być jazda na rowerze. Wyjaśnienie komuś jak należy jeździć na rowerze jest skazane na porażkę. Jest to wbrew pozorom bardziej skomplikowane niż pedałowanie i kręcenie kierownicą. Wymaga poczucia równowagi i odpowiedniego balansowania ciałem. Jednak osoby, które potrafią jeździć na rowerze wcale nie czują, by coś takiego robiły.\r\nInnym przykładem może być wiedza ekspercka. Ludzie specjalizujący się w pewnych zadaniach, potrafiący wykonywać je z niesamowitą biegłością, często nie mają wglądu w to jak to robią. Gazzaniga zaprosił do laboratorium mistrza szachowego Patricka Wolffa, który miał za zadanie odtworzyć układ pionków na szachownicy, oglądając go przez 5 sekund. Wolff był w stanie to zrobić, jeśli układ, który oglądał był sensowny z szachowego punktu widzenia. Jeśli jednak pionki były ustawione losowo, radził sobie z podobną skutecznością jak każdy inny człowiek. Nie miał więc świetnej pamięci wzrokowej, lecz jego moduł zajmujący się grupowaniem percepcyjnym nauczony wieloletnią grą w szachy błyskawicznie rozpoznawał złożone wzorce szachowe. Sam Wolff był tego nieświadomy, nie był w stanie powiedzieć jak udaje mu się osiągnąć dobry wynik w jednym przypadku i przeciętny w drugim. Jego interpretator dostawał tylko informacje o końcowym wyniku (odtworzeniu szachownicy), nie miał jednak informacji jak udało się to osiągnąć.\r\nInformacje przekazywane do świadomości przez mózg są selektywne. Nie wszystko co mózg zauważy i przetworzy dostanie się do naszej świadomości, czy też jakby powiedział Gazzaniga, do interpretatora. Jeśli badanym zostanie zaprezentowana przez kilka-kilkanaście milisekund przestraszona twarz, badani jej świadomie nie zauważą, jednak dostrzeżemy zwiększoną aktywność ciała migdałowatego - struktury przetwarzającej bodźce zagrażające (Whalen et al., 1998). Jest to kolejny z przykładów, że nie mamy świadomego dostępu do pewnych informacji przetwarzanych przez nasz mózg.\r\nMożna powiedzieć, że mózg regularnie nas “oszukuje.” Jeśli dotkniemy palcem do nosa, jednocześnie poczujemy dotyk w nosie i w palcu, mimo że sygnał z palca dotrze do mózgu znacznie później. Mózg tworzy własną reprezentację czasu przekazywaną świadomości. Badani, u których po wykonaniu spontanicznej akcji zaaplikowano silny impuls magnetyczny na pole przedruchowe, postrzegali, że intencja wykonania ruchu pojawiła się wcześniej niż gdy nie otrzymali impulsu (Lau et al., 2007).\r\nSposób przetwarzania informacji przez mózg ma krytyczne znaczenie nie tylko dla postrzegania sprawczości u siebie samych, ale i u innych ludzi. Zespół Rebecki Saxe zakłócił badanym działanie pewnego obszaru mózgu - prawego styku skroniowo-ciemieniowego, za pomocą impulsu magnetycznego. Następnie przedstawiła im cztery historie, w których Grace podała swojemu przyjacielowi cukierniczkę myśląc, że jest tam cukier/trucizna, gdy naprawdę był tam cukier/trucizna. Badani, którzy nie otrzymali impulsu, negatywnie oceniali Grace, gdy ta myślała, że w cukierniczce jest trucizna, niezależnie od tego czy faktycznie tam była. Badani którzy impuls otrzymali, łagodniej oceniali Grace, gdy ta myśląc, że podaje truciznę, podała cukier (Young et al., 2010). Badani, u których praca styku skroniowo-ciemieniowego została zakłócona, skupiali się na skutkach działań Grace, nie biorąc pod uwagę jej intencji. Prawy styk ciemieniowo-skroniowy jest częścią obwodu nerwowego zajmującego się teorią umysłu - zdolnością pozwalającą między innymi na przejęcie perspektywy innych osób. Sposób w jaki badani dokonują oceny moralnej Grace zależy od pracy tego obwodu. Gdyby mieli zadecydować czy i jaką karę wymierzyć Grace, informacje przetwarzane przez styk skroniowo-ciemieniowy miałby spore znaczenie dla tej decyzji.\r\nNie ma odpowiedzialności, nie ma kary\r\nPowyższe przykłady miały za zadanie pokazać, że odczuwana przez nas rzeczywistość jest sprytną iluzją, która nie zawsze przystaje do rzeczywistości fizycznej. Choć najbardziej spektakularnie widać to u ludzi z uszkodzeniami mózgu, postrzeganie ludzi “zdrowych” też jest wynikiem pracy mózgu, pracy niezależnej od nas samych. Wydaje się, że taki brutalny mechanicyzm odziera świat z jakiegokolwiek sensu. Jednak wiedza o tym, że “wolna wola” nie istnieje, raczej nie zmieni sposobu w jaki zachowujemy się na co dzień. Ma to jednak znaczenie dla praktyk społecznych. Jednym z przykładów może być system penitencjarny. System penitencjarny stawia sobie różne cele: odizolowanie jednostek niebezpiecznych od reszty społeczeństwa, resocjalizacji czy karze. Kara opiera się na pojęciu odpowiedzialności, jest odpłatą (sankcjonowaną zemstą) za przestępstwo.\r\nOkoło 180 lat temu stworzono regułę M’Naghtena - precyzyjne pojęcie niepoczytalności. Stało się to podczas procesu Daniela M’Naghtena w Wielkiej Brytanii, oskarżonego o zabójstwo i uniewinnionego z powodu choroby psychicznej. Wyrok ten wywołał ogromne kontrowersje, w wyniku czego powołano komisję, która miała określić dokładne kryteria niepoczytalności. Osoba, która nie jest świadoma, że popełnia przestępstwo, nie wie, że to co zrobiła jest “złe” w chwili popełnienia czynu nie może podlegać karze. Był to przełom społeczny, zdecydowano bowiem, że istnieją wyjątki od odpowiedzialności (Maroń, 2018).\r\n\r\nNie był to jednak pierwszy wyrok biorący pod uwagę poczytalność sprawcy. Istniał precedens (Rex v. Arnold z 1724) zwany “standardem dzikiej bestii,” jednak dopiero po sprawie M’Naghtena zestandaryzowano wskaźniki niepoczytalności.\r\nIdea ta, choć z oporem, na stałe zagościła w zachodnich systemach prawnych. Powoli pojawiła się w innych obszarach społecznych - sto pięćdziesiąt lat po sprawie M’Naghtena Kościół Katolicki - dla którego pojęcie wolnej woli i odpowiedzialności za czyny są niezwykle ważne, również uznał owy argument. Jak możemy przeczytać w Katechizmie: “Ciężkie zaburzenia psychiczne, strach lub poważna obawa przed próbą, cierpieniem lub torturami mogą zmniejszyć odpowiedzialność samobójcy” (Katechizm Kościoła Katolickiego, 2015), uznając, że depresja zmniejsza opdowiedzialnośc za czyn uważany jako grzech ciężki.\r\nChoć ówcześnie idea, że człowiek może nie odpowiadać za swoje czynny była myślą nowatorską, funkcjonuje do dziś w stanie praktycznie nie zmienionym, mimo znacznego postępu nauki. Znaleźliśmy się współcześnie w miejscu, gdzie dzielimy ludzi na zdolnych i tych niezdolnych do odpowiedzialności. Kryterium rozróżnienia polega na poprawności funkcjonowania mózgu. Problem w tym, że według wiedzy neuronaukowej ciężko jest mówić o odpowiedzialności kogokolwiek (Farah, 2005). Przytoczone wcześniej przykłady miały pokazać, że zachowanie zależne jest od pracy mózgu, na którą jednostka nie ma wpływu. Neurobiolog Robert Sapolsky poddał pod wątpliwość obecną definicje niepoczytalności skupiającą się na nieświadomości sprawcy, że popełnia czyn zabroniony. Istotnym zespołem ośrodków mózgów odpowiedzialnych za hamowanie zachowań impulsywnych jest kora przedczołowa. Osoby z zaburzonym jej funkcjonowaniem doskonale zdają sobie sprawę z swoich zachowań, nie są jednak w stanie się od nich powstrzymać (Sapolsky, 2006).\r\nWśród wielu neuronaukowców i przedstawicieli innych dziedzin istnieje pogląd, że kara oparta na odpowiedzialności, która jest sankcjonowaną prawnie zemstą, jest co najmniej wątpliwa. Richard Dawkins stwierdził “Odpłata jako zasada moralna jest niezgodna z naukowym poglądem na ludzkie zachowanie. Jako naukowcy wierzymy, że ludzkie mózgi, chociaż mogą nie działać w taki sam sposób, jak komputery stworzone przez człowieka, są tak samo rządzone prawami fizyki. Kiedy komputer działa nieprawidłowo, nie karzemy go. Odnajdujemy problem i naprawiamy go, zwykle poprzez wymianę uszkodzonego komponentu, sprzętowego lub programowego” (Dawkins, 2006). Joshua Green i Jonathan Cohen uważają, że należy odrzucić koncept retrybucji, a zamiast tego skupić się na odseparowaniu od społeczeństwa (jeśli to potrzebne) i resocjalizacji. Postulują, że wraz ze zrozumieniem przyczyn działań przestępców, powinniśmy zmienić stosunek do nich (Greene & Cohen, 2004). Jest to logiczną konsekwencją faktu, że retrybucje wobec osób psychicznie chorych uważamy za niehumanitarne (ponieważ nie mają wpływu na to co robią), to zgodnie z wiedzą neuronaukową nie powinniśmy stosować retrybucji wobec nikogo.\r\nNiekoniecznie jednak brak “wolnej woli” człowieka implikuje konieczność zrezygnowania z retrybucyjnej funkcji kary. Psychiatra Sally Satel i psycholog Scott O. Lilienfeld w książce “Pranie mózgu” argumentują, że niezależnie od statusu wolnej woli, ludzie mają poczucie wolności wyboru i działają podług niego, a kara ma ewolucyjną funkcję społeczną (Satel & Lilienfeld, 2017). Już małe dzieci odbierają zachowanie innych ludzi w kategoriach intencjonalności i mają silne wrodzone zachowania moralne, włącznie z karaniem źle zachowujących się jednostek (Hamlin, 2013). Eksperymenty przeprowadzone przez ekonomistów behawioralnych pokazały, że wielu ludzi dyscyplinuje jednostki zachowujące się “nie-fair,” nawet jeśli były tylko biernymi obserwatorami takiego zachowania i są w stanie poświęcić na to własne zasoby (Fehr & Fischbacher, 2004). Pozwala to ludziom na osiąganie celów wymagającej współpracy wielu osób i minimalizacji “oszustów.”\r\nPonadto Satel i Lilienfeld przywołują eksperyment psycholożki i prawniczki Kenworthey Bilz, w którym wykazała, że nieukaranie sprawcy gwałtu obniża status społeczny ofiary (Bilz, 2016). Sugerują, że brak moralnego zadośćuczynienia przy użyciu proporcjonalnej kary może prowadzić do zachwiania równowagi społecznej. Przywołują hipotezę “sprawiedliwego świata” sformułowaną przez Melvina Lernera. Jest to błąd poznawczy polegający na tendencji do wiary, że ludzie zasługują na nieszczęścia i sukcesy, które ich spotykają. W eksperymencie przeprowadzonym przez Lernera badani obserwowali aktorkę wykonującą zadanie pamięciowe, jednocześnie otrzymującą bolesne impulsy elektrycznie (udawane, o czym badani nie wiedzieli). Gdy badani mogli zdecydować o przerwaniu eksperymentu lub dowiadywali się, że aktorka otrzyma wynagrodzenie pieniężne oceniali ją znacznie wyżej, niż w sytuacji gdy biernie przyglądali się sytuacji bez możliwości wpływu na nią (Lerner & Miller, 1978). Lerner doszedł do wniosku, że widok osoby cierpiącej, która nie ma szans na rekompensatę skłania ludzi do deprecjonowania ofiary. Satel i Lilienfeld sugerują, że rezygnacja ze sprawiedliwej odpłaty byłaby negatywna w skutkach zarówno dla ofiary jak i moralności społecznej.\r\nInni wyrażają bardziej umiarkowany pogląd, że kwestia wolnej woli jest w gruncie rzeczy nieistotna dla prawa, dostrzegają jednak, że włączenie wiedzy o człowieku do procesu wymierzania sprawiedliwości może poskutkować bardziej empatycznym traktowaniem przestępców (przynajmniej tych o których wiemy, że mają znacznie zakłócone procesy podejmowania decyzji), przy zachowaniu psychologicznej potrzeby retrybucji ofiar i społeczeństwa (Goodenough & Prehn, 2004). Jest to próba pewnego kompromisu, wskazująca na potrzebę zmian w systemie prawnym, niemniej nie polegająca na odrzuceniu retrybucyjnej funkcji kary.\r\nEpilog\r\nChoć odpłata jest głęboko zakorzeniona w ludzkiej kulturze i biologii, nie oznacza to, że nie jesteśmy w stanie zmienić naszego podejścia. Człowiek wielokrotnie wprowadzał zmiany społeczne, jak wówczas mówiono, niezgodne z jego naturą czy uderzające w porządek społeczny. Jeśli jako gatunek mielibyśmy działać tak jak aktualnie postrzegamy rzeczywistość, niewiele byśmy osiągnęli. Green i Cohen podają przykład tego jak nasza percepcja fizycznej rzeczywistości rozmija się z tym jak jest faktycznie. Przykładem może być czas. Czas jest względny, zależny od pola grawitacyjnego i prędkości z jaką układ odniesienia się porusza. Jednak nasze codzienne doświadczenie mówi nam, że czas jest uniwersalny i płynie liniowo. Ma to silne przyczyny ewolucyjne. Czy to oznacza, że jesteśmy na zawsze ograniczeni przez nasze liniowe postrzeganie czasu? Zdecydowanie nie. Wykorzystujemy zdobytą wiedzę w tworzeniu rzeczywistości np. w systemach GPS. Podobnie jest w przypadku omawianego problemu. Możemy wykorzystać wiedzę, którą mamy, by zmienić nasze zachowania społeczne.\r\n\r\n\r\n\r\nArendt, H. (1996). Wola (p. 28). Czytelnik.\r\n\r\n\r\nBilz, K. (2016). Testing the expressive theory of punishment. Journal of Empirical Legal Studies, 13(2), 358–392.\r\n\r\n\r\nDamasio, A. R. (2011). Błąd kartezjusza: Emocje, rozum i ludzki mózg. Dom Wydawniczy\" Rebis\".\r\n\r\n\r\nDawkins, R. (2006). Let’s all stop beating basil’s car. https://www.edge.org/response-detail/11416\r\n\r\n\r\nDescartes, R. (1980). Rozprawa o metodzie (pp. 72–73). Państwowy Instytut Wydawniczy.\r\n\r\n\r\nFarah, M. J. (2005). Neuroethics: The practical and the philosophical. Trends in Cognitive Sciences, 9(1), 34–40.\r\n\r\n\r\nFehr, E., & Fischbacher, U. (2004). Third-party punishment and social norms. Evolution and Human Behavior, 25(2), 63–87.\r\n\r\n\r\nFigurska, M., Stańczyk, M., & Kulesza, K. (2008). Humans cannot consciously generate random numbers sequences: Polemic study. Medical Hypotheses, 70(1), 182–185.\r\n\r\n\r\nGazzaniga, M. S. (2013). Kto tu rządzi-ja czy mój mózg?: Neuronauka a istnienie wolnej woli (pp. 74–76). Smak Słowa.\r\n\r\n\r\nGazzaniga, M. S. (2020). Instynkt świadomości jak z mózgu wyłania się umysł? (pp. 89–90). Smak Słowa.\r\n\r\n\r\nGoodenough, O. R., & Prehn, K. (2004). A neuroscientific approach to normative judgment in law and justice. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 359(1451), 1709–1726.\r\n\r\n\r\nGreene, J., & Cohen, J. (2004). For the law, neuroscience changes nothing and everything. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 359(1451), 1775–1785.\r\n\r\n\r\nHamlin, J. K. (2013). Moral judgment and action in preverbal infants and toddlers: Evidence for an innate moral core. Current Directions in Psychological Science, 22(3), 186–193.\r\n\r\n\r\nKatechizm kościoła katolickiego. (2015). 2280, 2282.\r\n\r\n\r\nLau, H. C., Rogers, R. D., & Passingham, R. E. (2007). Manipulating the experienced onset of intention after action execution. Journal of Cognitive Neuroscience, 19(1), 81–90.\r\n\r\n\r\nLerner, M. J., & Miller, D. T. (1978). Just world research and the attribution process: Looking back and ahead. Psychological Bulletin, 85(5), 1030.\r\n\r\n\r\nMaroń, G. (2018). Zabójstwo „z rozkazu boga” a niepoczytalność sprawcy w świetle orzecznictwa sądów USA ( murder on the command of god versus a perpetrator’s insanity in the light of the case law of u.s. courts). 2018.\r\n\r\n\r\nMeyer, P. (1998). Złudzenie konieczne (pp. 187–188). Państwowy Instytut Wydawniczy.\r\n\r\n\r\nSapolsky, R. M. (2006). The frontal cortex and the criminal justice system. Law and the Brain, 227.\r\n\r\n\r\nSatel, S., & Lilienfeld, S. O. (2017). Pranie mózgu. Uwodzicielska moc (bezmyślnych) neuronauk (pp. 191–228). CiS.\r\n\r\n\r\nSchopenhauer, A. (1991). O wolności ludzkiej woli (pp. 16–17). bis.\r\n\r\n\r\nSchulz, M.-A., Schmalbach, B., Brugger, P., & Witt, K. (2012). Analysing humanly generated random number sequences: A pattern-based approach. PloS One, 7(7), e41531.\r\n\r\n\r\nSeife, C. (2000). Cold numbers unmake the quantum mind. Science, 287(5454), 791–791.\r\n\r\n\r\nSoon, C. S., He, A. H., Bode, S., & Haynes, J.-D. (2013). Predicting free choices for abstract intentions. Proceedings of the National Academy of Sciences, 110(15), 6217–6222.\r\n\r\n\r\nWhalen, P. J., Rauch, S. L., Etcoff, N. L., McInerney, S. C., Lee, M. B., & Jenike, M. A. (1998). Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge. Journal of Neuroscience, 18(1), 411–418.\r\n\r\n\r\nYoung, L., Camprodon, J. A., Hauser, M., Pascual-Leone, A., & Saxe, R. (2010). Disruption of the right temporoparietal junction with transcranial magnetic stimulation reduces the role of beliefs in moral judgments. Proceedings of the National Academy of Sciences, 107(15), 6753–6758.\r\n\r\n\r\nCo interesujące, pojawia się tu dodatkowy problem. W wielu dziedzinach nauki wykorzystujemy założenie o niezależności pomiarów, który uzyskujemy zwykle dzięki np. generatorom liczb pseudolosowych czy podwójnie ślepej próbie. Zwykle to wystarcza by uzyskać rzetelne wyniki. Jeśli jednak chcemy zbadać fundamentalną właściwość jaką jest samo istnienie losowości, nie możemy wykorzystać liczb pseudolosowych do ustawienia parametrów urządzeń, ponieważ liczby pseudolosowe są deterministyczne. Prawdziwą losowość uzyskuje się podczas pomiarów efektów kwantowych, ale tu zakładamy losowość zjawisk kwantowych, by udowodnić losowość zjawisk kwantowych. Fizycy starają się poradzić sobie z tym problemem na wiele sposobów, np. mierząc fotony pochodzące z gwiazd oddalonych o 600 lat świetlnych czy prosząc internautów o głosowanie w wyborze parametrów. Zabezpiecza to przez zarzutem prostego łańcucha deterministycznego, nie wyklucza jednak superdeterminizmu, to znaczy możliwości, że takie, a nie inne zachowanie kwantowych cząsteczek podczas pomiarów wynika z mechanizmu przyczynowo-skutkowego, którego nie rozumiemy. Wykład o twierdzeniu Bella autrostwa Michała Ecksteina - https://www.youtube.com/watch?v=eRDxsl06f30&t=194s↩︎\r\nInteresujący przegląd zagadnień związanych z zjawiskami kwantowymi w biologii można znaleźć w książce Paula Davisa “Demon w maszynie.”↩︎\r\n",
    "preview": "posts/2021-09-13-o-odpowiedzialnoci-za-czyny-w-dobie-lotw-na-marsa/pexels-suzy-hazelwood-1422673.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-12-odwied-baz-wiedzy/",
    "title": "Odwiedź Bazę Wiedzy!",
    "description": {},
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {}
      }
    ],
    "date": "2021-09-12",
    "categories": [
      "Sortownia"
    ],
    "contents": "\r\n\r\n\r\nOdwiedź Bazę Wiedzy!\r\nOtwieramy pierwszy moduł Sortowni Wiedzy! Jest nią… Baza Wiedzy. Sprawa jest bardzo prosta – zbieramy przydatne linki w jedno miejsce. Tylko tyle i aż tyle. Jeśli chcecie dowiedzieć się od czego zacząć poszukiwania artykułów naukowych, jeśli chcecie poznać podkasty popnaukowe, jeśli chcecie poznać ciekawe kanały na Youtube, albo dotrzeć do przydatnych źródeł z wybranych dziedzin – zajrzyjcie i przetestujcie wyszukiwarkę.\r\n\r\nWe współczesnym świecie bardzo wiele można nauczyć się nie wychodząc z domu (i za darmo). Internet jest pełen zasobów i narzędzi do nauki, weryfikacji informacji i edukującej rozrywki. Na niektóre łatwo trafić. Inne znajduje się po długich poszukiwaniach lub przypadkiem. Menedżery zakładek w naszych przeglądarkach wypchane są przydatnymi linkami, często nieopisanymi lub w złych podfolderach. Chęci na uporządkowanie tego zrodziła pomysł Bazy Wiedzy.\r\n\r\nLinki są zorganizowane według kategorii gatunkowo-funkcjonalnych (np. podkasty, serwisy informacyjne, strony zawierające informacje o grantach). Jest również specjalna kategoria “tematyczne/dziedzinowe”, która rozwinie dalsze okno wyboru wg dziedzin naukowych (np. lingwistyka, neuronauki). Ta z kolei pozwoli na jeszcze dokładniejsze zawężenie poszukiwań, wg tagów (dowolne hasła, lepiej kategoryzujące strony).\r\nBaza Wiedzy jest i będzie w ciągłym rozwoju, a znajdować się na niej będą ręcznie wybrane linki do istotnych stron. Zależy nam na stworzeniu prostego, funkcjonalnego przybornika do świata wiedzy. Kategorie zorganizowaliśmy tak, aby różne osoby mogły znaleźć coś przydatnego. Dla osób zaczynających poszukiwanie wiedzy stworzyliśmy kategorię “Gdzie zacząć”, na której znajdziecie odnośniki do baz z zasobami i wyszukiwarek. Dla osób szukających rozrywki i wiadomości naukowych mamy kategorie podkastów, kanałów wideo i serwisów informacyjnych. Z kolei osoby już eksplorujące wybrane dziedziny naukowe, mamy kategorię “dziedziny”, która pozwala wybrać specyficzne pole badawcze.\r\nWy też możecie dołożyć swoje trzy grosze do rozwoju Bazy Wiedzy! Jeśli znacie przydatne strony, albo jeśli macie wiedzę ekspercką z zakresu jakiejś dziedziny – piszcie do nas! Informacje w zakładce “Kontakt”.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-12-odwied-baz-wiedzy/books-1655783_1280.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-12-witajcie-w-sortowni/",
    "title": "Witajcie w Sortowni",
    "description": {},
    "author": [
      {
        "name": "Kamil Kopacewicz",
        "url": {}
      }
    ],
    "date": "2021-09-11",
    "categories": [
      "Sortownia"
    ],
    "contents": "\r\nOtwieramy stronę! Witajcie na Sortowni Wiedzy (a może powinniśmy powiedzieć - w Sortowni?). Ta strona ma być przede wszystkim swego rodzaju punktem przesiadkowym. Węzłem, z którego dalej można przeskoczyć do innych stron z wiedzą naukową. My w pierwszej kolejności chcemy zgromadzić przydatne odnośniki i nakierować Was na te najbardziej wartościowe. Już teraz możecie zajrzeć do naszej Bazy Wiedzy. Co prawda wciąż jeszcze panuje tam chaos, który próbujemy opanować - ale wraz z mijającym czasem, powinno zacząć się tam przejaśniać. Cel i funkcja Bazy Wiedzy są niezwykle proste. Wręcz obraźliwie proste. To po prostu baza przydatnych linków. Tyle i aż tyle. Odnośniki zebrane w jednym miejscu, opisane i podzielone na kategorie. Znajdziecie tam linki do stron popnaukowych (serwisy, blogi, podkasty), jak i linki do przydatnych repozytoriów i baz naukowych. Linki są naszym subiektywnym wyborem tego, co uważamy za przydatne i istotne. I tu otwarcie mówimy - to nie jest obiektywna selekcja, choć staramy się do obiektywizmu dążyć.\r\nZachęcamy Was do podsyłania nam nowych linków na adres sortownia.wiedzy@gmail.com. Siłą rzeczy, nie znamy się na większości dziedzin naukowych, więc będziemy niezmiernie wdzięczni za pomoc w uzupełnianiu bazy.\r\nBaza Wiedzy to jednak tylko początek. Już teraz mamy w planach kolejne moduły strony, również związane z rozpowszechnianiem nauki. Będziemy publikować teksty opiniotwórcze na blogu i poradniki (m.in. z wiedzą o tym jak korzystać z nauki, jak szukać artykułów, jak rozpoznawać strategie retoryczne itd.). Mamy tysiąc pomysłów na nowe, przydatne narzędzia do poszukiwania wiedzy naukowej – te jednak będziemy powoli i ostrożnie rozwijać, wraz z rozrastaniem się strony.\r\nDla kogo w ogóle to wszystko robimy? Jaki to ma sens? No więc tak – po pierwsze, tak zebrana wiedza jest po prostu przydatna, nawet dla nas samych. Już teraz zdarza się nam szukać linków w Bazie kiedy szukamy (tego co gdzieś tam dawno temu daliśmy do zakładek w wyszukiwarce, ale nikt nie pamięta gdzie i kiedy to było). Baza Wiedzy, nawet w swojej podstawowej formie, pozwala dość szybko znaleźć przydatne strony związane z nauką.\r\nPo drugie, ze strony mogą korzystać studen_ i naukowc_, poszukujący jakiegoś punktu wyjściowego do poszukiwań. Osoby nie zaznajomione z poszukiwaniem wiedzy naukowej znajdą u nas wiele przydatnych odnośników do najważniejszych stron, od których należy zacząć zbieranie bibliografii. Po trzecie, osoby zupełnie niezwiązane ze światem nauki mogą być zainteresowane naszym wyborem stron popnaukowych. W szczególności polecamy sekcję podkastów. Po czwarte, będziemy kierować część materiałów do influencer_ i popularyzator_ nauki. Jednym z naszych celów jest zwiększenie ilości wiedzy naukowej w debacie publicznej. W związku z tym, jeśli masz platformę i własną publiczność, na naszej stronie znajdziesz materiały, które pomogą Ci w dalszym przekazywaniu informacji naukowych.\r\nMamy nadzieję, że to co robimy będzie przydatne. To dla nas najważniejsze. Chcemy zrobić coś pozytywnego w wymiarze społecznym, nawet jeśli będzie to bardzo mała rzecz. Piszcie do nas z uwagami, pomysłami, reakcjami na adres sortownia.wiedzy@gmail.com.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-12-witajcie-w-sortowni/hero-image.jpg",
    "last_modified": "2022-04-10T22:08:04+02:00",
    "input_file": {}
  }
]
