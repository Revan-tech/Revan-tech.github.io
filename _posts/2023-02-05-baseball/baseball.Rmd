---
title: "Dlaczego średnia uderzeń graczy nie przewiduje sukcesu w pojedyńczym uderzeniu?"
description: |
  O psychologach i małych efektach
preview: "Statystyczne_Dygresje.jpg"
author:
  - name: Szymon Mąka
    url: {https://revan-tech.github.io/kontakt.html}
date: 02-05-2023
output:
  distill::distill_article:
    self_contained: false
    toc: true
    includes:
      in_header:
        - hero-image.html
    css: theme.css  
bibliography: bib.bib
categories: 
  - Statystyczne Dygresje
csl: apa.csl
---
```{r include=FALSE}
Var = function(x){sum((x - mean(x))^2)/length(x)}
```


Przeczytałem dwa lata temu artykuł Davida Fundera _"Evaluating Effect Size in Psychological Research: Sense and Nonsense"_ w ramach obowiązkowych zajęć z metodologii dla doktorantów [@funder2019evaluating]. Artykuł skupia się na problematyce małych wielkości efektów w psychologii i ich interpreteacji. Jest cytowany w prawie 1500 innych artykułach, co świadczy o jego dużej popularności. Funder krytykuje standardy dotyczące wielkości efektów opartych na współczynniku korelacji liniowej zaproponowanych przez Jacoba Cohena i proponuje nowe kryteria, jednocześnie podając możliwe przyczyny występowania małych wielkości efektów w psychologii. 

Choć w artykule słusznie argumentuje, że wielkość wielkości efektów powinna być oceniana w zależności od badanego zjawiska, **artykuł zawierał dużo twierdzeń, które były dla mnie niepokojące metodologicznie.** 

Najbardziej jednak zadziwił mnie wnioskek Fundera, że mały efekt korelacji liniowej sumuje się w jakiś sposób w czasie. Co to znaczy? Początkowo sądziłem, że być może  chodzi o uśrednianie wielkorotnych pomiarów u pojedyńczej osoby, by uzyskać mniej zaszumioną zmienną. Na przykład gdy mierzymy czas reakcji badanego podczas wykonywania zadania na komputerze, badany w pojedyńczej próbie może się zamyślić i zaareagować wolniej. Może też akurat patrzeć tam gdzie pojawi się bodziec i zareagować szybciej niż gdyby musiał tego bodźca szukać. Uśredniając czas reakcji pozbywamy się szumu wynikającego z takich zdarzeń. Jednak Funderowi nie o to chodziło. 

Funder oparł to twierdzenie na artykule z lat osiemdziesiątych psychologa Roberta Abelsona [@abelson1985variance]. Abelson obliczył korelację między sukcesem w pojedynczym pojedynczym podejściu profesjonalnych bejsbolistów z Major League, z ich średnią uderzeń w sezonie. Z jego obliczeń wyszło, że średnia w sezonie wyjaśniała tylko 0.00317 wariancji sukcesu w pojedyńczym podejściu. Abelson zdziwiony stwierdził, że ponieważ niewątpliwie sukces w pojedyńczym podejściu jest silnie związany ze średnią w sezonie, to owa korelacja jest zdecydowanie za mała, a on ma doczynienia z jakimś paradoksem.

Funder stwierdził, że skoro najlepsza dostępna miara zdoloności zawodnika (jego średnia w sezonie), wyjaśnia tak mało wariancji sukcesu w pojedyńczym rzucie, a niewątpliwie jego średnia jest silnie związana z sukcesem w pojedyńczym rzucie, to związek ten musi objawiać (kumulować) się w czasie. W pojedyńczym rzucie nie jest taki istotny, jak w dwudziestu rzutach. Innymi słowy wnioskowanie Fundera wyglądało mniej więcej tak: średnią w sezonie można potraktować jako miarę jakieś cechy osobowości np. neurotyczności czy ekstrawertyzmu, a sukces w pojedyńczym podejściu jako zachowanie powiązane z tą cechą. Funder twierdzi, że małe korelacje cech osobowości z danymi zachowaniami, wcale nie są małe, ponieważ ich prawdziwa siła objawia się w dłuższej perspektywie czasowej. Czyli, że w jakiś sposób się kumulują. 

<aside>
![](58xkq0.jpg)
</aside>

Taki wniosek może być podbudowujący. Małe efekty w psychologii wcale nie są małe. **Jednak jest to wniosek zupełnie nieporawny.** Wynik Abelsona można prosto wyjaśnić statystycznie i żaden sposób nie oznacza to, że małe efekty nie są małe. 

Żeby zrozumieć skąd wziął się tak mały wynik Abelsona, prześledzmy jego rozumowanie. Średnie profesjonalnych graczy zawierały się między 0.2 a 0.3 więc uznał arbitralnie, że średnia średnich graczy to 0.27, a odchylenie standardowe średniej średnich to 0.25. 

Przypomnijmy sobie, że wariancję zmiennej możemy rozłożyć:

$$ Var(X) = Var(E(X|Y)) + E(Var(X|Y))$$

Gdzie $Var(E(X|Y))$ to wariancja przewidywania, a $E(Var(X|Y))$ to wariancja błędu. 

Procent wyjaśnionej wariancji przez zmienną $Y$ możemy obliczyć:

$$eta^2 = \frac{Var(E(X|Y))}{Var(X)}$$

Z tego wzoru wynika, że procent wariancji zostanie wyjaśnionej zależy od tego jak bardzo średnie X pod warunkiem różych wartości Y różnią się od siebie. 

Zasymujmy dane Abelsona przy użyciu języka R. Powiedzmy, że mamy 1000 profesjonalnych graczy. Dla każdego losujemy jego prawdziwą średnią pojedyńczych sukcesów ze skróconego rozkładu normalnego (między 0.2 a 0.3), o średniej 0.27 i odchyleniu standardowym 0.25. Natępnie każdemu graczowi losujemy 520 uderzeń (tyle jest w sezonie), a prawdopodobieństwo sukcesu wyznacza jego średnia. 

Mamy zmienne: $X$ - zmienna binarna (1 - sukces w pojedyńczyn uderzeniu, 0 - brak sukcesu), $Y$ średnia liczba uderzeń w sezonie, $Z$ - poszczególny gracz. 

```{r}
library(tidyverse)
library(truncnorm)
library(knitr)

means = rtruncnorm(1000, 0.2,0.3, 0.27, 0.25)
data = data.frame()

for( i in 1:1000) {
  X = sample(c(0,1),520,TRUE, prob = c(1-means[i], means[i]))
  true_mean = rep(means[i],520)
  Y = rep(mean(X),520)
  Subject = rep(i,520)
  data = rbind(data,data.frame(X,Y,true_mean,Subject))
}

cat(paste("Korelacja wynosi ", round(cor(data$X,data$Y),3),".", sep = ""))
cat( paste("Wariancja błędu wynosi",
round(sum(tapply(data$X, data$Y, Var)*tapply(data$X, data$Y, length)/length(data$X)),3)),".",sep = "")
cat(paste("Wariancja przewidywania wynosi", round(Var(data$X) 
- sum(tapply(data$X, data$Y, Var)*tapply(data$X, data$Y, length)/length(data$X)),3),"."))

```

Jak widzimy korelacja jest tutaj bardzo mała. Ale co się stanie jeśli zasymulujemy dane, w których średnie graczy będą losowane z rozkładu z taką samą średnią i odchyleniem standardowym ale z ograniczeniem przedziału od 0 do 1.

```{r}
means = rtruncnorm(1000, 0,1, 0.27, 0.25)
data2 = data.frame()
for( i in 1:1000) {
  
  X = sample(c(0,1),520,TRUE, prob = c(1-means[i], means[i]))
  true_mean = rep(means[i],520)
  Y = rep(mean(X),520)
  Subject = rep(i,520)
  data2 = rbind(data2,data.frame(X,Y,true_mean,Subject))

}
cat(paste("Korelacja wynosi ", round(cor(data2$X,data2$Y),3),".", sep = ""))
cat( paste("Wariancja błędu wynosi",
round(sum(tapply(data2$X, data2$Y, Var)*tapply(data2$X, data2$Y, length)/length(data2$X)),3)),".",sep = "")
cat(paste("Wariancja przewidywania wynosi", round(Var(data2$X) 
- sum(tapply(data2$X, data2$Y, Var)*tapply(data2$X, data2$Y, length)/length(data2$X)),3),"."))
```

Otrzymaliśmy dużo wyższą korelację. Zobaczmy co się tu stało. Wariancja przewidywania znacznie wzrosła, natomiast wariancja błędu zmalała. Dlaczego tak się stało skoro związek pomiędzy zmiennymi jest taki sam w obu symulacjach?

Odpowiedź na to pytanie zawiera się w samym wzorze na wyjaśniną wariancję. Przyjrzyjmy się najpierw wariancji przewidywania $Var(E(X|Y))$. Ponieważ $Y = E(X|Z)$ to wtedy $Var(E(X|Y)) = Var(E(X|Z)) = Var(Y)$.

To znaczy, że wysokość wariancji przewidywania zależy tylko od tego jak bardzo średnie graczy się różnią. Im bardziej gracze są do siebie podobni, tym mniejsza wariancja przewidywania w stosunku do wariancji błędu. **Innymi słowy jeśli wszyscy gracze mają bardzo podobną średnią uderzeń, to nie będzie ona dobrze wyjaśniać różnic pomiędzy nimi.**

Wariancja wyjaśniana (całkowita) składa się z wariancji błędu i wariancji przewidywania. Ponieważ X jest zmienną binarną to

$$Var(X) = E(X)(1-E(X))$$

Dla przykładu Abelsona wariancja całkowita to $Var(X) = 0.27(1-0.27) = 0.1971$. 

Wariancja przewidywania w przykładzie Abelsona zależy od tego jak różne od siebie są średnie graczy. skoro wiemy, że średnie graczy znajdują się w przedziale <0.2, 0.3> i mają średnią 0.27, moglibyśmy się pokusić o pytanie ile maksymalnie ta zmienna mogłaby wyjaśnić wariancji pojedyńczego sukcesu? 

Jeśli mamy zmienną w przedziale $<0, c>$ to:
$$\displaystyle \sum_i x_i^2 = \sum_i x_i\cdot x_i \leq \sum_i c\cdot x_i = cn\bar{x}$$

ponadto:

$$\begin{align*}
n\cdot \text{Var}(\mathbf{x}) &= \sum_i (x_i - \bar{x})^2= \sum_i x_i^2 - 2x_i\bar{x} + \bar{x}^2\\
&= \sum_i x_i^2 - 2\bar{x}\sum_i x_i + n\bar{x}^2= \sum_i x_i^2 - n\bar{x}^2\\
&\leq cn\bar{x} - n\bar{x}^2 = n\bar{x}(c-\bar{x})
\end{align*}$$

więc

$$\text{Var}(\mathbf{x}) \leq \bar{x}(c-\bar{x})$$

średnia uderzeń graczy Abelsona  

$$eta^2 = \frac{Var(Y)}{Var(X)} \le \frac{0.7(0.1-0.7)}{0.1971} = 0.01065$$

może w najlepszym wypadku wyjaśnić 1% wariancji. Widzimy ze wzoru, że zachowując wartość średniej sukcesów jako stałą, rozszerzanie przedziału, w którym znajdują się możliwe średnie graczy, będzie zwiększało procent możliwej do wyjaśnienia wariancji. 

Wydaje mi się, że zdziwienie Abelsona wynikało z faktu, że spodziewał się, że procent wyjaśnionej wariancji w pojedyńczym sukcesie przez średnią gracza powinen być wysoki, ponieważ przewidujemy zachowanie na podstawie średniej miary tegoż zachowania, czyli jego średniej w dłuższym okresie (analgoicznie do neurotyzmu mającego przewidywać zachowania neurotyczne). 

Jednak Abelson licząc procent wyjasnionej wariancji zadał inne pytanie, mianowicie **jak dobrze możemy przewidzieć sukces gracza w pojedyńczej próbie na podostawie jego średniej**. Jak zauważyliśmy, nie będzie dobrze przewidywać, ponieważ zmienna Y nie różnicuje wystarczająco dobrze pomiędzy graczami względem wielkości błędu. Można to rozumieć w katagoriach niskiej rzetelności. 

To zjawisko generalizuje się na inne przykłady. Żeby sobie to zwizualizować zobaczmy średnie graczy w sezonie z symulacji pierwszej (średnia z 520 prób pojedyńczych odbić) naprzeciw prawdziwych średnich (prawdopodobieństw, których użyłem do wygenerowania danych, możemy je potraktować jako ich prawdziwą zdolność gry w bejsbol). 

<aside>
Generalizuje się fakt, że im bardziej różne od siebie są średnie warunkowe E(X|Y), tym więcej Y wyjaśni wariancji X.
</aside>


```{r}
data_plot <- data %>% group_by(Subject) %>% summarise(skill = min(true_mean), mean = min(Y))


plot(data_plot$mean,data_plot$skill)

cat(paste("Korelacja wynosi ", round(cor(data_plot$skill,data_plot$mean),3),".", sep = ""))

```

Teraz to samo z symulacją 2. 

```{r}
data2_plot <- data2 %>% group_by(Subject) %>% summarise(skill = min(true_mean), mean = min(Y))

plot(data2_plot$mean,data2_plot$skill)

cat(paste("Korelacja wynosi ", round(cor(data2_plot$skill,data2_plot$mean),3),".", sep = ""))
```

Jak widzimy korelacja jest większa w drugiej symulacji, mimo, że związek pomiędzy zmiennymi jest taki sam. Mamy tu do czynienia ze zjawiskiem Restricted Range Corelation - czyli sytuacji w której zmienna w próbie ma mniejszą wariancję niż w populacji. Ponieważ obserwacje są do siebie bardziej podobne, (są bliżej siebie na wykresie) wpływ błędu jest większy. Przybliżmy teraz drugi wykres tak, by widzieć wartości prawdziwej średniej z zakresu między 0.2 a 0.3.

```{r}
data3_plot = data2_plot %>% filter(skill >0.2 & skill < 0.3)

plot(data3_plot$mean,data3_plot$skill)

cat(paste("Korelacja wynosi ", round(cor(data3_plot$skill,data3_plot$mean),3),".", sep = ""))
```

Widzimy, że ten wykres przypomina pierwszy wykres. Zawęziliśmy zakres zmiennej, więc błąd wydaje się relatywnie większy. 

Co natomiast możemy powiedzieć o małych wielkościach efektu w psychologii? Mały efekt, to po prostu mały efekt. Nie kumuluje się w czasie. Ale to nie znaczy, że jest nieistony. Jeśli badamy zachowanie człowieka, byłoby wręcz dziwnie, jeśli efekt jednej zmiennej byłby duży, zwłaszcza w przypadku złożonych zachowań. 